{
    "docs": [
        {
            "location": "/",
            "text": "M3DB Operator\n\n\nIntroduction\n\n\nWelcome to the documentation for the M3DB operator, a \nKubernetes operator\n for running the open-source\ntimeseries database \nM3DB\n on Kubernetes.\n\n\nPlease note that this is \nalpha software\n, and as such its APIs and behavior are subject to breaking changes. While we\naim to produce thoroughly tested reliable software there may be undiscovered bugs.\n\n\nFor more background on the M3DB operator, see our \nKubeCon keynote\n on its origins and usage at Uber.\n\n\nPhilosophy\n\n\nThe M3DB operator aims to automate everyday tasks around managing M3DB. Specifically, it aims to automate:\n\n\n\n\nCreating M3DB clusters\n\n\nDestroying M3DB clusters\n\n\nExpanding clusters (adding instances)\n\n\nShrinking clusters (removing instances)\n\n\nReplacing failed instances\n\n\n\n\nIt explicitly does not try to automate every single edge case a user may ever run into. For example, it does not aim to\nautomate disaster recovery if an entire cluster is taken down. Such use cases may still require human intervention, but\nthe operator will aim to not conflict with such operations a human may have to take on a cluster.\n\n\nGenerally speaking, the operator's philosophy is if \nit would be unclear to a human what action to take, we will not\ntry to guess.",
            "title": "Introduction"
        },
        {
            "location": "/#m3db-operator",
            "text": "",
            "title": "M3DB Operator"
        },
        {
            "location": "/#introduction",
            "text": "Welcome to the documentation for the M3DB operator, a  Kubernetes operator  for running the open-source\ntimeseries database  M3DB  on Kubernetes.  Please note that this is  alpha software , and as such its APIs and behavior are subject to breaking changes. While we\naim to produce thoroughly tested reliable software there may be undiscovered bugs.  For more background on the M3DB operator, see our  KubeCon keynote  on its origins and usage at Uber.",
            "title": "Introduction"
        },
        {
            "location": "/#philosophy",
            "text": "The M3DB operator aims to automate everyday tasks around managing M3DB. Specifically, it aims to automate:   Creating M3DB clusters  Destroying M3DB clusters  Expanding clusters (adding instances)  Shrinking clusters (removing instances)  Replacing failed instances   It explicitly does not try to automate every single edge case a user may ever run into. For example, it does not aim to\nautomate disaster recovery if an entire cluster is taken down. Such use cases may still require human intervention, but\nthe operator will aim to not conflict with such operations a human may have to take on a cluster.  Generally speaking, the operator's philosophy is if  it would be unclear to a human what action to take, we will not\ntry to guess.",
            "title": "Philosophy"
        },
        {
            "location": "/getting_started/requirements/",
            "text": "Requirements\n\n\nKubernetes Versions\n\n\nThe M3DB operator current targets Kubernetes 1.11 and 1.12. Given the operator's current production use cases at Uber,\nwe typically target the two most recent minor Kubernetes versions supported by GKE. We welcome community contributions\nto support more recent versions while meeting the aforementioned GKE targets!\n\n\nMulti-Zone Kubernetes Cluster\n\n\nThe M3DB operator is intended to be used with Kubernetes clusters that span at least 3 zones within a region to create\nhighly available clusters and maintain quorum in the event of region failures. Instructions for creating regional\nclusters on GKE can be found \nhere\n.\n\n\nEtcd\n\n\nM3DB stores its cluster topology and all other runtime metadata in \netcd\n.\n\n\nFor \ntesting / non-production use cases\n, we provide simple manifests for running etcd on Kubernetes in our \nexample\nmanifests\n: one for running ephemeral etcd containers and one for running etcd using basic persistent\nvolumes. If using the \netcd-pd\n yaml manifest, we recommend a modification to use a \nStorageClass\n equivalent to your\ncloud provider's fastest remote disk (such as \npd-ssd\n on GCP).\n\n\nFor production use cases, we recommend running etcd (in order of preference):\n\n\n\n\nExternal to your Kubernetes cluster to avoid circular dependencies.\n\n\nUsing the \netcd operator\n.",
            "title": "Requirements"
        },
        {
            "location": "/getting_started/requirements/#requirements",
            "text": "",
            "title": "Requirements"
        },
        {
            "location": "/getting_started/requirements/#kubernetes-versions",
            "text": "The M3DB operator current targets Kubernetes 1.11 and 1.12. Given the operator's current production use cases at Uber,\nwe typically target the two most recent minor Kubernetes versions supported by GKE. We welcome community contributions\nto support more recent versions while meeting the aforementioned GKE targets!",
            "title": "Kubernetes Versions"
        },
        {
            "location": "/getting_started/requirements/#multi-zone-kubernetes-cluster",
            "text": "The M3DB operator is intended to be used with Kubernetes clusters that span at least 3 zones within a region to create\nhighly available clusters and maintain quorum in the event of region failures. Instructions for creating regional\nclusters on GKE can be found  here .",
            "title": "Multi-Zone Kubernetes Cluster"
        },
        {
            "location": "/getting_started/requirements/#etcd",
            "text": "M3DB stores its cluster topology and all other runtime metadata in  etcd .  For  testing / non-production use cases , we provide simple manifests for running etcd on Kubernetes in our  example\nmanifests : one for running ephemeral etcd containers and one for running etcd using basic persistent\nvolumes. If using the  etcd-pd  yaml manifest, we recommend a modification to use a  StorageClass  equivalent to your\ncloud provider's fastest remote disk (such as  pd-ssd  on GCP).  For production use cases, we recommend running etcd (in order of preference):   External to your Kubernetes cluster to avoid circular dependencies.  Using the  etcd operator .",
            "title": "Etcd"
        },
        {
            "location": "/getting_started/installation/",
            "text": "Installation\n\n\nBe sure to take a look at the \nrequirements\n before installing the operator.\n\n\nHelm\n\n\n\n\nAdd the \nm3db-operator\n repo:\n\n\n\n\nhelm repo add m3db https://m3-helm-charts.storage.googleapis.com/stable\n\n\n\n\n\n\nInstall the \nm3db-operator\n chart:\n\n\n\n\nhelm install m3db/m3db-operator --namespace m3db-operator\n\n\n\n\nNote\n: If uninstalling an instance of the operator that was installed with Helm, some resources such as the\nClusterRole, ClusterRoleBinding, and ServiceAccount may need to be deleted manually.\n\n\nManually\n\n\nInstall the bundled operator manifests in the current namespace:\n\n\nkubectl apply -f https://raw.githubusercontent.com/m3db/m3db-operator/master/bundle.yaml",
            "title": "Installation"
        },
        {
            "location": "/getting_started/installation/#installation",
            "text": "Be sure to take a look at the  requirements  before installing the operator.",
            "title": "Installation"
        },
        {
            "location": "/getting_started/installation/#helm",
            "text": "Add the  m3db-operator  repo:   helm repo add m3db https://m3-helm-charts.storage.googleapis.com/stable   Install the  m3db-operator  chart:   helm install m3db/m3db-operator --namespace m3db-operator  Note : If uninstalling an instance of the operator that was installed with Helm, some resources such as the\nClusterRole, ClusterRoleBinding, and ServiceAccount may need to be deleted manually.",
            "title": "Helm"
        },
        {
            "location": "/getting_started/installation/#manually",
            "text": "Install the bundled operator manifests in the current namespace:  kubectl apply -f https://raw.githubusercontent.com/m3db/m3db-operator/master/bundle.yaml",
            "title": "Manually"
        },
        {
            "location": "/getting_started/create_cluster/",
            "text": "Creating a Cluster\n\n\nOnce you've \ninstalled\n the M3DB operator and read over the \nrequirements\n, you can start\ncreating some M3DB clusters!\n\n\nBasic Cluster\n\n\nThe following creates an M3DB cluster spread across 3 zones, with each M3DB instance being able to store up to 350gb of\ndata using your Kubernetes cluster's default storage class. For examples of different cluster topologies, such as zonal\nclusters, see the docs on \nnode affinity\n.\n\n\nEtcd\n\n\nCreate an etcd cluster with persistent volumes:\n\nkubectl apply -f https://raw.githubusercontent.com/m3db/m3db-operator/v0.5.0/example/etcd/etcd-pd.yaml\n\n\n\nWe recommend modifying the \nstorageClassName\n in the manifest to one that matches your cloud provider's fastest remote\nstorage option, such as \npd-ssd\n on GCP.\n\n\nM3DB\n\n\napiVersion\n:\n \noperator.m3db.io/v1alpha1\n\n\nkind\n:\n \nM3DBCluster\n\n\nmetadata\n:\n\n  \nname\n:\n \npersistent-cluster\n\n\nspec\n:\n\n  \nimage\n:\n \nquay.io/m3db/m3dbnode:latest\n\n  \nreplicationFactor\n:\n \n3\n\n  \nnumberOfShards\n:\n \n256\n\n  \nisolationGroups\n:\n\n  \n-\n \nname\n:\n \ngroup1\n\n    \nnumInstances\n:\n \n1\n\n    \nnodeAffinityTerms\n:\n\n    \n-\n \nkey\n:\n \nfailure-domain.beta.kubernetes.io/zone\n\n      \nvalues\n:\n\n      \n-\n \n<zone-a>\n\n  \n-\n \nname\n:\n \ngroup2\n\n    \nnumInstances\n:\n \n1\n\n    \nnodeAffinityTerms\n:\n\n    \n-\n \nkey\n:\n \nfailure-domain.beta.kubernetes.io/zone\n\n      \nvalues\n:\n\n      \n-\n \n<zone-b>\n\n  \n-\n \nname\n:\n \ngroup3\n\n    \nnumInstances\n:\n \n1\n\n    \nnodeAffinityTerms\n:\n\n    \n-\n \nkey\n:\n \nfailure-domain.beta.kubernetes.io/zone\n\n      \nvalues\n:\n\n      \n-\n \n<zone-c>\n\n  \netcdEndpoints\n:\n\n  \n-\n \nhttp://etcd-0.etcd:2379\n\n  \n-\n \nhttp://etcd-1.etcd:2379\n\n  \n-\n \nhttp://etcd-2.etcd:2379\n\n  \npodIdentityConfig\n:\n\n    \nsources\n:\n \n[]\n\n  \nnamespaces\n:\n\n    \n-\n \nname\n:\n \nmetrics-10s:2d\n\n      \npreset\n:\n \n10s:2d\n\n  \ndataDirVolumeClaimTemplate\n:\n\n    \nmetadata\n:\n\n      \nname\n:\n \nm3db-data\n\n    \nspec\n:\n\n      \naccessModes\n:\n\n      \n-\n \nReadWriteOnce\n\n      \nresources\n:\n\n        \nrequests\n:\n\n          \nstorage\n:\n \n350Gi\n\n        \nlimits\n:\n\n          \nstorage\n:\n \n350Gi\n\n\n\n\n\nEphemeral Cluster\n\n\nWARNING:\n This setup is not intended for production-grade clusters, but rather for \"kicking the tires\" with the\noperator and M3DB. It is intended to work across almost any Kubernetes environment, and as such has as few dependencies\nas possible (namely persistent storage). See below for instructions on creating a more durable cluster.\n\n\nEtcd\n\n\nCreate an etcd cluster in the same namespace your M3DB cluster will be created in. If you don't have persistent storage\navailable, this will create a cluster that will not use persistent storage and will likely become unavailable if any of\nthe pods die:\n\n\nkubectl apply -f https://raw.githubusercontent.com/m3db/m3db-operator/v0.5.0/example/etcd/etcd-basic.yaml\n\n# Verify etcd health once pods available\nkubectl exec etcd-0 -- env ETCDCTL_API=3 etcdctl endpoint health\n# 127.0.0.1:2379 is healthy: successfully committed proposal: took = 2.94668ms\n\n\n\n\nIf you have remote storage available and would like to jump straight to using it, apply the following manifest for etcd\ninstead:\n\nkubectl apply -f https://raw.githubusercontent.com/m3db/m3db-operator/v0.5.0/example/etcd/etcd-pd.yaml\n\n\n\nM3DB\n\n\nOnce etcd is available, you can create an M3DB cluster. An example of a very basic M3DB cluster definition is as\nfollows:\n\n\napiVersion\n:\n \noperator.m3db.io/v1alpha1\n\n\nkind\n:\n \nM3DBCluster\n\n\nmetadata\n:\n\n  \nname\n:\n \nsimple-cluster\n\n\nspec\n:\n\n  \nimage\n:\n \nquay.io/m3db/m3dbnode:latest\n\n  \nreplicationFactor\n:\n \n3\n\n  \nnumberOfShards\n:\n \n256\n\n  \netcdEndpoints\n:\n\n  \n-\n \nhttp://etcd-0.etcd:2379\n\n  \n-\n \nhttp://etcd-1.etcd:2379\n\n  \n-\n \nhttp://etcd-2.etcd:2379\n\n  \nisolationGroups\n:\n\n  \n-\n \nname\n:\n \ngroup1\n\n    \nnumInstances\n:\n \n1\n\n    \nnodeAffinityTerms\n:\n\n    \n-\n \nkey\n:\n \nfailure-domain.beta.kubernetes.io/zone\n\n      \nvalues\n:\n\n      \n-\n \n<zone-a>\n\n  \n-\n \nname\n:\n \ngroup2\n\n    \nnumInstances\n:\n \n1\n\n    \nnodeAffinityTerms\n:\n\n    \n-\n \nkey\n:\n \nfailure-domain.beta.kubernetes.io/zone\n\n      \nvalues\n:\n\n      \n-\n \n<zone-b>\n\n  \n-\n \nname\n:\n \ngroup3\n\n    \nnumInstances\n:\n \n1\n\n    \nnodeAffinityTerms\n:\n\n    \n-\n \nkey\n:\n \nfailure-domain.beta.kubernetes.io/zone\n\n      \nvalues\n:\n\n      \n-\n \n<zone-c>\n\n  \npodIdentityConfig\n:\n\n    \nsources\n:\n\n      \n-\n \nPodUID\n\n  \nnamespaces\n:\n\n    \n-\n \nname\n:\n \nmetrics-10s:2d\n\n      \npreset\n:\n \n10s:2d\n\n\n\n\n\nThis will create a highly available cluster with RF=3 spread evenly across the three given zones within a region. A\npod's UID will be used for its \nidentity\n. The cluster will have 1 \nnamespace\n that stores\nmetrics for 2 days at 10s resolution.\n\n\nNext, apply your manifest:\n\n$ kubectl apply -f example/simple-cluster.yaml\nm3dbcluster.operator.m3db.io/simple-cluster created\n\n\n\nShortly after all pods are created you should see the cluster ready!\n\n\n$ kubectl get po -l operator.m3db.io/app=m3db\nNAME                    READY   STATUS    RESTARTS   AGE\nsimple-cluster-rep0-0   1/1     Running   0          1m\nsimple-cluster-rep1-0   1/1     Running   0          56s\nsimple-cluster-rep2-0   1/1     Running   0          37s\n\n\n\n\nWe can verify that the cluster has finished streaming data by peers by checking that an instance has bootstrapped:\n\n$ kubectl exec simple-cluster-rep2-0 -- curl -sSf localhost:9002/health\n{\"ok\":true,\"status\":\"up\",\"bootstrapped\":true}",
            "title": "Creating a Cluster"
        },
        {
            "location": "/getting_started/create_cluster/#creating-a-cluster",
            "text": "Once you've  installed  the M3DB operator and read over the  requirements , you can start\ncreating some M3DB clusters!",
            "title": "Creating a Cluster"
        },
        {
            "location": "/getting_started/create_cluster/#basic-cluster",
            "text": "The following creates an M3DB cluster spread across 3 zones, with each M3DB instance being able to store up to 350gb of\ndata using your Kubernetes cluster's default storage class. For examples of different cluster topologies, such as zonal\nclusters, see the docs on  node affinity .",
            "title": "Basic Cluster"
        },
        {
            "location": "/getting_started/create_cluster/#etcd",
            "text": "Create an etcd cluster with persistent volumes: kubectl apply -f https://raw.githubusercontent.com/m3db/m3db-operator/v0.5.0/example/etcd/etcd-pd.yaml  We recommend modifying the  storageClassName  in the manifest to one that matches your cloud provider's fastest remote\nstorage option, such as  pd-ssd  on GCP.",
            "title": "Etcd"
        },
        {
            "location": "/getting_started/create_cluster/#m3db",
            "text": "apiVersion :   operator.m3db.io/v1alpha1  kind :   M3DBCluster  metadata : \n   name :   persistent-cluster  spec : \n   image :   quay.io/m3db/m3dbnode:latest \n   replicationFactor :   3 \n   numberOfShards :   256 \n   isolationGroups : \n   -   name :   group1 \n     numInstances :   1 \n     nodeAffinityTerms : \n     -   key :   failure-domain.beta.kubernetes.io/zone \n       values : \n       -   <zone-a> \n   -   name :   group2 \n     numInstances :   1 \n     nodeAffinityTerms : \n     -   key :   failure-domain.beta.kubernetes.io/zone \n       values : \n       -   <zone-b> \n   -   name :   group3 \n     numInstances :   1 \n     nodeAffinityTerms : \n     -   key :   failure-domain.beta.kubernetes.io/zone \n       values : \n       -   <zone-c> \n   etcdEndpoints : \n   -   http://etcd-0.etcd:2379 \n   -   http://etcd-1.etcd:2379 \n   -   http://etcd-2.etcd:2379 \n   podIdentityConfig : \n     sources :   [] \n   namespaces : \n     -   name :   metrics-10s:2d \n       preset :   10s:2d \n   dataDirVolumeClaimTemplate : \n     metadata : \n       name :   m3db-data \n     spec : \n       accessModes : \n       -   ReadWriteOnce \n       resources : \n         requests : \n           storage :   350Gi \n         limits : \n           storage :   350Gi",
            "title": "M3DB"
        },
        {
            "location": "/getting_started/create_cluster/#ephemeral-cluster",
            "text": "WARNING:  This setup is not intended for production-grade clusters, but rather for \"kicking the tires\" with the\noperator and M3DB. It is intended to work across almost any Kubernetes environment, and as such has as few dependencies\nas possible (namely persistent storage). See below for instructions on creating a more durable cluster.",
            "title": "Ephemeral Cluster"
        },
        {
            "location": "/getting_started/create_cluster/#etcd_1",
            "text": "Create an etcd cluster in the same namespace your M3DB cluster will be created in. If you don't have persistent storage\navailable, this will create a cluster that will not use persistent storage and will likely become unavailable if any of\nthe pods die:  kubectl apply -f https://raw.githubusercontent.com/m3db/m3db-operator/v0.5.0/example/etcd/etcd-basic.yaml\n\n# Verify etcd health once pods available\nkubectl exec etcd-0 -- env ETCDCTL_API=3 etcdctl endpoint health\n# 127.0.0.1:2379 is healthy: successfully committed proposal: took = 2.94668ms  If you have remote storage available and would like to jump straight to using it, apply the following manifest for etcd\ninstead: kubectl apply -f https://raw.githubusercontent.com/m3db/m3db-operator/v0.5.0/example/etcd/etcd-pd.yaml",
            "title": "Etcd"
        },
        {
            "location": "/getting_started/create_cluster/#m3db_1",
            "text": "Once etcd is available, you can create an M3DB cluster. An example of a very basic M3DB cluster definition is as\nfollows:  apiVersion :   operator.m3db.io/v1alpha1  kind :   M3DBCluster  metadata : \n   name :   simple-cluster  spec : \n   image :   quay.io/m3db/m3dbnode:latest \n   replicationFactor :   3 \n   numberOfShards :   256 \n   etcdEndpoints : \n   -   http://etcd-0.etcd:2379 \n   -   http://etcd-1.etcd:2379 \n   -   http://etcd-2.etcd:2379 \n   isolationGroups : \n   -   name :   group1 \n     numInstances :   1 \n     nodeAffinityTerms : \n     -   key :   failure-domain.beta.kubernetes.io/zone \n       values : \n       -   <zone-a> \n   -   name :   group2 \n     numInstances :   1 \n     nodeAffinityTerms : \n     -   key :   failure-domain.beta.kubernetes.io/zone \n       values : \n       -   <zone-b> \n   -   name :   group3 \n     numInstances :   1 \n     nodeAffinityTerms : \n     -   key :   failure-domain.beta.kubernetes.io/zone \n       values : \n       -   <zone-c> \n   podIdentityConfig : \n     sources : \n       -   PodUID \n   namespaces : \n     -   name :   metrics-10s:2d \n       preset :   10s:2d   This will create a highly available cluster with RF=3 spread evenly across the three given zones within a region. A\npod's UID will be used for its  identity . The cluster will have 1  namespace  that stores\nmetrics for 2 days at 10s resolution.  Next, apply your manifest: $ kubectl apply -f example/simple-cluster.yaml\nm3dbcluster.operator.m3db.io/simple-cluster created  Shortly after all pods are created you should see the cluster ready!  $ kubectl get po -l operator.m3db.io/app=m3db\nNAME                    READY   STATUS    RESTARTS   AGE\nsimple-cluster-rep0-0   1/1     Running   0          1m\nsimple-cluster-rep1-0   1/1     Running   0          56s\nsimple-cluster-rep2-0   1/1     Running   0          37s  We can verify that the cluster has finished streaming data by peers by checking that an instance has bootstrapped: $ kubectl exec simple-cluster-rep2-0 -- curl -sSf localhost:9002/health\n{\"ok\":true,\"status\":\"up\",\"bootstrapped\":true}",
            "title": "M3DB"
        },
        {
            "location": "/getting_started/delete_cluster/",
            "text": "Deleting a Cluster\n\n\nDelete your M3DB cluster with \nkubectl\n:\n\nkubectl delete m3dbcluster simple-cluster\n\n\n\nBy default, the operator will delete the placement and namespaces associated with a cluster before the CRD resource\ndeleted. If you do \nnot\n want this behavior, set \nkeepEtcdDataOnDelete\n to \ntrue\n on your cluster spec.\n\n\nUnder the hood, the operator uses Kubernetes \nfinalizers\n to ensure the cluster CRD is not deleted until the operator\nhas had a chance to do cleanup.\n\n\nDebugging Stuck Cluster Deletion\n\n\nIf for some reason the operator is unable to delete the placement and namespace for the cluster, the cluster CRD itself\nwill be stuck in a state where it can not be deleted, due to the way finalizers work in Kubernetes. The operator might\nbe unable to clean up the data for many reasons, for example if the M3DB cluster itself is not available to serve the\nAPIs for cleanup or if etcd is down and cannot fulfill the deleted.\n\n\nTo allow the CRD to be deleted, you can \nkubectl edit m3dbcluster $CLUSTER\n and remove the\n\noperator.m3db.io/etcd-deletion\n finalizer. For example, in the following cluster you'd remove the finalizer from \nmetadata.finalizers\n:\n\n\napiVersion\n:\n \noperator.m3db.io/v1alpha1\n\n\nkind\n:\n \nM3DBCluster\n\n\nmetadata\n:\n\n  \n...\n\n  \nfinalizers\n:\n\n  \n-\n \noperator.m3db.io/etcd-deletion\n\n  \nname\n:\n \nm3db-cluster\n\n\n...\n\n\n\n\n\nNote that if you do this, you'll have to manually remove the relevant data in etcd. For a cluster in namespace \n$NS\n\nwith name \n$CLUSTER\n, the keys are:\n\n\n\n\n_sd.placement/$NS/$CLUSTER/m3db\n\n\n_kv/$NS/$CLUSTER/m3db.node.namespaces",
            "title": "Deleting a Cluster"
        },
        {
            "location": "/getting_started/delete_cluster/#deleting-a-cluster",
            "text": "Delete your M3DB cluster with  kubectl : kubectl delete m3dbcluster simple-cluster  By default, the operator will delete the placement and namespaces associated with a cluster before the CRD resource\ndeleted. If you do  not  want this behavior, set  keepEtcdDataOnDelete  to  true  on your cluster spec.  Under the hood, the operator uses Kubernetes  finalizers  to ensure the cluster CRD is not deleted until the operator\nhas had a chance to do cleanup.",
            "title": "Deleting a Cluster"
        },
        {
            "location": "/getting_started/delete_cluster/#debugging-stuck-cluster-deletion",
            "text": "If for some reason the operator is unable to delete the placement and namespace for the cluster, the cluster CRD itself\nwill be stuck in a state where it can not be deleted, due to the way finalizers work in Kubernetes. The operator might\nbe unable to clean up the data for many reasons, for example if the M3DB cluster itself is not available to serve the\nAPIs for cleanup or if etcd is down and cannot fulfill the deleted.  To allow the CRD to be deleted, you can  kubectl edit m3dbcluster $CLUSTER  and remove the operator.m3db.io/etcd-deletion  finalizer. For example, in the following cluster you'd remove the finalizer from  metadata.finalizers :  apiVersion :   operator.m3db.io/v1alpha1  kind :   M3DBCluster  metadata : \n   ... \n   finalizers : \n   -   operator.m3db.io/etcd-deletion \n   name :   m3db-cluster  ...   Note that if you do this, you'll have to manually remove the relevant data in etcd. For a cluster in namespace  $NS \nwith name  $CLUSTER , the keys are:   _sd.placement/$NS/$CLUSTER/m3db  _kv/$NS/$CLUSTER/m3db.node.namespaces",
            "title": "Debugging Stuck Cluster Deletion"
        },
        {
            "location": "/getting_started/monitoring/",
            "text": "Monitoring\n\n\nM3DB exposes metrics via a Prometheus endpoint. If using the \nPrometheus Operator\n, you can apply a\n\nServiceMonitor\n to have your M3DB pods automatically scraped by Prometheus:\n\n\nkubectl apply -f https://raw.githubusercontent.com/m3db/m3db-operator/master/example/prometheus-servicemonitor.yaml\n\n\n\n\nYou can visit the \"targets\" page of the Prometheus UI to verify the pods are being scraped. To view these metrics using\nGrafana, follow the \nM3 docs\n to install the M3DB Grafana dashboard.",
            "title": "Monitoring"
        },
        {
            "location": "/getting_started/monitoring/#monitoring",
            "text": "M3DB exposes metrics via a Prometheus endpoint. If using the  Prometheus Operator , you can apply a ServiceMonitor  to have your M3DB pods automatically scraped by Prometheus:  kubectl apply -f https://raw.githubusercontent.com/m3db/m3db-operator/master/example/prometheus-servicemonitor.yaml  You can visit the \"targets\" page of the Prometheus UI to verify the pods are being scraped. To view these metrics using\nGrafana, follow the  M3 docs  to install the M3DB Grafana dashboard.",
            "title": "Monitoring"
        },
        {
            "location": "/configuration/configuring_m3db/",
            "text": "Configuring M3DB\n\n\nBy default the operator will apply a configmap with basic M3DB options and settings for the coordinator to direct\nPrometheus reads/writes to the cluster. This template can be found\n\nhere\n.\n\n\nTo apply custom a configuration for the M3DB cluster, one can set the \nconfigMapName\n parameter of the cluster \nspec\n to\nan existing configmap.\n\n\nEnvironment Warning\n\n\nIf providing a custom config map, the \nenv\n you specify in your \nconfig\n \nmust\n be \n$NAMESPACE/$NAME\n, where\n\n$NAMESPACE\n is the Kubernetes namespace your cluster is in and \n$NAME\n is the name of the cluster. For example, with\nthe following cluster:\n\n\napiVersion\n:\n \noperator.m3db.io/v1alpha1\n\n\nkind\n:\n \nM3DBCluster\n\n\nmetadata\n:\n\n  \nname\n:\n \ncluster-a\n\n  \nnamespace\n:\n \nproduction\n\n\n...\n\n\n\n\n\nThe value of \nenv\n in your config \nMUST\n be \nproduction/cluster-a\n. This restriction allows multiple M3DB clusters to\nsafely share the same etcd cluster.",
            "title": "Configuring M3DB"
        },
        {
            "location": "/configuration/configuring_m3db/#configuring-m3db",
            "text": "By default the operator will apply a configmap with basic M3DB options and settings for the coordinator to direct\nPrometheus reads/writes to the cluster. This template can be found here .  To apply custom a configuration for the M3DB cluster, one can set the  configMapName  parameter of the cluster  spec  to\nan existing configmap.",
            "title": "Configuring M3DB"
        },
        {
            "location": "/configuration/configuring_m3db/#environment-warning",
            "text": "If providing a custom config map, the  env  you specify in your  config   must  be  $NAMESPACE/$NAME , where $NAMESPACE  is the Kubernetes namespace your cluster is in and  $NAME  is the name of the cluster. For example, with\nthe following cluster:  apiVersion :   operator.m3db.io/v1alpha1  kind :   M3DBCluster  metadata : \n   name :   cluster-a \n   namespace :   production  ...   The value of  env  in your config  MUST  be  production/cluster-a . This restriction allows multiple M3DB clusters to\nsafely share the same etcd cluster.",
            "title": "Environment Warning"
        },
        {
            "location": "/configuration/pod_identity/",
            "text": "Pod Identity\n\n\nMotivation\n\n\nM3DB assumes that if a process is started and owns sealed shards marked as \nAvailable\n that its data for those shards is\nvalid and does not have to be fetched from peers. Consequentially this means it will begin serving reads for that data.\nFor more background on M3DB topology, see the \nM3DB topology docs\n.\n\n\nIn most environments in which M3DB has been deployed in production, it has been on a set of hosts predetermined by\nwhomever is managing the cluster. This means that an M3DB instance is identified in a toplogy by its hostname, and that\nwhen an M3DB process comes up and finds its hostname in the cluster with \nAvailable\n shards that it can serve reads for\nthose shards.\n\n\nThis does not work on Kubernetes, particularly when working with StatefulSets, as a pod may be rescheduled on a new node\nor with new storage attached but its name may stay the same. If we were to naively use an instance's hostname (pod\nname), and it were to get rescheduled on a new node with no data, it could assume that absence of data is valid and\nbegin returning empty results for read requests.\n\n\nTo account for this, the M3DB Operator determines an M3DB instance's identity in the topology based on a configurable\nset of metadata about the pod.\n\n\nConfiguration\n\n\nThe M3DB operator uses a configurable set of metadata about a pod to determine its identity in the M3DB placement. This\nis encapsulated in the \nPodIdentityConfig\n field of a cluster's spec. In addition to the configures sources,\na pod's name will always be included.\n\n\nEvery pod in an M3DB cluster is annotated with its identity and is passed to the M3DB instance via a downward API\nvolume.\n\n\nSources\n\n\nThis section will be filled out as a number of pending PRs land.\n\n\nRecommendations\n\n\nNo Persistent Storage\n\n\nIf not using PVs, you should set \nsources\n to \nPodUID\n:\n\npodIdentityConfig:\n  sources:\n  - PodUID\n\n\n\nThis way whenever a container is rescheduled, the operator will initiate a replace and it will stream data from its\npeers before serving reads. Note that not having persistent storage is not a recommended way to run M3DB.\n\n\nRemote Persistent Storage\n\n\nIf using remote storage you do not need to set sources, as it will default to just the pods name. The data for an M3DB\ninstance will move around with its container.\n\n\nLocal Persistent Storage\n\n\nIf using persistent local volumes, you should set sources to \nNodeName\n. In this configuration M3DB will consider a pod\nto be the same so long as it's on the same node. Replaces will only be triggered if a pod with the same name is moved to\na new host.\n\n\nNote that if using local SSDs on GKE, node names may stay the same even though a VM has been recreated. We also support\n\nProviderID\n, which will use the underlying VM's unique ID number in GCE to identity host uniqueness.",
            "title": "Pod Identity"
        },
        {
            "location": "/configuration/pod_identity/#pod-identity",
            "text": "",
            "title": "Pod Identity"
        },
        {
            "location": "/configuration/pod_identity/#motivation",
            "text": "M3DB assumes that if a process is started and owns sealed shards marked as  Available  that its data for those shards is\nvalid and does not have to be fetched from peers. Consequentially this means it will begin serving reads for that data.\nFor more background on M3DB topology, see the  M3DB topology docs .  In most environments in which M3DB has been deployed in production, it has been on a set of hosts predetermined by\nwhomever is managing the cluster. This means that an M3DB instance is identified in a toplogy by its hostname, and that\nwhen an M3DB process comes up and finds its hostname in the cluster with  Available  shards that it can serve reads for\nthose shards.  This does not work on Kubernetes, particularly when working with StatefulSets, as a pod may be rescheduled on a new node\nor with new storage attached but its name may stay the same. If we were to naively use an instance's hostname (pod\nname), and it were to get rescheduled on a new node with no data, it could assume that absence of data is valid and\nbegin returning empty results for read requests.  To account for this, the M3DB Operator determines an M3DB instance's identity in the topology based on a configurable\nset of metadata about the pod.",
            "title": "Motivation"
        },
        {
            "location": "/configuration/pod_identity/#configuration",
            "text": "The M3DB operator uses a configurable set of metadata about a pod to determine its identity in the M3DB placement. This\nis encapsulated in the  PodIdentityConfig  field of a cluster's spec. In addition to the configures sources,\na pod's name will always be included.  Every pod in an M3DB cluster is annotated with its identity and is passed to the M3DB instance via a downward API\nvolume.",
            "title": "Configuration"
        },
        {
            "location": "/configuration/pod_identity/#sources",
            "text": "This section will be filled out as a number of pending PRs land.",
            "title": "Sources"
        },
        {
            "location": "/configuration/pod_identity/#recommendations",
            "text": "",
            "title": "Recommendations"
        },
        {
            "location": "/configuration/pod_identity/#no-persistent-storage",
            "text": "If not using PVs, you should set  sources  to  PodUID : podIdentityConfig:\n  sources:\n  - PodUID  This way whenever a container is rescheduled, the operator will initiate a replace and it will stream data from its\npeers before serving reads. Note that not having persistent storage is not a recommended way to run M3DB.",
            "title": "No Persistent Storage"
        },
        {
            "location": "/configuration/pod_identity/#remote-persistent-storage",
            "text": "If using remote storage you do not need to set sources, as it will default to just the pods name. The data for an M3DB\ninstance will move around with its container.",
            "title": "Remote Persistent Storage"
        },
        {
            "location": "/configuration/pod_identity/#local-persistent-storage",
            "text": "If using persistent local volumes, you should set sources to  NodeName . In this configuration M3DB will consider a pod\nto be the same so long as it's on the same node. Replaces will only be triggered if a pod with the same name is moved to\na new host.  Note that if using local SSDs on GKE, node names may stay the same even though a VM has been recreated. We also support ProviderID , which will use the underlying VM's unique ID number in GCE to identity host uniqueness.",
            "title": "Local Persistent Storage"
        },
        {
            "location": "/configuration/namespaces/",
            "text": "Namespaces\n\n\nM3DB uses the concept of \nnamespaces\n to determine how metrics are stored and retained. The M3DB\noperator allows a user to define their own namespaces, or to use a set of presets we consider to be suitable for\nproduction use cases.\n\n\nNamespaces are configured as part of an \nm3dbcluster\n \nspec\n.\n\n\nPresets\n\n\n10s:2d\n\n\nThis preset will store metrics at 10 second resolution for 2 days. For example, in your cluster spec:\n\n\nspec\n:\n\n\n...\n\n  \nnamespaces\n:\n\n  \n-\n \nname\n:\n \nmetrics-short-term\n\n    \npreset\n:\n \n10s:2d\n\n\n\n\n\n1m:40d\n\n\nThis preset will store metrics at 1 minute resolution for 40 days.\n\n\nspec\n:\n\n\n...\n\n  \nnamespaces\n:\n\n  \n-\n \nname\n:\n \nmetrics-long-term\n\n    \npreset\n:\n \n1m:40d\n\n\n\n\n\nCustom Namespaces\n\n\nYou can also define your own custom namespaces by setting the \nNamespaceOptions\n within a cluster spec. The\n\nAPI\n lists all available fields. As an example, a namespace to store 7 days of data may look like:\n\n...\n\n\nspec\n:\n\n\n...\n\n  \nnamespaces\n:\n\n  \n-\n \nname\n:\n \ncustom-7d\n\n    \noptions\n:\n\n      \nbootstrapEnabled\n:\n \ntrue\n\n      \nflushEnabled\n:\n \ntrue\n\n      \nwritesToCommitLog\n:\n \ntrue\n\n      \ncleanupEnabled\n:\n \ntrue\n\n      \nsnapshotEnabled\n:\n \ntrue\n\n      \nrepairEnabled\n:\n \nfalse\n\n      \nretentionOptions\n:\n\n        \nretentionPeriod\n:\n \n168h\n\n        \nblockSize\n:\n \n12h\n\n        \nbufferFuture\n:\n \n20m\n\n        \nbufferPast\n:\n \n20m\n\n        \nblockDataExpiry\n:\n \ntrue\n\n        \nblockDataExpiryAfterNotAccessPeriod\n:\n \n5m\n\n      \nindexOptions\n:\n\n        \nenabled\n:\n \ntrue\n\n        \nblockSize\n:\n \n12h",
            "title": "Namespaces"
        },
        {
            "location": "/configuration/namespaces/#namespaces",
            "text": "M3DB uses the concept of  namespaces  to determine how metrics are stored and retained. The M3DB\noperator allows a user to define their own namespaces, or to use a set of presets we consider to be suitable for\nproduction use cases.  Namespaces are configured as part of an  m3dbcluster   spec .",
            "title": "Namespaces"
        },
        {
            "location": "/configuration/namespaces/#presets",
            "text": "",
            "title": "Presets"
        },
        {
            "location": "/configuration/namespaces/#10s2d",
            "text": "This preset will store metrics at 10 second resolution for 2 days. For example, in your cluster spec:  spec :  ... \n   namespaces : \n   -   name :   metrics-short-term \n     preset :   10s:2d",
            "title": "10s:2d"
        },
        {
            "location": "/configuration/namespaces/#1m40d",
            "text": "This preset will store metrics at 1 minute resolution for 40 days.  spec :  ... \n   namespaces : \n   -   name :   metrics-long-term \n     preset :   1m:40d",
            "title": "1m:40d"
        },
        {
            "location": "/configuration/namespaces/#custom-namespaces",
            "text": "You can also define your own custom namespaces by setting the  NamespaceOptions  within a cluster spec. The API  lists all available fields. As an example, a namespace to store 7 days of data may look like: ...  spec :  ... \n   namespaces : \n   -   name :   custom-7d \n     options : \n       bootstrapEnabled :   true \n       flushEnabled :   true \n       writesToCommitLog :   true \n       cleanupEnabled :   true \n       snapshotEnabled :   true \n       repairEnabled :   false \n       retentionOptions : \n         retentionPeriod :   168h \n         blockSize :   12h \n         bufferFuture :   20m \n         bufferPast :   20m \n         blockDataExpiry :   true \n         blockDataExpiryAfterNotAccessPeriod :   5m \n       indexOptions : \n         enabled :   true \n         blockSize :   12h",
            "title": "Custom Namespaces"
        },
        {
            "location": "/configuration/node_affinity/",
            "text": "Node Affinity & Cluster Topology\n\n\nNode Affinity\n\n\nKubernetes allows pods to be assigned to nodes based on various critera through \nnode affinity\n.\n\n\nM3DB was built with failure tolerance as a core feature. M3DB's \nisolation groups\n allow shards to be\nplaced across failure domains such that the loss of no single domain can cause the cluster to lose quorum. More details\non M3DB's resiliency can be found in the \ndeployment docs\n.\n\n\nBy leveraging Kubernetes' node affinity and M3DB's isolation groups, the operator can guarantee that M3DB pods are\ndistributed across failure domains. For example, in a Kubernetes cluster spread across 3 zones in a cloud region, the\n\nisolationGroups\n configuration below would guarantee that no single zone failure could degrade the M3DB cluster.\n\n\nM3DB is unaware of the underlying zone topology: it just views the isolation groups as \ngroup1\n, \ngroup2\n, \ngroup3\n in\nits \nplacement\n. Thanks to the Kubernetes scheduler, however, these groups are actually scheduled across\nseparate failure domains.\n\n\napiVersion\n:\n \noperator.m3db.io/v1alpha1\n\n\nkind\n:\n \nM3DBCluster\n\n\n...\n\n\nspec\n:\n\n  \nreplicationFactor\n:\n \n3\n\n  \nisolationGroups\n:\n\n  \n-\n \nname\n:\n \ngroup1\n\n    \nnumInstances\n:\n \n3\n\n    \nnodeAffinityTerms\n:\n\n    \n-\n \nkey\n:\n \nfailure-domain.beta.kubernetes.io/zone\n\n      \nvalues\n:\n\n      \n-\n \nus-east1-b\n\n  \n-\n \nname\n:\n \ngroup2\n\n    \nnumInstances\n:\n \n3\n\n    \nnodeAffinityTerms\n:\n\n    \n-\n \nkey\n:\n \nfailure-domain.beta.kubernetes.io/zone\n\n      \nvalues\n:\n\n      \n-\n \nus-east1-c\n\n  \n-\n \nname\n:\n \ngroup3\n\n    \nnumInstances\n:\n \n3\n\n    \nnodeAffinityTerms\n:\n\n    \n-\n \nkey\n:\n \nfailure-domain.beta.kubernetes.io/zone\n\n      \nvalues\n:\n\n      \n-\n \nus-east1-d\n\n\n\n\n\nTolerations\n\n\nIn addition to allowing pods to be assigned to certain nodes via node affinity, Kubernetes allows pods to be \nrepelled\n\nfrom nodes through \ntaints\n if they don't tolerate the taint. For example, the following config would ensure:\n\n\n\n\n\n\nPods are spread across zones.\n\n\n\n\n\n\nPods are only assigned to nodes in the \nm3db-dedicated-pool\n pool.\n\n\n\n\n\n\nNo other pods could be assigned to those nodes (assuming they were tainted with the taint \nm3db-dedicated-taint\n).\n\n\n\n\n\n\napiVersion\n:\n \noperator.m3db.io/v1alpha1\n\n\nkind\n:\n \nM3DBCluster\n\n\n...\n\n\nspec\n:\n\n  \nreplicationFactor\n:\n \n3\n\n  \nisolationGroups\n:\n\n  \n-\n \nname\n:\n \ngroup1\n\n    \nnumInstances\n:\n \n3\n\n    \nnodeAffinityTerms\n:\n\n    \n-\n \nkey\n:\n \nfailure-domain.beta.kubernetes.io/zone\n\n      \nvalues\n:\n\n      \n-\n \nus-east1-b\n\n    \n-\n \nkey\n:\n \nnodepool\n\n      \nvalues\n:\n\n      \n-\n \nm3db-dedicated-pool\n\n  \n-\n \nname\n:\n \ngroup2\n\n    \nnumInstances\n:\n \n3\n\n    \nnodeAffinityTerms\n:\n\n    \n-\n \nkey\n:\n \nfailure-domain.beta.kubernetes.io/zone\n\n      \nvalues\n:\n\n      \n-\n \nus-east1-c\n\n    \n-\n \nkey\n:\n \nnodepool\n\n      \nvalues\n:\n\n      \n-\n \nm3db-dedicated-pool\n\n  \n-\n \nname\n:\n \ngroup3\n\n    \nnumInstances\n:\n \n3\n\n    \nnodeAffinityTerms\n:\n\n    \n-\n \nkey\n:\n \nfailure-domain.beta.kubernetes.io/zone\n\n      \nvalues\n:\n\n      \n-\n \nus-east1-d\n\n    \n-\n \nkey\n:\n \nnodepool\n\n      \nvalues\n:\n\n      \n-\n \nm3db-dedicated-pool\n\n  \ntolerations\n:\n\n  \n-\n \nkey\n:\n \nm3db-dedicated\n\n    \neffect\n:\n \nNoSchedule\n\n    \noperator\n:\n \nExists\n\n\n\n\n\nExample Affinity Configurations\n\n\nZonal Cluster\n\n\nThe examples so far have focused on multi-zone Kubernetes clusters. Some users may only have a cluster in a single zone\nand accept the reduced fault tolerance. The following configuration shows how to configure the operator in a zonal\ncluster.\n\n\napiVersion\n:\n \noperator.m3db.io/v1alpha1\n\n\nkind\n:\n \nM3DBCluster\n\n\n...\n\n\nspec\n:\n\n  \nreplicationFactor\n:\n \n3\n\n  \nisolationGroups\n:\n\n  \n-\n \nname\n:\n \ngroup1\n\n    \nnumInstances\n:\n \n3\n\n    \nnodeAffinityTerms\n:\n\n    \n-\n \nkey\n:\n \nfailure-domain.beta.kubernetes.io/zone\n\n      \nvalues\n:\n\n      \n-\n \nus-east1-b\n\n  \n-\n \nname\n:\n \ngroup2\n\n    \nnumInstances\n:\n \n3\n\n    \nnodeAffinityTerms\n:\n\n    \n-\n \nkey\n:\n \nfailure-domain.beta.kubernetes.io/zone\n\n      \nvalues\n:\n\n      \n-\n \nus-east1-b\n\n  \n-\n \nname\n:\n \ngroup3\n\n    \nnumInstances\n:\n \n3\n\n    \nnodeAffinityTerms\n:\n\n    \n-\n \nkey\n:\n \nfailure-domain.beta.kubernetes.io/zone\n\n      \nvalues\n:\n\n      \n-\n \nus-east1-b\n\n\n\n\n\n6 Zone Cluster\n\n\nIn the above examples we created clusters with 1 isolation group in each of 3 zones. Because \nvalues\n within a single\n\nNodeAffinityTerm\n are OR'd, we can also spread an isolationgroup across multiple zones. For\nexample, if we had 6 zones available to us:\n\n\napiVersion\n:\n \noperator.m3db.io/v1alpha1\n\n\nkind\n:\n \nM3DBCluster\n\n\n...\n\n\nspec\n:\n\n  \nreplicationFactor\n:\n \n3\n\n  \nisolationGroups\n:\n\n  \n-\n \nname\n:\n \ngroup1\n\n    \nnumInstances\n:\n \n3\n\n    \nnodeAffinityTerms\n:\n\n    \n-\n \nkey\n:\n \nfailure-domain.beta.kubernetes.io/zone\n\n      \nvalues\n:\n\n      \n-\n \nus-east1-a\n\n      \n-\n \nus-east1-b\n\n  \n-\n \nname\n:\n \ngroup2\n\n    \nnumInstances\n:\n \n3\n\n    \nnodeAffinityTerms\n:\n\n    \n-\n \nkey\n:\n \nfailure-domain.beta.kubernetes.io/zone\n\n      \nvalues\n:\n\n      \n-\n \nus-east1-c\n\n      \n-\n \nus-east1-d\n\n  \n-\n \nname\n:\n \ngroup3\n\n    \nnumInstances\n:\n \n3\n\n    \nnodeAffinityTerms\n:\n\n    \n-\n \nkey\n:\n \nfailure-domain.beta.kubernetes.io/zone\n\n      \nvalues\n:\n\n      \n-\n \nus-east1-e\n\n      \n-\n \nus-east1-f\n\n\n\n\n\nNo Affinity\n\n\nIf there are no failure domains available, one can have a cluster with no affinity where the pods will be scheduled however Kubernetes would place them by default:\n\n\napiVersion\n:\n \noperator.m3db.io/v1alpha1\n\n\nkind\n:\n \nM3DBCluster\n\n\n...\n\n\nspec\n:\n\n  \nreplicationFactor\n:\n \n3\n\n  \nisolationGroups\n:\n\n  \n-\n \nname\n:\n \ngroup1\n\n    \nnumInstances\n:\n \n3\n\n  \n-\n \nname\n:\n \ngroup2\n\n    \nnumInstances\n:\n \n3\n\n  \n-\n \nname\n:\n \ngroup3\n\n    \nnumInstances\n:\n \n3",
            "title": "Node Affinity & Cluster Topology"
        },
        {
            "location": "/configuration/node_affinity/#node-affinity-cluster-topology",
            "text": "",
            "title": "Node Affinity &amp; Cluster Topology"
        },
        {
            "location": "/configuration/node_affinity/#node-affinity",
            "text": "Kubernetes allows pods to be assigned to nodes based on various critera through  node affinity .  M3DB was built with failure tolerance as a core feature. M3DB's  isolation groups  allow shards to be\nplaced across failure domains such that the loss of no single domain can cause the cluster to lose quorum. More details\non M3DB's resiliency can be found in the  deployment docs .  By leveraging Kubernetes' node affinity and M3DB's isolation groups, the operator can guarantee that M3DB pods are\ndistributed across failure domains. For example, in a Kubernetes cluster spread across 3 zones in a cloud region, the isolationGroups  configuration below would guarantee that no single zone failure could degrade the M3DB cluster.  M3DB is unaware of the underlying zone topology: it just views the isolation groups as  group1 ,  group2 ,  group3  in\nits  placement . Thanks to the Kubernetes scheduler, however, these groups are actually scheduled across\nseparate failure domains.  apiVersion :   operator.m3db.io/v1alpha1  kind :   M3DBCluster  ...  spec : \n   replicationFactor :   3 \n   isolationGroups : \n   -   name :   group1 \n     numInstances :   3 \n     nodeAffinityTerms : \n     -   key :   failure-domain.beta.kubernetes.io/zone \n       values : \n       -   us-east1-b \n   -   name :   group2 \n     numInstances :   3 \n     nodeAffinityTerms : \n     -   key :   failure-domain.beta.kubernetes.io/zone \n       values : \n       -   us-east1-c \n   -   name :   group3 \n     numInstances :   3 \n     nodeAffinityTerms : \n     -   key :   failure-domain.beta.kubernetes.io/zone \n       values : \n       -   us-east1-d",
            "title": "Node Affinity"
        },
        {
            "location": "/configuration/node_affinity/#tolerations",
            "text": "In addition to allowing pods to be assigned to certain nodes via node affinity, Kubernetes allows pods to be  repelled \nfrom nodes through  taints  if they don't tolerate the taint. For example, the following config would ensure:    Pods are spread across zones.    Pods are only assigned to nodes in the  m3db-dedicated-pool  pool.    No other pods could be assigned to those nodes (assuming they were tainted with the taint  m3db-dedicated-taint ).    apiVersion :   operator.m3db.io/v1alpha1  kind :   M3DBCluster  ...  spec : \n   replicationFactor :   3 \n   isolationGroups : \n   -   name :   group1 \n     numInstances :   3 \n     nodeAffinityTerms : \n     -   key :   failure-domain.beta.kubernetes.io/zone \n       values : \n       -   us-east1-b \n     -   key :   nodepool \n       values : \n       -   m3db-dedicated-pool \n   -   name :   group2 \n     numInstances :   3 \n     nodeAffinityTerms : \n     -   key :   failure-domain.beta.kubernetes.io/zone \n       values : \n       -   us-east1-c \n     -   key :   nodepool \n       values : \n       -   m3db-dedicated-pool \n   -   name :   group3 \n     numInstances :   3 \n     nodeAffinityTerms : \n     -   key :   failure-domain.beta.kubernetes.io/zone \n       values : \n       -   us-east1-d \n     -   key :   nodepool \n       values : \n       -   m3db-dedicated-pool \n   tolerations : \n   -   key :   m3db-dedicated \n     effect :   NoSchedule \n     operator :   Exists",
            "title": "Tolerations"
        },
        {
            "location": "/configuration/node_affinity/#example-affinity-configurations",
            "text": "",
            "title": "Example Affinity Configurations"
        },
        {
            "location": "/configuration/node_affinity/#zonal-cluster",
            "text": "The examples so far have focused on multi-zone Kubernetes clusters. Some users may only have a cluster in a single zone\nand accept the reduced fault tolerance. The following configuration shows how to configure the operator in a zonal\ncluster.  apiVersion :   operator.m3db.io/v1alpha1  kind :   M3DBCluster  ...  spec : \n   replicationFactor :   3 \n   isolationGroups : \n   -   name :   group1 \n     numInstances :   3 \n     nodeAffinityTerms : \n     -   key :   failure-domain.beta.kubernetes.io/zone \n       values : \n       -   us-east1-b \n   -   name :   group2 \n     numInstances :   3 \n     nodeAffinityTerms : \n     -   key :   failure-domain.beta.kubernetes.io/zone \n       values : \n       -   us-east1-b \n   -   name :   group3 \n     numInstances :   3 \n     nodeAffinityTerms : \n     -   key :   failure-domain.beta.kubernetes.io/zone \n       values : \n       -   us-east1-b",
            "title": "Zonal Cluster"
        },
        {
            "location": "/configuration/node_affinity/#6-zone-cluster",
            "text": "In the above examples we created clusters with 1 isolation group in each of 3 zones. Because  values  within a single NodeAffinityTerm  are OR'd, we can also spread an isolationgroup across multiple zones. For\nexample, if we had 6 zones available to us:  apiVersion :   operator.m3db.io/v1alpha1  kind :   M3DBCluster  ...  spec : \n   replicationFactor :   3 \n   isolationGroups : \n   -   name :   group1 \n     numInstances :   3 \n     nodeAffinityTerms : \n     -   key :   failure-domain.beta.kubernetes.io/zone \n       values : \n       -   us-east1-a \n       -   us-east1-b \n   -   name :   group2 \n     numInstances :   3 \n     nodeAffinityTerms : \n     -   key :   failure-domain.beta.kubernetes.io/zone \n       values : \n       -   us-east1-c \n       -   us-east1-d \n   -   name :   group3 \n     numInstances :   3 \n     nodeAffinityTerms : \n     -   key :   failure-domain.beta.kubernetes.io/zone \n       values : \n       -   us-east1-e \n       -   us-east1-f",
            "title": "6 Zone Cluster"
        },
        {
            "location": "/configuration/node_affinity/#no-affinity",
            "text": "If there are no failure domains available, one can have a cluster with no affinity where the pods will be scheduled however Kubernetes would place them by default:  apiVersion :   operator.m3db.io/v1alpha1  kind :   M3DBCluster  ...  spec : \n   replicationFactor :   3 \n   isolationGroups : \n   -   name :   group1 \n     numInstances :   3 \n   -   name :   group2 \n     numInstances :   3 \n   -   name :   group3 \n     numInstances :   3",
            "title": "No Affinity"
        },
        {
            "location": "/configuration/node_endpoint/",
            "text": "Node Endpoint\n\n\nM3DB stores an \nendpoint\n field on placement instances that is used for communication between DB nodes and from\nother components such as the coordinator.\n\n\nThe operator allows customizing the format of this endpoint by setting the \nnodeEndpointFormat\n field on a cluster spec.\nThe format of this field uses \nGo templates\n, with the following template fields currently supported:\n\n\n\n\n\n\n\n\nField\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nPodName\n\n\nName of the pod\n\n\n\n\n\n\nM3DBService\n\n\nName of the generated M3DB service\n\n\n\n\n\n\nPodNamespace\n\n\nNamespace the pod is in\n\n\n\n\n\n\nPort\n\n\nPort M3DB is serving RPCs on\n\n\n\n\n\n\n\n\nThe default format is:\n\n{{ .PodName }}.{{ .M3DBService }}:{{ .Port }}\n\n\n\nAs an example of an override, to expose an M3DB cluster to containers in other Kubernetes namespaces \nnodeEndpointFormat\n can be set to:\n\n{{ .PodName }}.{{ .M3DBService }}.{{ .PodNamespace }}:{{ .Port }}",
            "title": "Node Endpoint"
        },
        {
            "location": "/configuration/node_endpoint/#node-endpoint",
            "text": "M3DB stores an  endpoint  field on placement instances that is used for communication between DB nodes and from\nother components such as the coordinator.  The operator allows customizing the format of this endpoint by setting the  nodeEndpointFormat  field on a cluster spec.\nThe format of this field uses  Go templates , with the following template fields currently supported:     Field  Description      PodName  Name of the pod    M3DBService  Name of the generated M3DB service    PodNamespace  Namespace the pod is in    Port  Port M3DB is serving RPCs on     The default format is: {{ .PodName }}.{{ .M3DBService }}:{{ .Port }}  As an example of an override, to expose an M3DB cluster to containers in other Kubernetes namespaces  nodeEndpointFormat  can be set to: {{ .PodName }}.{{ .M3DBService }}.{{ .PodNamespace }}:{{ .Port }}",
            "title": "Node Endpoint"
        },
        {
            "location": "/api/",
            "text": "API Docs\n\n\nThis document enumerates the Custom Resource Definitions used by the M3DB Operator. It is auto-generated from code comments.\n\n\nTable of Contents\n\n\n\n\nClusterCondition\n\n\nClusterSpec\n\n\nIsolationGroup\n\n\nM3DBCluster\n\n\nM3DBClusterList\n\n\nM3DBStatus\n\n\nNodeAffinityTerm\n\n\nIndexOptions\n\n\nNamespace\n\n\nNamespaceOptions\n\n\nRetentionOptions\n\n\nPodIdentity\n\n\nPodIdentityConfig\n\n\n\n\nClusterCondition\n\n\nClusterCondition represents various conditions the cluster can be in.\n\n\n\n\n\n\n\n\nField\n\n\nDescription\n\n\nScheme\n\n\nRequired\n\n\n\n\n\n\n\n\n\n\ntype\n\n\nType of cluster condition.\n\n\nClusterConditionType\n\n\nfalse\n\n\n\n\n\n\nstatus\n\n\nStatus of the condition (True, False, Unknown).\n\n\ncorev1.ConditionStatus\n\n\nfalse\n\n\n\n\n\n\nlastUpdateTime\n\n\nLast time this condition was updated.\n\n\nstring\n\n\nfalse\n\n\n\n\n\n\nlastTransitionTime\n\n\nLast time this condition transitioned from one status to another.\n\n\nstring\n\n\nfalse\n\n\n\n\n\n\nreason\n\n\nReason this condition last changed.\n\n\nstring\n\n\nfalse\n\n\n\n\n\n\nmessage\n\n\nHuman-friendly message about this condition.\n\n\nstring\n\n\nfalse\n\n\n\n\n\n\n\n\nBack to TOC\n\n\nClusterSpec\n\n\nClusterSpec defines the desired state for a M3 cluster to be converge to.\n\n\n\n\n\n\n\n\nField\n\n\nDescription\n\n\nScheme\n\n\nRequired\n\n\n\n\n\n\n\n\n\n\nimage\n\n\nImage specifies which docker image to use with the cluster\n\n\nstring\n\n\nfalse\n\n\n\n\n\n\nreplicationFactor\n\n\nReplicationFactor defines how many replicas\n\n\nint32\n\n\nfalse\n\n\n\n\n\n\nnumberOfShards\n\n\nNumberOfShards defines how many shards in total\n\n\nint32\n\n\nfalse\n\n\n\n\n\n\nisolationGroups\n\n\nIsolationGroups specifies a map of key-value pairs. Defines which isolation groups to deploy persistent volumes for data nodes\n\n\n[]\nIsolationGroup\n\n\nfalse\n\n\n\n\n\n\nnamespaces\n\n\nNamespaces specifies the namespaces this cluster will hold.\n\n\n[]\nNamespace\n\n\nfalse\n\n\n\n\n\n\netcdEndpoints\n\n\nEtcdEndpoints defines the etcd endpoints to use for service discovery. Must be set if no custom configmap is defined. If set, etcd endpoints will be templated in to the default configmap template.\n\n\n[]string\n\n\nfalse\n\n\n\n\n\n\nkeepEtcdDataOnDelete\n\n\nKeepEtcdDataOnDelete determines whether the operator will remove cluster metadata (placement + namespaces) in etcd when the cluster is deleted. Unless true, etcd data will be cleared when the cluster is deleted.\n\n\nbool\n\n\nfalse\n\n\n\n\n\n\nenableCarbonIngester\n\n\nEnableCarbonIngester enables the listener port for the carbon ingester\n\n\nbool\n\n\nfalse\n\n\n\n\n\n\nconfigMapName\n\n\nConfigMapName specifies the ConfigMap to use for this cluster. If unset a default configmap with template variables for etcd endpoints will be used. See \\\"Configuring M3DB\\\" in the docs for more.\n\n\n*string\n\n\nfalse\n\n\n\n\n\n\npodIdentityConfig\n\n\nPodIdentityConfig sets the configuration for pod identity. If unset only pod name and UID will be used.\n\n\n*PodIdentityConfig\n\n\nfalse\n\n\n\n\n\n\ncontainerResources\n\n\nResources defines memory / cpu constraints for each container in the cluster.\n\n\ncorev1.ResourceRequirements\n\n\nfalse\n\n\n\n\n\n\ndataDirVolumeClaimTemplate\n\n\nDataDirVolumeClaimTemplate is the volume claim template for an M3DB instance's data. It claims PersistentVolumes for cluster storage, volumes are dynamically provisioned by when the StorageClass is defined.\n\n\n*\ncorev1.PersistentVolumeClaim\n\n\nfalse\n\n\n\n\n\n\npodSecurityContext\n\n\nPodSecurityContext allows the user to specify an optional security context for pods.\n\n\n*corev1.PodSecurityContext\n\n\nfalse\n\n\n\n\n\n\nsecurityContext\n\n\nSecurityContext allows the user to specify a container-level security context.\n\n\n*corev1.SecurityContext\n\n\nfalse\n\n\n\n\n\n\nimagePullSecrets\n\n\nImagePullSecrets will be added to every pod.\n\n\n[]\ncorev1.LocalObjectReference\n\n\nfalse\n\n\n\n\n\n\nenvVars\n\n\nEnvVars defines custom environment variables to be passed to M3DB containers.\n\n\n[]corev1.EnvVar\n\n\nfalse\n\n\n\n\n\n\nlabels\n\n\nLabels sets the base labels that will be applied to resources created by the cluster. // TODO(schallert): design doc on labeling scheme.\n\n\nmap[string]string\n\n\nfalse\n\n\n\n\n\n\nannotations\n\n\nAnnotations sets the base annotations that will be applied to resources created by the cluster.\n\n\nmap[string]string\n\n\nfalse\n\n\n\n\n\n\ntolerations\n\n\nTolerations sets the tolerations that will be applied to all M3DB pods.\n\n\n[]corev1.Toleration\n\n\nfalse\n\n\n\n\n\n\npriorityClassName\n\n\nPriorityClassName sets the priority class for all M3DB pods.\n\n\nstring\n\n\nfalse\n\n\n\n\n\n\nnodeEndpointFormat\n\n\nNodeEndpointFormat allows overriding of the endpoint used for a node in the M3DB placement. Defaults to \\\"{{ .PodName }}.{{ .M3DBService }}:{{ .Port }}\\\". Useful if access to the cluster from other namespaces is desired. See \\\"Node Endpoint\\\" docs for full variables available.\n\n\nstring\n\n\nfalse\n\n\n\n\n\n\nhostNetwork\n\n\nHostNetwork indicates whether M3DB pods should run in the same network namespace as the node its on. This option should be used sparingly due to security concerns outlined in the linked documentation. \nhttps://kubernetes.io/docs/concepts/policy/pod-security-policy/#host-namespaces\n\n\nbool\n\n\nfalse\n\n\n\n\n\n\ndnsPolicy\n\n\nDNSPolicy allows the user to set the pod's DNSPolicy. This is often used in conjunction with HostNetwork.+optional\n\n\n*corev1.DNSPolicy\n\n\nfalse\n\n\n\n\n\n\n\n\nBack to TOC\n\n\nIsolationGroup\n\n\nIsolationGroup defines the name of zone as well attributes for the zone configuration\n\n\n\n\n\n\n\n\nField\n\n\nDescription\n\n\nScheme\n\n\nRequired\n\n\n\n\n\n\n\n\n\n\nname\n\n\nName is the value that will be used in StatefulSet labels, pod labels, and M3DB placement \\\"isolationGroup\\\" fields.\n\n\nstring\n\n\ntrue\n\n\n\n\n\n\nnodeAffinityTerms\n\n\nNodeAffinityTerms is an array of NodeAffinityTerm requirements, which are ANDed together to indicate what nodes an isolation group can be assigned to.\n\n\n[]\nNodeAffinityTerm\n\n\nfalse\n\n\n\n\n\n\nnumInstances\n\n\nNumInstances defines the number of instances.\n\n\nint32\n\n\ntrue\n\n\n\n\n\n\nstorageClassName\n\n\nStorageClassName is the name of the StorageClass to use for this isolation group. This allows ensuring that PVs will be created in the same zone as the pinned statefulset on Kubernetes < 1.12 (when topology aware volume scheduling was introduced). Only has effect if the clusters \ndataDirVolumeClaimTemplate\n is non-nil. If set, the volume claim template will have its storageClassName field overridden per-isolationgroup. If unset the storageClassName of the volumeClaimTemplate will be used.\n\n\nstring\n\n\nfalse\n\n\n\n\n\n\n\n\nBack to TOC\n\n\nM3DBCluster\n\n\nM3DBCluster defines the cluster\n\n\n\n\n\n\n\n\nField\n\n\nDescription\n\n\nScheme\n\n\nRequired\n\n\n\n\n\n\n\n\n\n\nmetadata\n\n\n\n\nmetav1.ObjectMeta\n\n\nfalse\n\n\n\n\n\n\ntype\n\n\n\n\nstring\n\n\ntrue\n\n\n\n\n\n\nspec\n\n\n\n\nClusterSpec\n\n\ntrue\n\n\n\n\n\n\nstatus\n\n\n\n\nM3DBStatus\n\n\nfalse\n\n\n\n\n\n\n\n\nBack to TOC\n\n\nM3DBClusterList\n\n\nM3DBClusterList represents a list of M3DB Clusters\n\n\n\n\n\n\n\n\nField\n\n\nDescription\n\n\nScheme\n\n\nRequired\n\n\n\n\n\n\n\n\n\n\nmetadata\n\n\n\n\nmetav1.ListMeta\n\n\nfalse\n\n\n\n\n\n\nitems\n\n\n\n\n[]\nM3DBCluster\n\n\ntrue\n\n\n\n\n\n\n\n\nBack to TOC\n\n\nM3DBStatus\n\n\nM3DBStatus contains the current state the M3DB cluster along with a human readable message\n\n\n\n\n\n\n\n\nField\n\n\nDescription\n\n\nScheme\n\n\nRequired\n\n\n\n\n\n\n\n\n\n\nstate\n\n\nState is a enum of green, yellow, and red denoting the health of the cluster\n\n\nM3DBState\n\n\nfalse\n\n\n\n\n\n\nconditions\n\n\nVarious conditions about the cluster.\n\n\n[]\nClusterCondition\n\n\nfalse\n\n\n\n\n\n\nmessage\n\n\nMessage is a human readable message indicating why the cluster is in it's current state\n\n\nstring\n\n\nfalse\n\n\n\n\n\n\nobservedGeneration\n\n\nObservedGeneration is the last generation of the cluster the controller observed. Kubernetes will automatically increment metadata.Generation every time the cluster spec is changed.\n\n\nint64\n\n\nfalse\n\n\n\n\n\n\n\n\nBack to TOC\n\n\nNodeAffinityTerm\n\n\nNodeAffinityTerm represents a node label and a set of label values, any of which can be matched to assign a pod to a node.\n\n\n\n\n\n\n\n\nField\n\n\nDescription\n\n\nScheme\n\n\nRequired\n\n\n\n\n\n\n\n\n\n\nkey\n\n\nKey is the label of the node.\n\n\nstring\n\n\ntrue\n\n\n\n\n\n\nvalues\n\n\nValues is an array of values, any of which a node can have for a pod to be assigned to it.\n\n\n[]string\n\n\ntrue\n\n\n\n\n\n\n\n\nBack to TOC\n\n\nIndexOptions\n\n\nIndexOptions defines parameters for indexing.\n\n\n\n\n\n\n\n\nField\n\n\nDescription\n\n\nScheme\n\n\nRequired\n\n\n\n\n\n\n\n\n\n\nenabled\n\n\nEnabled controls whether metric indexing is enabled.\n\n\nbool\n\n\nfalse\n\n\n\n\n\n\nblockSize\n\n\nBlockSize controls the index block size.\n\n\nstring\n\n\nfalse\n\n\n\n\n\n\n\n\nBack to TOC\n\n\nNamespace\n\n\nNamespace defines an M3DB namespace or points to a preset M3DB namespace.\n\n\n\n\n\n\n\n\nField\n\n\nDescription\n\n\nScheme\n\n\nRequired\n\n\n\n\n\n\n\n\n\n\nname\n\n\nName is the namespace name.\n\n\nstring\n\n\nfalse\n\n\n\n\n\n\npreset\n\n\nPreset indicates preset namespace options.\n\n\nstring\n\n\nfalse\n\n\n\n\n\n\noptions\n\n\nOptions points to optional custom namespace configuration.\n\n\n*\nNamespaceOptions\n\n\nfalse\n\n\n\n\n\n\n\n\nBack to TOC\n\n\nNamespaceOptions\n\n\nNamespaceOptions defines parameters for an M3DB namespace. See \nhttps://m3db.github.io/m3/operational_guide/namespace_configuration/\n for more details.\n\n\n\n\n\n\n\n\nField\n\n\nDescription\n\n\nScheme\n\n\nRequired\n\n\n\n\n\n\n\n\n\n\nbootstrapEnabled\n\n\nBootstrapEnabled control if bootstrapping is enabled.\n\n\nbool\n\n\nfalse\n\n\n\n\n\n\nflushEnabled\n\n\nFlushEnabled controls whether flushing is enabled.\n\n\nbool\n\n\nfalse\n\n\n\n\n\n\nwritesToCommitLog\n\n\nWritesToCommitLog controls whether commit log writes are enabled.\n\n\nbool\n\n\nfalse\n\n\n\n\n\n\ncleanupEnabled\n\n\nCleanupEnabled controls whether cleanups are enabled.\n\n\nbool\n\n\nfalse\n\n\n\n\n\n\nrepairEnabled\n\n\nRepairEnabled controls whether repairs are enabled.\n\n\nbool\n\n\nfalse\n\n\n\n\n\n\nsnapshotEnabled\n\n\nSnapshotEnabled controls whether snapshotting is enabled.\n\n\nbool\n\n\nfalse\n\n\n\n\n\n\nretentionOptions\n\n\nRetentionOptions sets the retention parameters.\n\n\nRetentionOptions\n\n\nfalse\n\n\n\n\n\n\nindexOptions\n\n\nIndexOptions sets the indexing parameters.\n\n\nIndexOptions\n\n\nfalse\n\n\n\n\n\n\n\n\nBack to TOC\n\n\nRetentionOptions\n\n\nRetentionOptions defines parameters for data retention.\n\n\n\n\n\n\n\n\nField\n\n\nDescription\n\n\nScheme\n\n\nRequired\n\n\n\n\n\n\n\n\n\n\nretentionPeriod\n\n\nRetentionPeriod controls how long data for the namespace is retained.\n\n\nstring\n\n\nfalse\n\n\n\n\n\n\nblockSize\n\n\nBlockSize controls the block size for the namespace.\n\n\nstring\n\n\nfalse\n\n\n\n\n\n\nbufferFuture\n\n\nBufferFuture controls how far in the future metrics can be written.\n\n\nstring\n\n\nfalse\n\n\n\n\n\n\nbufferPast\n\n\nBufferPast controls how far in the past metrics can be written.\n\n\nstring\n\n\nfalse\n\n\n\n\n\n\nblockDataExpiry\n\n\nBlockDataExpiry controls the block expiry.\n\n\nbool\n\n\nfalse\n\n\n\n\n\n\nblockDataExpiryAfterNotAccessPeriod\n\n\nBlockDataExpiry controls the not after access period for expiration.\n\n\nstring\n\n\nfalse\n\n\n\n\n\n\n\n\nBack to TOC\n\n\nPodIdentity\n\n\nPodIdentity contains all the fields that may be used to identify a pod's identity in the M3DB placement. Any non-empty fields will be used to identity uniqueness of a pod for the purpose of M3DB replace operations.\n\n\n\n\n\n\n\n\nField\n\n\nDescription\n\n\nScheme\n\n\nRequired\n\n\n\n\n\n\n\n\n\n\nname\n\n\n\n\nstring\n\n\nfalse\n\n\n\n\n\n\nuid\n\n\n\n\nstring\n\n\nfalse\n\n\n\n\n\n\nnodeName\n\n\n\n\nstring\n\n\nfalse\n\n\n\n\n\n\nnodeExternalID\n\n\n\n\nstring\n\n\nfalse\n\n\n\n\n\n\nnodeProviderID\n\n\n\n\nstring\n\n\nfalse\n\n\n\n\n\n\n\n\nBack to TOC\n\n\nPodIdentityConfig\n\n\nPodIdentityConfig contains cluster-level configuration for deriving pod identity.\n\n\n\n\n\n\n\n\nField\n\n\nDescription\n\n\nScheme\n\n\nRequired\n\n\n\n\n\n\n\n\n\n\nsources\n\n\nSources enumerates the sources from which to derive pod identity. Note that a pod's name will always be used. If empty, defaults to pod name and UID.\n\n\n[]PodIdentitySource\n\n\ntrue\n\n\n\n\n\n\n\n\nBack to TOC",
            "title": "API"
        },
        {
            "location": "/api/#api-docs",
            "text": "This document enumerates the Custom Resource Definitions used by the M3DB Operator. It is auto-generated from code comments.",
            "title": "API Docs"
        },
        {
            "location": "/api/#table-of-contents",
            "text": "ClusterCondition  ClusterSpec  IsolationGroup  M3DBCluster  M3DBClusterList  M3DBStatus  NodeAffinityTerm  IndexOptions  Namespace  NamespaceOptions  RetentionOptions  PodIdentity  PodIdentityConfig",
            "title": "Table of Contents"
        },
        {
            "location": "/api/#clustercondition",
            "text": "ClusterCondition represents various conditions the cluster can be in.     Field  Description  Scheme  Required      type  Type of cluster condition.  ClusterConditionType  false    status  Status of the condition (True, False, Unknown).  corev1.ConditionStatus  false    lastUpdateTime  Last time this condition was updated.  string  false    lastTransitionTime  Last time this condition transitioned from one status to another.  string  false    reason  Reason this condition last changed.  string  false    message  Human-friendly message about this condition.  string  false     Back to TOC",
            "title": "ClusterCondition"
        },
        {
            "location": "/api/#clusterspec",
            "text": "ClusterSpec defines the desired state for a M3 cluster to be converge to.     Field  Description  Scheme  Required      image  Image specifies which docker image to use with the cluster  string  false    replicationFactor  ReplicationFactor defines how many replicas  int32  false    numberOfShards  NumberOfShards defines how many shards in total  int32  false    isolationGroups  IsolationGroups specifies a map of key-value pairs. Defines which isolation groups to deploy persistent volumes for data nodes  [] IsolationGroup  false    namespaces  Namespaces specifies the namespaces this cluster will hold.  [] Namespace  false    etcdEndpoints  EtcdEndpoints defines the etcd endpoints to use for service discovery. Must be set if no custom configmap is defined. If set, etcd endpoints will be templated in to the default configmap template.  []string  false    keepEtcdDataOnDelete  KeepEtcdDataOnDelete determines whether the operator will remove cluster metadata (placement + namespaces) in etcd when the cluster is deleted. Unless true, etcd data will be cleared when the cluster is deleted.  bool  false    enableCarbonIngester  EnableCarbonIngester enables the listener port for the carbon ingester  bool  false    configMapName  ConfigMapName specifies the ConfigMap to use for this cluster. If unset a default configmap with template variables for etcd endpoints will be used. See \\\"Configuring M3DB\\\" in the docs for more.  *string  false    podIdentityConfig  PodIdentityConfig sets the configuration for pod identity. If unset only pod name and UID will be used.  *PodIdentityConfig  false    containerResources  Resources defines memory / cpu constraints for each container in the cluster.  corev1.ResourceRequirements  false    dataDirVolumeClaimTemplate  DataDirVolumeClaimTemplate is the volume claim template for an M3DB instance's data. It claims PersistentVolumes for cluster storage, volumes are dynamically provisioned by when the StorageClass is defined.  * corev1.PersistentVolumeClaim  false    podSecurityContext  PodSecurityContext allows the user to specify an optional security context for pods.  *corev1.PodSecurityContext  false    securityContext  SecurityContext allows the user to specify a container-level security context.  *corev1.SecurityContext  false    imagePullSecrets  ImagePullSecrets will be added to every pod.  [] corev1.LocalObjectReference  false    envVars  EnvVars defines custom environment variables to be passed to M3DB containers.  []corev1.EnvVar  false    labels  Labels sets the base labels that will be applied to resources created by the cluster. // TODO(schallert): design doc on labeling scheme.  map[string]string  false    annotations  Annotations sets the base annotations that will be applied to resources created by the cluster.  map[string]string  false    tolerations  Tolerations sets the tolerations that will be applied to all M3DB pods.  []corev1.Toleration  false    priorityClassName  PriorityClassName sets the priority class for all M3DB pods.  string  false    nodeEndpointFormat  NodeEndpointFormat allows overriding of the endpoint used for a node in the M3DB placement. Defaults to \\\"{{ .PodName }}.{{ .M3DBService }}:{{ .Port }}\\\". Useful if access to the cluster from other namespaces is desired. See \\\"Node Endpoint\\\" docs for full variables available.  string  false    hostNetwork  HostNetwork indicates whether M3DB pods should run in the same network namespace as the node its on. This option should be used sparingly due to security concerns outlined in the linked documentation.  https://kubernetes.io/docs/concepts/policy/pod-security-policy/#host-namespaces  bool  false    dnsPolicy  DNSPolicy allows the user to set the pod's DNSPolicy. This is often used in conjunction with HostNetwork.+optional  *corev1.DNSPolicy  false     Back to TOC",
            "title": "ClusterSpec"
        },
        {
            "location": "/api/#isolationgroup",
            "text": "IsolationGroup defines the name of zone as well attributes for the zone configuration     Field  Description  Scheme  Required      name  Name is the value that will be used in StatefulSet labels, pod labels, and M3DB placement \\\"isolationGroup\\\" fields.  string  true    nodeAffinityTerms  NodeAffinityTerms is an array of NodeAffinityTerm requirements, which are ANDed together to indicate what nodes an isolation group can be assigned to.  [] NodeAffinityTerm  false    numInstances  NumInstances defines the number of instances.  int32  true    storageClassName  StorageClassName is the name of the StorageClass to use for this isolation group. This allows ensuring that PVs will be created in the same zone as the pinned statefulset on Kubernetes < 1.12 (when topology aware volume scheduling was introduced). Only has effect if the clusters  dataDirVolumeClaimTemplate  is non-nil. If set, the volume claim template will have its storageClassName field overridden per-isolationgroup. If unset the storageClassName of the volumeClaimTemplate will be used.  string  false     Back to TOC",
            "title": "IsolationGroup"
        },
        {
            "location": "/api/#m3dbcluster",
            "text": "M3DBCluster defines the cluster     Field  Description  Scheme  Required      metadata   metav1.ObjectMeta  false    type   string  true    spec   ClusterSpec  true    status   M3DBStatus  false     Back to TOC",
            "title": "M3DBCluster"
        },
        {
            "location": "/api/#m3dbclusterlist",
            "text": "M3DBClusterList represents a list of M3DB Clusters     Field  Description  Scheme  Required      metadata   metav1.ListMeta  false    items   [] M3DBCluster  true     Back to TOC",
            "title": "M3DBClusterList"
        },
        {
            "location": "/api/#m3dbstatus",
            "text": "M3DBStatus contains the current state the M3DB cluster along with a human readable message     Field  Description  Scheme  Required      state  State is a enum of green, yellow, and red denoting the health of the cluster  M3DBState  false    conditions  Various conditions about the cluster.  [] ClusterCondition  false    message  Message is a human readable message indicating why the cluster is in it's current state  string  false    observedGeneration  ObservedGeneration is the last generation of the cluster the controller observed. Kubernetes will automatically increment metadata.Generation every time the cluster spec is changed.  int64  false     Back to TOC",
            "title": "M3DBStatus"
        },
        {
            "location": "/api/#nodeaffinityterm",
            "text": "NodeAffinityTerm represents a node label and a set of label values, any of which can be matched to assign a pod to a node.     Field  Description  Scheme  Required      key  Key is the label of the node.  string  true    values  Values is an array of values, any of which a node can have for a pod to be assigned to it.  []string  true     Back to TOC",
            "title": "NodeAffinityTerm"
        },
        {
            "location": "/api/#indexoptions",
            "text": "IndexOptions defines parameters for indexing.     Field  Description  Scheme  Required      enabled  Enabled controls whether metric indexing is enabled.  bool  false    blockSize  BlockSize controls the index block size.  string  false     Back to TOC",
            "title": "IndexOptions"
        },
        {
            "location": "/api/#namespace",
            "text": "Namespace defines an M3DB namespace or points to a preset M3DB namespace.     Field  Description  Scheme  Required      name  Name is the namespace name.  string  false    preset  Preset indicates preset namespace options.  string  false    options  Options points to optional custom namespace configuration.  * NamespaceOptions  false     Back to TOC",
            "title": "Namespace"
        },
        {
            "location": "/api/#namespaceoptions",
            "text": "NamespaceOptions defines parameters for an M3DB namespace. See  https://m3db.github.io/m3/operational_guide/namespace_configuration/  for more details.     Field  Description  Scheme  Required      bootstrapEnabled  BootstrapEnabled control if bootstrapping is enabled.  bool  false    flushEnabled  FlushEnabled controls whether flushing is enabled.  bool  false    writesToCommitLog  WritesToCommitLog controls whether commit log writes are enabled.  bool  false    cleanupEnabled  CleanupEnabled controls whether cleanups are enabled.  bool  false    repairEnabled  RepairEnabled controls whether repairs are enabled.  bool  false    snapshotEnabled  SnapshotEnabled controls whether snapshotting is enabled.  bool  false    retentionOptions  RetentionOptions sets the retention parameters.  RetentionOptions  false    indexOptions  IndexOptions sets the indexing parameters.  IndexOptions  false     Back to TOC",
            "title": "NamespaceOptions"
        },
        {
            "location": "/api/#retentionoptions",
            "text": "RetentionOptions defines parameters for data retention.     Field  Description  Scheme  Required      retentionPeriod  RetentionPeriod controls how long data for the namespace is retained.  string  false    blockSize  BlockSize controls the block size for the namespace.  string  false    bufferFuture  BufferFuture controls how far in the future metrics can be written.  string  false    bufferPast  BufferPast controls how far in the past metrics can be written.  string  false    blockDataExpiry  BlockDataExpiry controls the block expiry.  bool  false    blockDataExpiryAfterNotAccessPeriod  BlockDataExpiry controls the not after access period for expiration.  string  false     Back to TOC",
            "title": "RetentionOptions"
        },
        {
            "location": "/api/#podidentity",
            "text": "PodIdentity contains all the fields that may be used to identify a pod's identity in the M3DB placement. Any non-empty fields will be used to identity uniqueness of a pod for the purpose of M3DB replace operations.     Field  Description  Scheme  Required      name   string  false    uid   string  false    nodeName   string  false    nodeExternalID   string  false    nodeProviderID   string  false     Back to TOC",
            "title": "PodIdentity"
        },
        {
            "location": "/api/#podidentityconfig",
            "text": "PodIdentityConfig contains cluster-level configuration for deriving pod identity.     Field  Description  Scheme  Required      sources  Sources enumerates the sources from which to derive pod identity. Note that a pod's name will always be used. If empty, defaults to pod name and UID.  []PodIdentitySource  true     Back to TOC",
            "title": "PodIdentityConfig"
        }
    ]
}