{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"M3DB Operator Introduction Welcome to the documentation for the M3DB operator, a Kubernetes operator for running the open-source timeseries database M3DB on Kubernetes. Please note that this is alpha software , and as such its APIs and behavior are subject to breaking changes. While we aim to produce thoroughly tested reliable software there may be undiscovered bugs. For more background on the M3DB operator, see our KubeCon keynote on its origins and usage at Uber. Philosophy The M3DB operator aims to automate everyday tasks around managing M3DB. Specifically, it aims to automate: Creating M3DB clusters Updating M3DB clusters Destroying M3DB clusters Expanding clusters (adding instances) Shrinking clusters (removing instances) Replacing failed instances It explicitly does not try to automate every single edge case a user may ever run into. For example, it does not aim to automate disaster recovery if an entire cluster is taken down. Such use cases may still require human intervention, but the operator will aim to not conflict with such operations a human may have to take on a cluster. Generally speaking, the operator's philosophy is if it would be unclear to a human what action to take, we will not try to guess.","title":"Introduction"},{"location":"#m3db-operator","text":"","title":"M3DB Operator"},{"location":"#introduction","text":"Welcome to the documentation for the M3DB operator, a Kubernetes operator for running the open-source timeseries database M3DB on Kubernetes. Please note that this is alpha software , and as such its APIs and behavior are subject to breaking changes. While we aim to produce thoroughly tested reliable software there may be undiscovered bugs. For more background on the M3DB operator, see our KubeCon keynote on its origins and usage at Uber.","title":"Introduction"},{"location":"#philosophy","text":"The M3DB operator aims to automate everyday tasks around managing M3DB. Specifically, it aims to automate: Creating M3DB clusters Updating M3DB clusters Destroying M3DB clusters Expanding clusters (adding instances) Shrinking clusters (removing instances) Replacing failed instances It explicitly does not try to automate every single edge case a user may ever run into. For example, it does not aim to automate disaster recovery if an entire cluster is taken down. Such use cases may still require human intervention, but the operator will aim to not conflict with such operations a human may have to take on a cluster. Generally speaking, the operator's philosophy is if it would be unclear to a human what action to take, we will not try to guess.","title":"Philosophy"},{"location":"api/","text":"API Docs This document enumerates the Custom Resource Definitions used by the M3DB Operator. It is auto-generated from code comments. Table of Contents ClusterCondition ClusterSpec ExternalCoordinatorConfig IsolationGroup M3DBCluster M3DBClusterList M3DBStatus NodeAffinityTerm IndexOptions Namespace NamespaceOptions RetentionOptions PodIdentity PodIdentityConfig ClusterCondition ClusterCondition represents various conditions the cluster can be in. Field Description Scheme Required type Type of cluster condition. ClusterConditionType false status Status of the condition (True, False, Unknown). corev1.ConditionStatus false lastUpdateTime Last time this condition was updated. string false lastTransitionTime Last time this condition transitioned from one status to another. string false reason Reason this condition last changed. string false message Human-friendly message about this condition. string false Back to TOC ClusterSpec ClusterSpec defines the desired state for a M3 cluster to be converge to. Field Description Scheme Required image Image specifies which docker image to use with the cluster string false replicationFactor ReplicationFactor defines how many replicas int32 false numberOfShards NumberOfShards defines how many shards in total int32 false isolationGroups IsolationGroups specifies a map of key-value pairs. Defines which isolation groups to deploy persistent volumes for data nodes [] IsolationGroup false namespaces Namespaces specifies the namespaces this cluster will hold. [] Namespace false etcdEndpoints EtcdEndpoints defines the etcd endpoints to use for service discovery. Must be set if no custom configmap is defined. If set, etcd endpoints will be templated in to the default configmap template. []string false keepEtcdDataOnDelete KeepEtcdDataOnDelete determines whether the operator will remove cluster metadata (placement + namespaces) in etcd when the cluster is deleted. Unless true, etcd data will be cleared when the cluster is deleted. bool false enableCarbonIngester EnableCarbonIngester enables the listener port for the carbon ingester bool false configMapName ConfigMapName specifies the ConfigMap to use for this cluster. If unset a default configmap with template variables for etcd endpoints will be used. See \\\"Configuring M3DB\\\" in the docs for more. *string false podIdentityConfig PodIdentityConfig sets the configuration for pod identity. If unset only pod name and UID will be used. *PodIdentityConfig false containerResources Resources defines memory / cpu constraints for each container in the cluster. corev1.ResourceRequirements false dataDirVolumeClaimTemplate DataDirVolumeClaimTemplate is the volume claim template for an M3DB instance's data. It claims PersistentVolumes for cluster storage, volumes are dynamically provisioned by when the StorageClass is defined. * corev1.PersistentVolumeClaim false podSecurityContext PodSecurityContext allows the user to specify an optional security context for pods. *corev1.PodSecurityContext false securityContext SecurityContext allows the user to specify a container-level security context. *corev1.SecurityContext false imagePullSecrets ImagePullSecrets will be added to every pod. [] corev1.LocalObjectReference false envVars EnvVars defines custom environment variables to be passed to M3DB containers. []corev1.EnvVar false labels Labels sets the base labels that will be applied to resources created by the cluster. // TODO(schallert): design doc on labeling scheme. map[string]string false annotations Annotations sets the base annotations that will be applied to resources created by the cluster. map[string]string false tolerations Tolerations sets the tolerations that will be applied to all M3DB pods. []corev1.Toleration false priorityClassName PriorityClassName sets the priority class for all M3DB pods. string false nodeEndpointFormat NodeEndpointFormat allows overriding of the endpoint used for a node in the M3DB placement. Defaults to \\\"{{ .PodName }}.{{ .M3DBService }}:{{ .Port }}\\\". Useful if access to the cluster from other namespaces is desired. See \\\"Node Endpoint\\\" docs for full variables available. string false hostNetwork HostNetwork indicates whether M3DB pods should run in the same network namespace as the node its on. This option should be used sparingly due to security concerns outlined in the linked documentation. https://kubernetes.io/docs/concepts/policy/pod-security-policy/#host-namespaces bool false dnsPolicy DNSPolicy allows the user to set the pod's DNSPolicy. This is often used in conjunction with HostNetwork.+optional *corev1.DNSPolicy false externalCoordinator Specify a \\\"controlling\\\" coordinator for the cluster. * ExternalCoordinatorConfig false initContainers Custom setup for db nodes can be done via initContainers Provide the complete spec for the initContainer here If any storage volumes are needed in the initContainer see InitVolumes below []corev1.Container false initVolumes If the InitContainers require any storage volumes Provide the complete specification for the required Volumes here []corev1.Volume false podMetadata PodMetadata is for any Metadata that is unique to the pods, and does not belong on any other objects, such as Prometheus scrape tags metav1.ObjectMeta false parallelPodManagement ParallelPodManagement sets StatefulSets created by the operator to have Parallel pod management instead of OrderedReady. If nil, this will default to true. *bool true serviceAccountName To use a non-default service account, specify the name here otherwise the service account \\\"default\\\" will be used. This is useful for advanced use-cases such as pod security policies. The service account must exist. This operator will not create it. string false frozen Frozen is used to stop the operator from taking any further actions on a cluster. This is useful when troubleshooting as it guarantees the operator won't make any changes to the cluster. bool false Back to TOC ExternalCoordinatorConfig ExternalCoordinatorConfig defines parameters for using an external coordinator to control the cluster.\\n\\n- It is expected that there is a separate standalone coordinator cluster. - It is externally managed - not managed by this operator. - It is expected to have a service endpoint.\\n\\nSetup this db cluster, but do not assume a co-located coordinator. Instead provide a selector here so we can point to a separate coordinator service. Field Description Scheme Required selector map[string]string true Back to TOC IsolationGroup IsolationGroup defines the name of zone as well attributes for the zone configuration Field Description Scheme Required name Name is the value that will be used in StatefulSet labels, pod labels, and M3DB placement \\\"isolationGroup\\\" fields. string true nodeAffinityTerms NodeAffinityTerms is an array of NodeAffinityTerm requirements, which are ANDed together to indicate what nodes an isolation group can be assigned to. [] NodeAffinityTerm false numInstances NumInstances defines the number of instances. int32 true storageClassName StorageClassName is the name of the StorageClass to use for this isolation group. This allows ensuring that PVs will be created in the same zone as the pinned statefulset on Kubernetes < 1.12 (when topology aware volume scheduling was introduced). Only has effect if the clusters dataDirVolumeClaimTemplate is non-nil. If set, the volume claim template will have its storageClassName field overridden per-isolationgroup. If unset the storageClassName of the volumeClaimTemplate will be used. string false Back to TOC M3DBCluster M3DBCluster defines the cluster Field Description Scheme Required metadata metav1.ObjectMeta false type string true spec ClusterSpec true status M3DBStatus false Back to TOC M3DBClusterList M3DBClusterList represents a list of M3DB Clusters Field Description Scheme Required metadata metav1.ListMeta false items [] M3DBCluster true Back to TOC M3DBStatus M3DBStatus contains the current state the M3DB cluster along with a human readable message Field Description Scheme Required state State is a enum of green, yellow, and red denoting the health of the cluster M3DBState false conditions Various conditions about the cluster. [] ClusterCondition false message Message is a human readable message indicating why the cluster is in it's current state string false observedGeneration ObservedGeneration is the last generation of the cluster the controller observed. Kubernetes will automatically increment metadata.Generation every time the cluster spec is changed. int64 false Back to TOC NodeAffinityTerm NodeAffinityTerm represents a node label and a set of label values, any of which can be matched to assign a pod to a node. Field Description Scheme Required key Key is the label of the node. string true values Values is an array of values, any of which a node can have for a pod to be assigned to it. []string true Back to TOC IndexOptions IndexOptions defines parameters for indexing. Field Description Scheme Required enabled Enabled controls whether metric indexing is enabled. bool false blockSize BlockSize controls the index block size. string false Back to TOC Namespace Namespace defines an M3DB namespace or points to a preset M3DB namespace. Field Description Scheme Required name Name is the namespace name. string false preset Preset indicates preset namespace options. string false options Options points to optional custom namespace configuration. * NamespaceOptions false Back to TOC NamespaceOptions NamespaceOptions defines parameters for an M3DB namespace. See https://m3db.github.io/m3/operational_guide/namespace_configuration/ for more details. Field Description Scheme Required bootstrapEnabled BootstrapEnabled control if bootstrapping is enabled. bool false flushEnabled FlushEnabled controls whether flushing is enabled. bool false writesToCommitLog WritesToCommitLog controls whether commit log writes are enabled. bool false cleanupEnabled CleanupEnabled controls whether cleanups are enabled. bool false repairEnabled RepairEnabled controls whether repairs are enabled. bool false snapshotEnabled SnapshotEnabled controls whether snapshotting is enabled. bool false retentionOptions RetentionOptions sets the retention parameters. RetentionOptions false indexOptions IndexOptions sets the indexing parameters. IndexOptions false coldWritesEnabled ColdWritesEnabled controls whether cold writes are enabled. bool false Back to TOC RetentionOptions RetentionOptions defines parameters for data retention. Field Description Scheme Required retentionPeriod RetentionPeriod controls how long data for the namespace is retained. string false blockSize BlockSize controls the block size for the namespace. string false bufferFuture BufferFuture controls how far in the future metrics can be written. string false bufferPast BufferPast controls how far in the past metrics can be written. string false blockDataExpiry BlockDataExpiry controls the block expiry. bool false blockDataExpiryAfterNotAccessPeriod BlockDataExpiry controls the not after access period for expiration. string false Back to TOC PodIdentity PodIdentity contains all the fields that may be used to identify a pod's identity in the M3DB placement. Any non-empty fields will be used to identity uniqueness of a pod for the purpose of M3DB replace operations. Field Description Scheme Required name string false uid string false nodeName string false nodeExternalID string false nodeProviderID string false Back to TOC PodIdentityConfig PodIdentityConfig contains cluster-level configuration for deriving pod identity. Field Description Scheme Required sources Sources enumerates the sources from which to derive pod identity. Note that a pod's name will always be used. If empty, defaults to pod name and UID. []PodIdentitySource true Back to TOC","title":"API"},{"location":"api/#api-docs","text":"This document enumerates the Custom Resource Definitions used by the M3DB Operator. It is auto-generated from code comments.","title":"API Docs"},{"location":"api/#table-of-contents","text":"ClusterCondition ClusterSpec ExternalCoordinatorConfig IsolationGroup M3DBCluster M3DBClusterList M3DBStatus NodeAffinityTerm IndexOptions Namespace NamespaceOptions RetentionOptions PodIdentity PodIdentityConfig","title":"Table of Contents"},{"location":"api/#clustercondition","text":"ClusterCondition represents various conditions the cluster can be in. Field Description Scheme Required type Type of cluster condition. ClusterConditionType false status Status of the condition (True, False, Unknown). corev1.ConditionStatus false lastUpdateTime Last time this condition was updated. string false lastTransitionTime Last time this condition transitioned from one status to another. string false reason Reason this condition last changed. string false message Human-friendly message about this condition. string false Back to TOC","title":"ClusterCondition"},{"location":"api/#clusterspec","text":"ClusterSpec defines the desired state for a M3 cluster to be converge to. Field Description Scheme Required image Image specifies which docker image to use with the cluster string false replicationFactor ReplicationFactor defines how many replicas int32 false numberOfShards NumberOfShards defines how many shards in total int32 false isolationGroups IsolationGroups specifies a map of key-value pairs. Defines which isolation groups to deploy persistent volumes for data nodes [] IsolationGroup false namespaces Namespaces specifies the namespaces this cluster will hold. [] Namespace false etcdEndpoints EtcdEndpoints defines the etcd endpoints to use for service discovery. Must be set if no custom configmap is defined. If set, etcd endpoints will be templated in to the default configmap template. []string false keepEtcdDataOnDelete KeepEtcdDataOnDelete determines whether the operator will remove cluster metadata (placement + namespaces) in etcd when the cluster is deleted. Unless true, etcd data will be cleared when the cluster is deleted. bool false enableCarbonIngester EnableCarbonIngester enables the listener port for the carbon ingester bool false configMapName ConfigMapName specifies the ConfigMap to use for this cluster. If unset a default configmap with template variables for etcd endpoints will be used. See \\\"Configuring M3DB\\\" in the docs for more. *string false podIdentityConfig PodIdentityConfig sets the configuration for pod identity. If unset only pod name and UID will be used. *PodIdentityConfig false containerResources Resources defines memory / cpu constraints for each container in the cluster. corev1.ResourceRequirements false dataDirVolumeClaimTemplate DataDirVolumeClaimTemplate is the volume claim template for an M3DB instance's data. It claims PersistentVolumes for cluster storage, volumes are dynamically provisioned by when the StorageClass is defined. * corev1.PersistentVolumeClaim false podSecurityContext PodSecurityContext allows the user to specify an optional security context for pods. *corev1.PodSecurityContext false securityContext SecurityContext allows the user to specify a container-level security context. *corev1.SecurityContext false imagePullSecrets ImagePullSecrets will be added to every pod. [] corev1.LocalObjectReference false envVars EnvVars defines custom environment variables to be passed to M3DB containers. []corev1.EnvVar false labels Labels sets the base labels that will be applied to resources created by the cluster. // TODO(schallert): design doc on labeling scheme. map[string]string false annotations Annotations sets the base annotations that will be applied to resources created by the cluster. map[string]string false tolerations Tolerations sets the tolerations that will be applied to all M3DB pods. []corev1.Toleration false priorityClassName PriorityClassName sets the priority class for all M3DB pods. string false nodeEndpointFormat NodeEndpointFormat allows overriding of the endpoint used for a node in the M3DB placement. Defaults to \\\"{{ .PodName }}.{{ .M3DBService }}:{{ .Port }}\\\". Useful if access to the cluster from other namespaces is desired. See \\\"Node Endpoint\\\" docs for full variables available. string false hostNetwork HostNetwork indicates whether M3DB pods should run in the same network namespace as the node its on. This option should be used sparingly due to security concerns outlined in the linked documentation. https://kubernetes.io/docs/concepts/policy/pod-security-policy/#host-namespaces bool false dnsPolicy DNSPolicy allows the user to set the pod's DNSPolicy. This is often used in conjunction with HostNetwork.+optional *corev1.DNSPolicy false externalCoordinator Specify a \\\"controlling\\\" coordinator for the cluster. * ExternalCoordinatorConfig false initContainers Custom setup for db nodes can be done via initContainers Provide the complete spec for the initContainer here If any storage volumes are needed in the initContainer see InitVolumes below []corev1.Container false initVolumes If the InitContainers require any storage volumes Provide the complete specification for the required Volumes here []corev1.Volume false podMetadata PodMetadata is for any Metadata that is unique to the pods, and does not belong on any other objects, such as Prometheus scrape tags metav1.ObjectMeta false parallelPodManagement ParallelPodManagement sets StatefulSets created by the operator to have Parallel pod management instead of OrderedReady. If nil, this will default to true. *bool true serviceAccountName To use a non-default service account, specify the name here otherwise the service account \\\"default\\\" will be used. This is useful for advanced use-cases such as pod security policies. The service account must exist. This operator will not create it. string false frozen Frozen is used to stop the operator from taking any further actions on a cluster. This is useful when troubleshooting as it guarantees the operator won't make any changes to the cluster. bool false Back to TOC","title":"ClusterSpec"},{"location":"api/#externalcoordinatorconfig","text":"ExternalCoordinatorConfig defines parameters for using an external coordinator to control the cluster.\\n\\n- It is expected that there is a separate standalone coordinator cluster. - It is externally managed - not managed by this operator. - It is expected to have a service endpoint.\\n\\nSetup this db cluster, but do not assume a co-located coordinator. Instead provide a selector here so we can point to a separate coordinator service. Field Description Scheme Required selector map[string]string true Back to TOC","title":"ExternalCoordinatorConfig"},{"location":"api/#isolationgroup","text":"IsolationGroup defines the name of zone as well attributes for the zone configuration Field Description Scheme Required name Name is the value that will be used in StatefulSet labels, pod labels, and M3DB placement \\\"isolationGroup\\\" fields. string true nodeAffinityTerms NodeAffinityTerms is an array of NodeAffinityTerm requirements, which are ANDed together to indicate what nodes an isolation group can be assigned to. [] NodeAffinityTerm false numInstances NumInstances defines the number of instances. int32 true storageClassName StorageClassName is the name of the StorageClass to use for this isolation group. This allows ensuring that PVs will be created in the same zone as the pinned statefulset on Kubernetes < 1.12 (when topology aware volume scheduling was introduced). Only has effect if the clusters dataDirVolumeClaimTemplate is non-nil. If set, the volume claim template will have its storageClassName field overridden per-isolationgroup. If unset the storageClassName of the volumeClaimTemplate will be used. string false Back to TOC","title":"IsolationGroup"},{"location":"api/#m3dbcluster","text":"M3DBCluster defines the cluster Field Description Scheme Required metadata metav1.ObjectMeta false type string true spec ClusterSpec true status M3DBStatus false Back to TOC","title":"M3DBCluster"},{"location":"api/#m3dbclusterlist","text":"M3DBClusterList represents a list of M3DB Clusters Field Description Scheme Required metadata metav1.ListMeta false items [] M3DBCluster true Back to TOC","title":"M3DBClusterList"},{"location":"api/#m3dbstatus","text":"M3DBStatus contains the current state the M3DB cluster along with a human readable message Field Description Scheme Required state State is a enum of green, yellow, and red denoting the health of the cluster M3DBState false conditions Various conditions about the cluster. [] ClusterCondition false message Message is a human readable message indicating why the cluster is in it's current state string false observedGeneration ObservedGeneration is the last generation of the cluster the controller observed. Kubernetes will automatically increment metadata.Generation every time the cluster spec is changed. int64 false Back to TOC","title":"M3DBStatus"},{"location":"api/#nodeaffinityterm","text":"NodeAffinityTerm represents a node label and a set of label values, any of which can be matched to assign a pod to a node. Field Description Scheme Required key Key is the label of the node. string true values Values is an array of values, any of which a node can have for a pod to be assigned to it. []string true Back to TOC","title":"NodeAffinityTerm"},{"location":"api/#indexoptions","text":"IndexOptions defines parameters for indexing. Field Description Scheme Required enabled Enabled controls whether metric indexing is enabled. bool false blockSize BlockSize controls the index block size. string false Back to TOC","title":"IndexOptions"},{"location":"api/#namespace","text":"Namespace defines an M3DB namespace or points to a preset M3DB namespace. Field Description Scheme Required name Name is the namespace name. string false preset Preset indicates preset namespace options. string false options Options points to optional custom namespace configuration. * NamespaceOptions false Back to TOC","title":"Namespace"},{"location":"api/#namespaceoptions","text":"NamespaceOptions defines parameters for an M3DB namespace. See https://m3db.github.io/m3/operational_guide/namespace_configuration/ for more details. Field Description Scheme Required bootstrapEnabled BootstrapEnabled control if bootstrapping is enabled. bool false flushEnabled FlushEnabled controls whether flushing is enabled. bool false writesToCommitLog WritesToCommitLog controls whether commit log writes are enabled. bool false cleanupEnabled CleanupEnabled controls whether cleanups are enabled. bool false repairEnabled RepairEnabled controls whether repairs are enabled. bool false snapshotEnabled SnapshotEnabled controls whether snapshotting is enabled. bool false retentionOptions RetentionOptions sets the retention parameters. RetentionOptions false indexOptions IndexOptions sets the indexing parameters. IndexOptions false coldWritesEnabled ColdWritesEnabled controls whether cold writes are enabled. bool false Back to TOC","title":"NamespaceOptions"},{"location":"api/#retentionoptions","text":"RetentionOptions defines parameters for data retention. Field Description Scheme Required retentionPeriod RetentionPeriod controls how long data for the namespace is retained. string false blockSize BlockSize controls the block size for the namespace. string false bufferFuture BufferFuture controls how far in the future metrics can be written. string false bufferPast BufferPast controls how far in the past metrics can be written. string false blockDataExpiry BlockDataExpiry controls the block expiry. bool false blockDataExpiryAfterNotAccessPeriod BlockDataExpiry controls the not after access period for expiration. string false Back to TOC","title":"RetentionOptions"},{"location":"api/#podidentity","text":"PodIdentity contains all the fields that may be used to identify a pod's identity in the M3DB placement. Any non-empty fields will be used to identity uniqueness of a pod for the purpose of M3DB replace operations. Field Description Scheme Required name string false uid string false nodeName string false nodeExternalID string false nodeProviderID string false Back to TOC","title":"PodIdentity"},{"location":"api/#podidentityconfig","text":"PodIdentityConfig contains cluster-level configuration for deriving pod identity. Field Description Scheme Required sources Sources enumerates the sources from which to derive pod identity. Note that a pod's name will always be used. If empty, defaults to pod name and UID. []PodIdentitySource true Back to TOC","title":"PodIdentityConfig"},{"location":"configuration/configuring_m3db/","text":"Configuring M3DB By default the operator will apply a configmap with basic M3DB options and settings for the coordinator to direct Prometheus reads/writes to the cluster. This template can be found here . To apply custom a configuration for the M3DB cluster, one can set the configMapName parameter of the cluster spec to an existing configmap. Environment Warning If providing a custom config map, the env you specify in your config must be $NAMESPACE/$NAME , where $NAMESPACE is the Kubernetes namespace your cluster is in and $NAME is the name of the cluster. For example, with the following cluster: apiVersion : operator.m3db.io/v1alpha1 kind : M3DBCluster metadata : name : cluster-a namespace : production ... The value of env in your config MUST be production/cluster-a . This restriction allows multiple M3DB clusters to safely share the same etcd cluster.","title":"Configuring M3DB"},{"location":"configuration/configuring_m3db/#configuring-m3db","text":"By default the operator will apply a configmap with basic M3DB options and settings for the coordinator to direct Prometheus reads/writes to the cluster. This template can be found here . To apply custom a configuration for the M3DB cluster, one can set the configMapName parameter of the cluster spec to an existing configmap.","title":"Configuring M3DB"},{"location":"configuration/configuring_m3db/#environment-warning","text":"If providing a custom config map, the env you specify in your config must be $NAMESPACE/$NAME , where $NAMESPACE is the Kubernetes namespace your cluster is in and $NAME is the name of the cluster. For example, with the following cluster: apiVersion : operator.m3db.io/v1alpha1 kind : M3DBCluster metadata : name : cluster-a namespace : production ... The value of env in your config MUST be production/cluster-a . This restriction allows multiple M3DB clusters to safely share the same etcd cluster.","title":"Environment Warning"},{"location":"configuration/namespaces/","text":"Namespaces M3DB uses the concept of namespaces to determine how metrics are stored and retained. The M3DB operator allows a user to define their own namespaces, or to use a set of presets we consider to be suitable for production use cases. Namespaces are configured as part of an m3dbcluster spec . Presets 10s:2d This preset will store metrics at 10 second resolution for 2 days. For example, in your cluster spec: spec : ... namespaces : - name : metrics-short-term preset : 10s:2d 1m:40d This preset will store metrics at 1 minute resolution for 40 days. spec : ... namespaces : - name : metrics-long-term preset : 1m:40d Custom Namespaces You can also define your own custom namespaces by setting the NamespaceOptions within a cluster spec. The API lists all available fields. As an example, a namespace to store 7 days of data may look like: ... spec : ... namespaces : - name : custom-7d options : bootstrapEnabled : true flushEnabled : true writesToCommitLog : true cleanupEnabled : true snapshotEnabled : true repairEnabled : false retentionOptions : retentionPeriod : 168h blockSize : 12h bufferFuture : 20m bufferPast : 20m blockDataExpiry : true blockDataExpiryAfterNotAccessPeriod : 5m indexOptions : enabled : true blockSize : 12h","title":"Namespaces"},{"location":"configuration/namespaces/#namespaces","text":"M3DB uses the concept of namespaces to determine how metrics are stored and retained. The M3DB operator allows a user to define their own namespaces, or to use a set of presets we consider to be suitable for production use cases. Namespaces are configured as part of an m3dbcluster spec .","title":"Namespaces"},{"location":"configuration/namespaces/#presets","text":"","title":"Presets"},{"location":"configuration/namespaces/#10s2d","text":"This preset will store metrics at 10 second resolution for 2 days. For example, in your cluster spec: spec : ... namespaces : - name : metrics-short-term preset : 10s:2d","title":"10s:2d"},{"location":"configuration/namespaces/#1m40d","text":"This preset will store metrics at 1 minute resolution for 40 days. spec : ... namespaces : - name : metrics-long-term preset : 1m:40d","title":"1m:40d"},{"location":"configuration/namespaces/#custom-namespaces","text":"You can also define your own custom namespaces by setting the NamespaceOptions within a cluster spec. The API lists all available fields. As an example, a namespace to store 7 days of data may look like: ... spec : ... namespaces : - name : custom-7d options : bootstrapEnabled : true flushEnabled : true writesToCommitLog : true cleanupEnabled : true snapshotEnabled : true repairEnabled : false retentionOptions : retentionPeriod : 168h blockSize : 12h bufferFuture : 20m bufferPast : 20m blockDataExpiry : true blockDataExpiryAfterNotAccessPeriod : 5m indexOptions : enabled : true blockSize : 12h","title":"Custom Namespaces"},{"location":"configuration/node_affinity/","text":"Node Affinity & Cluster Topology Node Affinity Kubernetes allows pods to be assigned to nodes based on various critera through node affinity . M3DB was built with failure tolerance as a core feature. M3DB's isolation groups allow shards to be placed across failure domains such that the loss of no single domain can cause the cluster to lose quorum. More details on M3DB's resiliency can be found in the deployment docs . By leveraging Kubernetes' node affinity and M3DB's isolation groups, the operator can guarantee that M3DB pods are distributed across failure domains. For example, in a Kubernetes cluster spread across 3 zones in a cloud region, the isolationGroups configuration below would guarantee that no single zone failure could degrade the M3DB cluster. M3DB is unaware of the underlying zone topology: it just views the isolation groups as group1 , group2 , group3 in its placement . Thanks to the Kubernetes scheduler, however, these groups are actually scheduled across separate failure domains. apiVersion : operator.m3db.io/v1alpha1 kind : M3DBCluster ... spec : replicationFactor : 3 isolationGroups : - name : group1 numInstances : 3 nodeAffinityTerms : - key : failure-domain.beta.kubernetes.io/zone values : - us-east1-b - name : group2 numInstances : 3 nodeAffinityTerms : - key : failure-domain.beta.kubernetes.io/zone values : - us-east1-c - name : group3 numInstances : 3 nodeAffinityTerms : - key : failure-domain.beta.kubernetes.io/zone values : - us-east1-d Tolerations In addition to allowing pods to be assigned to certain nodes via node affinity, Kubernetes allows pods to be repelled from nodes through taints if they don't tolerate the taint. For example, the following config would ensure: Pods are spread across zones. Pods are only assigned to nodes in the m3db-dedicated-pool pool. No other pods could be assigned to those nodes (assuming they were tainted with the taint m3db-dedicated-taint ). apiVersion : operator.m3db.io/v1alpha1 kind : M3DBCluster ... spec : replicationFactor : 3 isolationGroups : - name : group1 numInstances : 3 nodeAffinityTerms : - key : failure-domain.beta.kubernetes.io/zone values : - us-east1-b - key : nodepool values : - m3db-dedicated-pool - name : group2 numInstances : 3 nodeAffinityTerms : - key : failure-domain.beta.kubernetes.io/zone values : - us-east1-c - key : nodepool values : - m3db-dedicated-pool - name : group3 numInstances : 3 nodeAffinityTerms : - key : failure-domain.beta.kubernetes.io/zone values : - us-east1-d - key : nodepool values : - m3db-dedicated-pool tolerations : - key : m3db-dedicated effect : NoSchedule operator : Exists Example Affinity Configurations Zonal Cluster The examples so far have focused on multi-zone Kubernetes clusters. Some users may only have a cluster in a single zone and accept the reduced fault tolerance. The following configuration shows how to configure the operator in a zonal cluster. apiVersion : operator.m3db.io/v1alpha1 kind : M3DBCluster ... spec : replicationFactor : 3 isolationGroups : - name : group1 numInstances : 3 nodeAffinityTerms : - key : failure-domain.beta.kubernetes.io/zone values : - us-east1-b - name : group2 numInstances : 3 nodeAffinityTerms : - key : failure-domain.beta.kubernetes.io/zone values : - us-east1-b - name : group3 numInstances : 3 nodeAffinityTerms : - key : failure-domain.beta.kubernetes.io/zone values : - us-east1-b 6 Zone Cluster In the above examples we created clusters with 1 isolation group in each of 3 zones. Because values within a single NodeAffinityTerm are OR'd, we can also spread an isolationgroup across multiple zones. For example, if we had 6 zones available to us: apiVersion : operator.m3db.io/v1alpha1 kind : M3DBCluster ... spec : replicationFactor : 3 isolationGroups : - name : group1 numInstances : 3 nodeAffinityTerms : - key : failure-domain.beta.kubernetes.io/zone values : - us-east1-a - us-east1-b - name : group2 numInstances : 3 nodeAffinityTerms : - key : failure-domain.beta.kubernetes.io/zone values : - us-east1-c - us-east1-d - name : group3 numInstances : 3 nodeAffinityTerms : - key : failure-domain.beta.kubernetes.io/zone values : - us-east1-e - us-east1-f No Affinity If there are no failure domains available, one can have a cluster with no affinity where the pods will be scheduled however Kubernetes would place them by default: apiVersion : operator.m3db.io/v1alpha1 kind : M3DBCluster ... spec : replicationFactor : 3 isolationGroups : - name : group1 numInstances : 3 - name : group2 numInstances : 3 - name : group3 numInstances : 3","title":"Node Affinity & Cluster Topology"},{"location":"configuration/node_affinity/#node-affinity-cluster-topology","text":"","title":"Node Affinity &amp; Cluster Topology"},{"location":"configuration/node_affinity/#node-affinity","text":"Kubernetes allows pods to be assigned to nodes based on various critera through node affinity . M3DB was built with failure tolerance as a core feature. M3DB's isolation groups allow shards to be placed across failure domains such that the loss of no single domain can cause the cluster to lose quorum. More details on M3DB's resiliency can be found in the deployment docs . By leveraging Kubernetes' node affinity and M3DB's isolation groups, the operator can guarantee that M3DB pods are distributed across failure domains. For example, in a Kubernetes cluster spread across 3 zones in a cloud region, the isolationGroups configuration below would guarantee that no single zone failure could degrade the M3DB cluster. M3DB is unaware of the underlying zone topology: it just views the isolation groups as group1 , group2 , group3 in its placement . Thanks to the Kubernetes scheduler, however, these groups are actually scheduled across separate failure domains. apiVersion : operator.m3db.io/v1alpha1 kind : M3DBCluster ... spec : replicationFactor : 3 isolationGroups : - name : group1 numInstances : 3 nodeAffinityTerms : - key : failure-domain.beta.kubernetes.io/zone values : - us-east1-b - name : group2 numInstances : 3 nodeAffinityTerms : - key : failure-domain.beta.kubernetes.io/zone values : - us-east1-c - name : group3 numInstances : 3 nodeAffinityTerms : - key : failure-domain.beta.kubernetes.io/zone values : - us-east1-d","title":"Node Affinity"},{"location":"configuration/node_affinity/#tolerations","text":"In addition to allowing pods to be assigned to certain nodes via node affinity, Kubernetes allows pods to be repelled from nodes through taints if they don't tolerate the taint. For example, the following config would ensure: Pods are spread across zones. Pods are only assigned to nodes in the m3db-dedicated-pool pool. No other pods could be assigned to those nodes (assuming they were tainted with the taint m3db-dedicated-taint ). apiVersion : operator.m3db.io/v1alpha1 kind : M3DBCluster ... spec : replicationFactor : 3 isolationGroups : - name : group1 numInstances : 3 nodeAffinityTerms : - key : failure-domain.beta.kubernetes.io/zone values : - us-east1-b - key : nodepool values : - m3db-dedicated-pool - name : group2 numInstances : 3 nodeAffinityTerms : - key : failure-domain.beta.kubernetes.io/zone values : - us-east1-c - key : nodepool values : - m3db-dedicated-pool - name : group3 numInstances : 3 nodeAffinityTerms : - key : failure-domain.beta.kubernetes.io/zone values : - us-east1-d - key : nodepool values : - m3db-dedicated-pool tolerations : - key : m3db-dedicated effect : NoSchedule operator : Exists","title":"Tolerations"},{"location":"configuration/node_affinity/#example-affinity-configurations","text":"","title":"Example Affinity Configurations"},{"location":"configuration/node_affinity/#zonal-cluster","text":"The examples so far have focused on multi-zone Kubernetes clusters. Some users may only have a cluster in a single zone and accept the reduced fault tolerance. The following configuration shows how to configure the operator in a zonal cluster. apiVersion : operator.m3db.io/v1alpha1 kind : M3DBCluster ... spec : replicationFactor : 3 isolationGroups : - name : group1 numInstances : 3 nodeAffinityTerms : - key : failure-domain.beta.kubernetes.io/zone values : - us-east1-b - name : group2 numInstances : 3 nodeAffinityTerms : - key : failure-domain.beta.kubernetes.io/zone values : - us-east1-b - name : group3 numInstances : 3 nodeAffinityTerms : - key : failure-domain.beta.kubernetes.io/zone values : - us-east1-b","title":"Zonal Cluster"},{"location":"configuration/node_affinity/#6-zone-cluster","text":"In the above examples we created clusters with 1 isolation group in each of 3 zones. Because values within a single NodeAffinityTerm are OR'd, we can also spread an isolationgroup across multiple zones. For example, if we had 6 zones available to us: apiVersion : operator.m3db.io/v1alpha1 kind : M3DBCluster ... spec : replicationFactor : 3 isolationGroups : - name : group1 numInstances : 3 nodeAffinityTerms : - key : failure-domain.beta.kubernetes.io/zone values : - us-east1-a - us-east1-b - name : group2 numInstances : 3 nodeAffinityTerms : - key : failure-domain.beta.kubernetes.io/zone values : - us-east1-c - us-east1-d - name : group3 numInstances : 3 nodeAffinityTerms : - key : failure-domain.beta.kubernetes.io/zone values : - us-east1-e - us-east1-f","title":"6 Zone Cluster"},{"location":"configuration/node_affinity/#no-affinity","text":"If there are no failure domains available, one can have a cluster with no affinity where the pods will be scheduled however Kubernetes would place them by default: apiVersion : operator.m3db.io/v1alpha1 kind : M3DBCluster ... spec : replicationFactor : 3 isolationGroups : - name : group1 numInstances : 3 - name : group2 numInstances : 3 - name : group3 numInstances : 3","title":"No Affinity"},{"location":"configuration/node_endpoint/","text":"Node Endpoint M3DB stores an endpoint field on placement instances that is used for communication between DB nodes and from other components such as the coordinator. The operator allows customizing the format of this endpoint by setting the nodeEndpointFormat field on a cluster spec. The format of this field uses Go templates , with the following template fields currently supported: Field Description PodName Name of the pod M3DBService Name of the generated M3DB service PodNamespace Namespace the pod is in Port Port M3DB is serving RPCs on The default format is: {{ .PodName }}.{{ .M3DBService }}:{{ .Port }} As an example of an override, to expose an M3DB cluster to containers in other Kubernetes namespaces nodeEndpointFormat can be set to: {{ .PodName }}.{{ .M3DBService }}.{{ .PodNamespace }}:{{ .Port }}","title":"Node Endpoint"},{"location":"configuration/node_endpoint/#node-endpoint","text":"M3DB stores an endpoint field on placement instances that is used for communication between DB nodes and from other components such as the coordinator. The operator allows customizing the format of this endpoint by setting the nodeEndpointFormat field on a cluster spec. The format of this field uses Go templates , with the following template fields currently supported: Field Description PodName Name of the pod M3DBService Name of the generated M3DB service PodNamespace Namespace the pod is in Port Port M3DB is serving RPCs on The default format is: {{ .PodName }}.{{ .M3DBService }}:{{ .Port }} As an example of an override, to expose an M3DB cluster to containers in other Kubernetes namespaces nodeEndpointFormat can be set to: {{ .PodName }}.{{ .M3DBService }}.{{ .PodNamespace }}:{{ .Port }}","title":"Node Endpoint"},{"location":"configuration/pod_identity/","text":"Pod Identity Motivation M3DB assumes that if a process is started and owns sealed shards marked as Available that its data for those shards is valid and does not have to be fetched from peers. Consequentially this means it will begin serving reads for that data. For more background on M3DB topology, see the M3DB topology docs . In most environments in which M3DB has been deployed in production, it has been on a set of hosts predetermined by whomever is managing the cluster. This means that an M3DB instance is identified in a toplogy by its hostname, and that when an M3DB process comes up and finds its hostname in the cluster with Available shards that it can serve reads for those shards. This does not work on Kubernetes, particularly when working with StatefulSets, as a pod may be rescheduled on a new node or with new storage attached but its name may stay the same. If we were to naively use an instance's hostname (pod name), and it were to get rescheduled on a new node with no data, it could assume that absence of data is valid and begin returning empty results for read requests. To account for this, the M3DB Operator determines an M3DB instance's identity in the topology based on a configurable set of metadata about the pod. Configuration The M3DB operator uses a configurable set of metadata about a pod to determine its identity in the M3DB placement. This is encapsulated in the PodIdentityConfig field of a cluster's spec. In addition to the configures sources, a pod's name will always be included. Every pod in an M3DB cluster is annotated with its identity and is passed to the M3DB instance via a downward API volume. Sources This section will be filled out as a number of pending PRs land. Recommendations No Persistent Storage If not using PVs, you should set sources to PodUID : podIdentityConfig: sources: - PodUID This way whenever a container is rescheduled, the operator will initiate a replace and it will stream data from its peers before serving reads. Note that not having persistent storage is not a recommended way to run M3DB. Remote Persistent Storage If using remote storage you do not need to set sources, as it will default to just the pods name. The data for an M3DB instance will move around with its container. Local Persistent Storage If using persistent local volumes, you should set sources to NodeName . In this configuration M3DB will consider a pod to be the same so long as it's on the same node. Replaces will only be triggered if a pod with the same name is moved to a new host. Note that if using local SSDs on GKE, node names may stay the same even though a VM has been recreated. We also support ProviderID , which will use the underlying VM's unique ID number in GCE to identity host uniqueness.","title":"Pod Identity"},{"location":"configuration/pod_identity/#pod-identity","text":"","title":"Pod Identity"},{"location":"configuration/pod_identity/#motivation","text":"M3DB assumes that if a process is started and owns sealed shards marked as Available that its data for those shards is valid and does not have to be fetched from peers. Consequentially this means it will begin serving reads for that data. For more background on M3DB topology, see the M3DB topology docs . In most environments in which M3DB has been deployed in production, it has been on a set of hosts predetermined by whomever is managing the cluster. This means that an M3DB instance is identified in a toplogy by its hostname, and that when an M3DB process comes up and finds its hostname in the cluster with Available shards that it can serve reads for those shards. This does not work on Kubernetes, particularly when working with StatefulSets, as a pod may be rescheduled on a new node or with new storage attached but its name may stay the same. If we were to naively use an instance's hostname (pod name), and it were to get rescheduled on a new node with no data, it could assume that absence of data is valid and begin returning empty results for read requests. To account for this, the M3DB Operator determines an M3DB instance's identity in the topology based on a configurable set of metadata about the pod.","title":"Motivation"},{"location":"configuration/pod_identity/#configuration","text":"The M3DB operator uses a configurable set of metadata about a pod to determine its identity in the M3DB placement. This is encapsulated in the PodIdentityConfig field of a cluster's spec. In addition to the configures sources, a pod's name will always be included. Every pod in an M3DB cluster is annotated with its identity and is passed to the M3DB instance via a downward API volume.","title":"Configuration"},{"location":"configuration/pod_identity/#sources","text":"This section will be filled out as a number of pending PRs land.","title":"Sources"},{"location":"configuration/pod_identity/#recommendations","text":"","title":"Recommendations"},{"location":"configuration/pod_identity/#no-persistent-storage","text":"If not using PVs, you should set sources to PodUID : podIdentityConfig: sources: - PodUID This way whenever a container is rescheduled, the operator will initiate a replace and it will stream data from its peers before serving reads. Note that not having persistent storage is not a recommended way to run M3DB.","title":"No Persistent Storage"},{"location":"configuration/pod_identity/#remote-persistent-storage","text":"If using remote storage you do not need to set sources, as it will default to just the pods name. The data for an M3DB instance will move around with its container.","title":"Remote Persistent Storage"},{"location":"configuration/pod_identity/#local-persistent-storage","text":"If using persistent local volumes, you should set sources to NodeName . In this configuration M3DB will consider a pod to be the same so long as it's on the same node. Replaces will only be triggered if a pod with the same name is moved to a new host. Note that if using local SSDs on GKE, node names may stay the same even though a VM has been recreated. We also support ProviderID , which will use the underlying VM's unique ID number in GCE to identity host uniqueness.","title":"Local Persistent Storage"},{"location":"getting_started/create_cluster/","text":"Creating a Cluster Once you've installed the M3DB operator and read over the requirements , you can start creating some M3DB clusters! Basic Cluster The following creates an M3DB cluster spread across 3 zones, with each M3DB instance being able to store up to 350gb of data using your Kubernetes cluster's default storage class. For examples of different cluster topologies, such as zonal clusters, see the docs on node affinity . Etcd Create an etcd cluster with persistent volumes: kubectl apply -f https://raw.githubusercontent.com/m3db/m3db-operator/v0.9.0/example/etcd/etcd-pd.yaml We recommend modifying the storageClassName in the manifest to one that matches your cloud provider's fastest remote storage option, such as pd-ssd on GCP. M3DB apiVersion : operator.m3db.io/v1alpha1 kind : M3DBCluster metadata : name : persistent-cluster spec : image : quay.io/m3db/m3dbnode:latest replicationFactor : 3 numberOfShards : 256 isolationGroups : - name : group1 numInstances : 1 nodeAffinityTerms : - key : failure-domain.beta.kubernetes.io/zone values : - <zone-a> - name : group2 numInstances : 1 nodeAffinityTerms : - key : failure-domain.beta.kubernetes.io/zone values : - <zone-b> - name : group3 numInstances : 1 nodeAffinityTerms : - key : failure-domain.beta.kubernetes.io/zone values : - <zone-c> etcdEndpoints : - http://etcd-0.etcd:2379 - http://etcd-1.etcd:2379 - http://etcd-2.etcd:2379 podIdentityConfig : sources : [] namespaces : - name : metrics-10s:2d preset : 10s:2d dataDirVolumeClaimTemplate : metadata : name : m3db-data spec : accessModes : - ReadWriteOnce resources : requests : storage : 350Gi limits : storage : 350Gi Ephemeral Cluster WARNING: This setup is not intended for production-grade clusters, but rather for \"kicking the tires\" with the operator and M3DB. It is intended to work across almost any Kubernetes environment, and as such has as few dependencies as possible (namely persistent storage). See below for instructions on creating a more durable cluster. Etcd Create an etcd cluster in the same namespace your M3DB cluster will be created in. If you don't have persistent storage available, this will create a cluster that will not use persistent storage and will likely become unavailable if any of the pods die: kubectl apply -f https://raw.githubusercontent.com/m3db/m3db-operator/v0.9.0/example/etcd/etcd-basic.yaml # Verify etcd health once pods available kubectl exec etcd-0 -- env ETCDCTL_API=3 etcdctl endpoint health # 127.0.0.1:2379 is healthy: successfully committed proposal: took = 2.94668ms If you have remote storage available and would like to jump straight to using it, apply the following manifest for etcd instead: kubectl apply -f https://raw.githubusercontent.com/m3db/m3db-operator/v0.9.0/example/etcd/etcd-pd.yaml M3DB Once etcd is available, you can create an M3DB cluster. An example of a very basic M3DB cluster definition is as follows: apiVersion : operator.m3db.io/v1alpha1 kind : M3DBCluster metadata : name : simple-cluster spec : image : quay.io/m3db/m3dbnode:latest replicationFactor : 3 numberOfShards : 256 etcdEndpoints : - http://etcd-0.etcd:2379 - http://etcd-1.etcd:2379 - http://etcd-2.etcd:2379 isolationGroups : - name : group1 numInstances : 1 nodeAffinityTerms : - key : failure-domain.beta.kubernetes.io/zone values : - <zone-a> - name : group2 numInstances : 1 nodeAffinityTerms : - key : failure-domain.beta.kubernetes.io/zone values : - <zone-b> - name : group3 numInstances : 1 nodeAffinityTerms : - key : failure-domain.beta.kubernetes.io/zone values : - <zone-c> podIdentityConfig : sources : - PodUID namespaces : - name : metrics-10s:2d preset : 10s:2d This will create a highly available cluster with RF=3 spread evenly across the three given zones within a region. A pod's UID will be used for its identity . The cluster will have 1 namespace that stores metrics for 2 days at 10s resolution. Next, apply your manifest: $ kubectl apply -f example/simple-cluster.yaml m3dbcluster.operator.m3db.io/simple-cluster created Shortly after all pods are created you should see the cluster ready! $ kubectl get po -l operator.m3db.io/app=m3db NAME READY STATUS RESTARTS AGE simple-cluster-rep0-0 1/1 Running 0 1m simple-cluster-rep1-0 1/1 Running 0 56s simple-cluster-rep2-0 1/1 Running 0 37s We can verify that the cluster has finished streaming data by peers by checking that an instance has bootstrapped: $ kubectl exec simple-cluster-rep2-0 -- curl -sSf localhost:9002/health {\"ok\":true,\"status\":\"up\",\"bootstrapped\":true}","title":"Creating a Cluster"},{"location":"getting_started/create_cluster/#creating-a-cluster","text":"Once you've installed the M3DB operator and read over the requirements , you can start creating some M3DB clusters!","title":"Creating a Cluster"},{"location":"getting_started/create_cluster/#basic-cluster","text":"The following creates an M3DB cluster spread across 3 zones, with each M3DB instance being able to store up to 350gb of data using your Kubernetes cluster's default storage class. For examples of different cluster topologies, such as zonal clusters, see the docs on node affinity .","title":"Basic Cluster"},{"location":"getting_started/create_cluster/#etcd","text":"Create an etcd cluster with persistent volumes: kubectl apply -f https://raw.githubusercontent.com/m3db/m3db-operator/v0.9.0/example/etcd/etcd-pd.yaml We recommend modifying the storageClassName in the manifest to one that matches your cloud provider's fastest remote storage option, such as pd-ssd on GCP.","title":"Etcd"},{"location":"getting_started/create_cluster/#m3db","text":"apiVersion : operator.m3db.io/v1alpha1 kind : M3DBCluster metadata : name : persistent-cluster spec : image : quay.io/m3db/m3dbnode:latest replicationFactor : 3 numberOfShards : 256 isolationGroups : - name : group1 numInstances : 1 nodeAffinityTerms : - key : failure-domain.beta.kubernetes.io/zone values : - <zone-a> - name : group2 numInstances : 1 nodeAffinityTerms : - key : failure-domain.beta.kubernetes.io/zone values : - <zone-b> - name : group3 numInstances : 1 nodeAffinityTerms : - key : failure-domain.beta.kubernetes.io/zone values : - <zone-c> etcdEndpoints : - http://etcd-0.etcd:2379 - http://etcd-1.etcd:2379 - http://etcd-2.etcd:2379 podIdentityConfig : sources : [] namespaces : - name : metrics-10s:2d preset : 10s:2d dataDirVolumeClaimTemplate : metadata : name : m3db-data spec : accessModes : - ReadWriteOnce resources : requests : storage : 350Gi limits : storage : 350Gi","title":"M3DB"},{"location":"getting_started/create_cluster/#ephemeral-cluster","text":"WARNING: This setup is not intended for production-grade clusters, but rather for \"kicking the tires\" with the operator and M3DB. It is intended to work across almost any Kubernetes environment, and as such has as few dependencies as possible (namely persistent storage). See below for instructions on creating a more durable cluster.","title":"Ephemeral Cluster"},{"location":"getting_started/create_cluster/#etcd_1","text":"Create an etcd cluster in the same namespace your M3DB cluster will be created in. If you don't have persistent storage available, this will create a cluster that will not use persistent storage and will likely become unavailable if any of the pods die: kubectl apply -f https://raw.githubusercontent.com/m3db/m3db-operator/v0.9.0/example/etcd/etcd-basic.yaml # Verify etcd health once pods available kubectl exec etcd-0 -- env ETCDCTL_API=3 etcdctl endpoint health # 127.0.0.1:2379 is healthy: successfully committed proposal: took = 2.94668ms If you have remote storage available and would like to jump straight to using it, apply the following manifest for etcd instead: kubectl apply -f https://raw.githubusercontent.com/m3db/m3db-operator/v0.9.0/example/etcd/etcd-pd.yaml","title":"Etcd"},{"location":"getting_started/create_cluster/#m3db_1","text":"Once etcd is available, you can create an M3DB cluster. An example of a very basic M3DB cluster definition is as follows: apiVersion : operator.m3db.io/v1alpha1 kind : M3DBCluster metadata : name : simple-cluster spec : image : quay.io/m3db/m3dbnode:latest replicationFactor : 3 numberOfShards : 256 etcdEndpoints : - http://etcd-0.etcd:2379 - http://etcd-1.etcd:2379 - http://etcd-2.etcd:2379 isolationGroups : - name : group1 numInstances : 1 nodeAffinityTerms : - key : failure-domain.beta.kubernetes.io/zone values : - <zone-a> - name : group2 numInstances : 1 nodeAffinityTerms : - key : failure-domain.beta.kubernetes.io/zone values : - <zone-b> - name : group3 numInstances : 1 nodeAffinityTerms : - key : failure-domain.beta.kubernetes.io/zone values : - <zone-c> podIdentityConfig : sources : - PodUID namespaces : - name : metrics-10s:2d preset : 10s:2d This will create a highly available cluster with RF=3 spread evenly across the three given zones within a region. A pod's UID will be used for its identity . The cluster will have 1 namespace that stores metrics for 2 days at 10s resolution. Next, apply your manifest: $ kubectl apply -f example/simple-cluster.yaml m3dbcluster.operator.m3db.io/simple-cluster created Shortly after all pods are created you should see the cluster ready! $ kubectl get po -l operator.m3db.io/app=m3db NAME READY STATUS RESTARTS AGE simple-cluster-rep0-0 1/1 Running 0 1m simple-cluster-rep1-0 1/1 Running 0 56s simple-cluster-rep2-0 1/1 Running 0 37s We can verify that the cluster has finished streaming data by peers by checking that an instance has bootstrapped: $ kubectl exec simple-cluster-rep2-0 -- curl -sSf localhost:9002/health {\"ok\":true,\"status\":\"up\",\"bootstrapped\":true}","title":"M3DB"},{"location":"getting_started/delete_cluster/","text":"Deleting a Cluster Delete your M3DB cluster with kubectl : kubectl delete m3dbcluster simple-cluster By default, the operator will delete the placement and namespaces associated with a cluster before the CRD resource deleted. If you do not want this behavior, set keepEtcdDataOnDelete to true on your cluster spec. Under the hood, the operator uses Kubernetes finalizers to ensure the cluster CRD is not deleted until the operator has had a chance to do cleanup. Debugging Stuck Cluster Deletion If for some reason the operator is unable to delete the placement and namespace for the cluster, the cluster CRD itself will be stuck in a state where it can not be deleted, due to the way finalizers work in Kubernetes. The operator might be unable to clean up the data for many reasons, for example if the M3DB cluster itself is not available to serve the APIs for cleanup or if etcd is down and cannot fulfill the deleted. To allow the CRD to be deleted, you can kubectl edit m3dbcluster $CLUSTER and remove the operator.m3db.io/etcd-deletion finalizer. For example, in the following cluster you'd remove the finalizer from metadata.finalizers : apiVersion : operator.m3db.io/v1alpha1 kind : M3DBCluster metadata : ... finalizers : - operator.m3db.io/etcd-deletion name : m3db-cluster ... Note that if you do this, you'll have to manually remove the relevant data in etcd. For a cluster in namespace $NS with name $CLUSTER , the keys are: _sd.placement/$NS/$CLUSTER/m3db _kv/$NS/$CLUSTER/m3db.node.namespaces","title":"Deleting a Cluster"},{"location":"getting_started/delete_cluster/#deleting-a-cluster","text":"Delete your M3DB cluster with kubectl : kubectl delete m3dbcluster simple-cluster By default, the operator will delete the placement and namespaces associated with a cluster before the CRD resource deleted. If you do not want this behavior, set keepEtcdDataOnDelete to true on your cluster spec. Under the hood, the operator uses Kubernetes finalizers to ensure the cluster CRD is not deleted until the operator has had a chance to do cleanup.","title":"Deleting a Cluster"},{"location":"getting_started/delete_cluster/#debugging-stuck-cluster-deletion","text":"If for some reason the operator is unable to delete the placement and namespace for the cluster, the cluster CRD itself will be stuck in a state where it can not be deleted, due to the way finalizers work in Kubernetes. The operator might be unable to clean up the data for many reasons, for example if the M3DB cluster itself is not available to serve the APIs for cleanup or if etcd is down and cannot fulfill the deleted. To allow the CRD to be deleted, you can kubectl edit m3dbcluster $CLUSTER and remove the operator.m3db.io/etcd-deletion finalizer. For example, in the following cluster you'd remove the finalizer from metadata.finalizers : apiVersion : operator.m3db.io/v1alpha1 kind : M3DBCluster metadata : ... finalizers : - operator.m3db.io/etcd-deletion name : m3db-cluster ... Note that if you do this, you'll have to manually remove the relevant data in etcd. For a cluster in namespace $NS with name $CLUSTER , the keys are: _sd.placement/$NS/$CLUSTER/m3db _kv/$NS/$CLUSTER/m3db.node.namespaces","title":"Debugging Stuck Cluster Deletion"},{"location":"getting_started/installation/","text":"Installation Be sure to take a look at the requirements before installing the operator. Helm Add the m3db-operator repo: helm repo add m3db https://m3-helm-charts.storage.googleapis.com/stable Install the m3db-operator chart: helm install m3db-operator m3db/m3db-operator Note : If uninstalling an instance of the operator that was installed with Helm, some resources such as the ClusterRole, ClusterRoleBinding, and ServiceAccount may need to be deleted manually. Manually Install the bundled operator manifests in the current namespace: kubectl apply -f https://raw.githubusercontent.com/m3db/m3db-operator/master/bundle.yaml","title":"Installation"},{"location":"getting_started/installation/#installation","text":"Be sure to take a look at the requirements before installing the operator.","title":"Installation"},{"location":"getting_started/installation/#helm","text":"Add the m3db-operator repo: helm repo add m3db https://m3-helm-charts.storage.googleapis.com/stable Install the m3db-operator chart: helm install m3db-operator m3db/m3db-operator Note : If uninstalling an instance of the operator that was installed with Helm, some resources such as the ClusterRole, ClusterRoleBinding, and ServiceAccount may need to be deleted manually.","title":"Helm"},{"location":"getting_started/installation/#manually","text":"Install the bundled operator manifests in the current namespace: kubectl apply -f https://raw.githubusercontent.com/m3db/m3db-operator/master/bundle.yaml","title":"Manually"},{"location":"getting_started/monitoring/","text":"Monitoring M3DB exposes metrics via a Prometheus endpoint. If using the Prometheus Operator , you can apply a ServiceMonitor to have your M3DB pods automatically scraped by Prometheus: kubectl apply -f https://raw.githubusercontent.com/m3db/m3db-operator/master/example/prometheus-servicemonitor.yaml You can visit the \"targets\" page of the Prometheus UI to verify the pods are being scraped. To view these metrics using Grafana, follow the M3 docs to install the M3DB Grafana dashboard.","title":"Monitoring"},{"location":"getting_started/monitoring/#monitoring","text":"M3DB exposes metrics via a Prometheus endpoint. If using the Prometheus Operator , you can apply a ServiceMonitor to have your M3DB pods automatically scraped by Prometheus: kubectl apply -f https://raw.githubusercontent.com/m3db/m3db-operator/master/example/prometheus-servicemonitor.yaml You can visit the \"targets\" page of the Prometheus UI to verify the pods are being scraped. To view these metrics using Grafana, follow the M3 docs to install the M3DB Grafana dashboard.","title":"Monitoring"},{"location":"getting_started/requirements/","text":"Requirements Kubernetes Versions The M3DB operator current targets Kubernetes 1.11 and 1.12. Given the operator's current production use cases at Uber, we typically target the two most recent minor Kubernetes versions supported by GKE. We welcome community contributions to support more recent versions while meeting the aforementioned GKE targets! Multi-Zone Kubernetes Cluster The M3DB operator is intended to be used with Kubernetes clusters that span at least 3 zones within a region to create highly available clusters and maintain quorum in the event of region failures. Instructions for creating regional clusters on GKE can be found here . Etcd M3DB stores its cluster topology and all other runtime metadata in etcd . For testing / non-production use cases , we provide simple manifests for running etcd on Kubernetes in our example manifests : one for running ephemeral etcd containers and one for running etcd using basic persistent volumes. If using the etcd-pd yaml manifest, we recommend a modification to use a StorageClass equivalent to your cloud provider's fastest remote disk (such as pd-ssd on GCP). For production use cases, we recommend running etcd (in order of preference): External to your Kubernetes cluster to avoid circular dependencies. Using the etcd operator .","title":"Requirements"},{"location":"getting_started/requirements/#requirements","text":"","title":"Requirements"},{"location":"getting_started/requirements/#kubernetes-versions","text":"The M3DB operator current targets Kubernetes 1.11 and 1.12. Given the operator's current production use cases at Uber, we typically target the two most recent minor Kubernetes versions supported by GKE. We welcome community contributions to support more recent versions while meeting the aforementioned GKE targets!","title":"Kubernetes Versions"},{"location":"getting_started/requirements/#multi-zone-kubernetes-cluster","text":"The M3DB operator is intended to be used with Kubernetes clusters that span at least 3 zones within a region to create highly available clusters and maintain quorum in the event of region failures. Instructions for creating regional clusters on GKE can be found here .","title":"Multi-Zone Kubernetes Cluster"},{"location":"getting_started/requirements/#etcd","text":"M3DB stores its cluster topology and all other runtime metadata in etcd . For testing / non-production use cases , we provide simple manifests for running etcd on Kubernetes in our example manifests : one for running ephemeral etcd containers and one for running etcd using basic persistent volumes. If using the etcd-pd yaml manifest, we recommend a modification to use a StorageClass equivalent to your cloud provider's fastest remote disk (such as pd-ssd on GCP). For production use cases, we recommend running etcd (in order of preference): External to your Kubernetes cluster to avoid circular dependencies. Using the etcd operator .","title":"Etcd"},{"location":"getting_started/update_cluster/","text":"Updating a Cluster After your cluster has been running for some time you may decide you want to change the cluster's spec. For instance, you may want to upgrade to a newer release of M3DB or modify the cluster's config file. The operator can be used to safely rollout such changes so you don't need to do anything other than add an annotation to enable updates. The first step in updating a cluster is to update the cluster's M3DBCluster CRD with the changes you want to make. If you manage your cluster via manifests stored in YAML files then this is as simple as updating the manifest and applying your changes: kubectl apply -f example/my-cluster.yaml As a precaution, the operator won't immediately begin updating a cluster after your changes have been applied. Instead, you'll need to add the following annotation on each StatefulSet in the cluster to indicate to the operator that it is safe to update that StatefulSet : kubectl annotate statefulset my-cluster-rep0 operator.m3db.io/update = enabled When the operator sees this annotation, it will check if the current state of the StatefulSet differs from its desired state as defined by the M3DBCluster CRD. If so, the operator will update the StatefulSet to match its desired state, thereby triggering a rollout of the pods in the StatefulSet . The operator will also remove the operator.m3db.io/update=enabled annotation from the updated StatefulSet . If, on the other hand, the operator finds the update annotation on a StatefulSet but it doesn't need to be updated then the operator will remove the annotation but perform no other actions. Consequently, once you set the update annotation on a StatefulSet , you can watch for the annotation to be removed from it to know if the operator has seen and checked for an update. Since M3DB rollouts can take longer periods of time, it's often more convenient to set the annotation to enable updates on each StatefulSet in the cluster at once, and allow the operator to perform the rollout safely. The operator will update only one StatefulSet at a time and then wait for it to bootstrap and become healthy again before moving onto the next StatefulSet in the cluster so that no two replicas are ever down at the same time. kubectl annotate statefulset -l operator.m3db.io/cluster = my-cluster operator.m3db.io/update = enabled","title":"Updating a Cluster"},{"location":"getting_started/update_cluster/#updating-a-cluster","text":"After your cluster has been running for some time you may decide you want to change the cluster's spec. For instance, you may want to upgrade to a newer release of M3DB or modify the cluster's config file. The operator can be used to safely rollout such changes so you don't need to do anything other than add an annotation to enable updates. The first step in updating a cluster is to update the cluster's M3DBCluster CRD with the changes you want to make. If you manage your cluster via manifests stored in YAML files then this is as simple as updating the manifest and applying your changes: kubectl apply -f example/my-cluster.yaml As a precaution, the operator won't immediately begin updating a cluster after your changes have been applied. Instead, you'll need to add the following annotation on each StatefulSet in the cluster to indicate to the operator that it is safe to update that StatefulSet : kubectl annotate statefulset my-cluster-rep0 operator.m3db.io/update = enabled When the operator sees this annotation, it will check if the current state of the StatefulSet differs from its desired state as defined by the M3DBCluster CRD. If so, the operator will update the StatefulSet to match its desired state, thereby triggering a rollout of the pods in the StatefulSet . The operator will also remove the operator.m3db.io/update=enabled annotation from the updated StatefulSet . If, on the other hand, the operator finds the update annotation on a StatefulSet but it doesn't need to be updated then the operator will remove the annotation but perform no other actions. Consequently, once you set the update annotation on a StatefulSet , you can watch for the annotation to be removed from it to know if the operator has seen and checked for an update. Since M3DB rollouts can take longer periods of time, it's often more convenient to set the annotation to enable updates on each StatefulSet in the cluster at once, and allow the operator to perform the rollout safely. The operator will update only one StatefulSet at a time and then wait for it to bootstrap and become healthy again before moving onto the next StatefulSet in the cluster so that no two replicas are ever down at the same time. kubectl annotate statefulset -l operator.m3db.io/cluster = my-cluster operator.m3db.io/update = enabled","title":"Updating a Cluster"}]}