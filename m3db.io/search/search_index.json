{
    "docs": [
        {
            "location": "/",
            "text": "M3\n\n\nAbout\n\n\nAfter using open-source metrics solutions and finding issues with them at scale \u2013 such as reliability, cost, and\noperational complexity \u2013 \nM3\n was created from the ground up to provide Uber with a\nnative, distributed time series database, a highly-dynamic and performant aggregation service, a query engine, and\nother supporting infrastructure.\n\n\nKey Features\n\n\nM3 has several features, provided as discrete components, which make it an ideal platform for time series data at scale:\n\n\n\n\nA distributed time series database, \nM3DB\n, that provides scalable storage for time series data and a reverse index.\n\n\nA sidecar process, \nM3Coordinator\n, that allows M3DB to act as the long-term storage for Prometheus.\n\n\nA distributed query engine, \nM3Query\n, with native support for PromQL and Graphite (M3QL coming soon).\n\n\n\nAn aggregation tier, M3Aggregator, that runs as a dedicated metrics aggregator/downsampler allowing metrics to be stored at various retentions at different resolutions.\n\n\n\n\nGetting Started\n\n\nNote:\n Make sure to read our \nOperational Guides\n before running in production!\n\n\nGetting started with M3 is as easy as following one of the How-To guides.\n\n\n\n\nSingle M3DB node deployment\n\n\nClustered M3DB deployment\n\n\nM3DB on Kubernetes\n\n\nIsolated M3Query on deployment\n\n\n\n\nSupport\n\n\nFor support with any issues, questions about M3 or its operation, or to leave any comments, the team can be\nreached in a variety of ways:\n\n\n\n\nSlack\n\n\nEmail\n\n\nGithub issues",
            "title": "Introduction"
        },
        {
            "location": "/#m3",
            "text": "",
            "title": "M3"
        },
        {
            "location": "/#about",
            "text": "After using open-source metrics solutions and finding issues with them at scale \u2013 such as reliability, cost, and\noperational complexity \u2013  M3  was created from the ground up to provide Uber with a\nnative, distributed time series database, a highly-dynamic and performant aggregation service, a query engine, and\nother supporting infrastructure.",
            "title": "About"
        },
        {
            "location": "/#key-features",
            "text": "M3 has several features, provided as discrete components, which make it an ideal platform for time series data at scale:   A distributed time series database,  M3DB , that provides scalable storage for time series data and a reverse index.  A sidecar process,  M3Coordinator , that allows M3DB to act as the long-term storage for Prometheus.  A distributed query engine,  M3Query , with native support for PromQL and Graphite (M3QL coming soon).  An aggregation tier, M3Aggregator, that runs as a dedicated metrics aggregator/downsampler allowing metrics to be stored at various retentions at different resolutions.",
            "title": "Key Features"
        },
        {
            "location": "/#getting-started",
            "text": "Note:  Make sure to read our  Operational Guides  before running in production!  Getting started with M3 is as easy as following one of the How-To guides.   Single M3DB node deployment  Clustered M3DB deployment  M3DB on Kubernetes  Isolated M3Query on deployment",
            "title": "Getting Started"
        },
        {
            "location": "/#support",
            "text": "For support with any issues, questions about M3 or its operation, or to leave any comments, the team can be\nreached in a variety of ways:   Slack  Email  Github issues",
            "title": "Support"
        },
        {
            "location": "/overview/components/",
            "text": "Components\n\n\nM3 Coordinator\n\n\nM3 Coordinator is a service that coordinates reads and writes between upstream systems, such as Prometheus, and M3DB. It is a bridge that users can deploy to access the benefits of M3DB such as long term storage and multi-DC setup with other monitoring systems, such as Prometheus. See \nthis presentation\n for more on long term storage in Prometheus.\n\n\nM3DB\n\n\nM3DB is a distributed time series database that provides scalable storage and a reverse index of time series. It is optimized as a cost effective and reliable realtime and long term retention metrics store and index.  For more details, see the \nM3DB documentation\n.\n\n\nM3 Query\n\n\nM3 Query is a service that houses a distributed query engine for querying both realtime and historical metrics, supporting several different query languages. It is designed to support both low latency realtime queries and queries that can take longer to execute, aggregating over much larger datasets, for analytical use cases.  For more details, see the \nquery engine documentation\n.\n\n\nM3 Aggregator\n\n\nM3 Aggregator is a service that runs as a dedicated metrics aggregator and provides stream based downsampling, based on dynamic rules stored in etcd. It uses leader election and aggregation window tracking, leveraging etcd to manage this state, to reliably emit at-least-once aggregations for downsampled metrics to long term storage. This provides cost effective and reliable downsampling & roll up of metrics. These features also reside in the M3 Coordinator, however the dedicated aggregator is sharded and replicated, whereas the M3 Coordinator is not and requires care to deploy and run in a highly available way. There is work remaining to make the aggregator more accessible to users without requiring them to write their own compatible producer and consumer.",
            "title": "Components"
        },
        {
            "location": "/overview/components/#components",
            "text": "",
            "title": "Components"
        },
        {
            "location": "/overview/components/#m3-coordinator",
            "text": "M3 Coordinator is a service that coordinates reads and writes between upstream systems, such as Prometheus, and M3DB. It is a bridge that users can deploy to access the benefits of M3DB such as long term storage and multi-DC setup with other monitoring systems, such as Prometheus. See  this presentation  for more on long term storage in Prometheus.",
            "title": "M3 Coordinator"
        },
        {
            "location": "/overview/components/#m3db",
            "text": "M3DB is a distributed time series database that provides scalable storage and a reverse index of time series. It is optimized as a cost effective and reliable realtime and long term retention metrics store and index.  For more details, see the  M3DB documentation .",
            "title": "M3DB"
        },
        {
            "location": "/overview/components/#m3-query",
            "text": "M3 Query is a service that houses a distributed query engine for querying both realtime and historical metrics, supporting several different query languages. It is designed to support both low latency realtime queries and queries that can take longer to execute, aggregating over much larger datasets, for analytical use cases.  For more details, see the  query engine documentation .",
            "title": "M3 Query"
        },
        {
            "location": "/overview/components/#m3-aggregator",
            "text": "M3 Aggregator is a service that runs as a dedicated metrics aggregator and provides stream based downsampling, based on dynamic rules stored in etcd. It uses leader election and aggregation window tracking, leveraging etcd to manage this state, to reliably emit at-least-once aggregations for downsampled metrics to long term storage. This provides cost effective and reliable downsampling & roll up of metrics. These features also reside in the M3 Coordinator, however the dedicated aggregator is sharded and replicated, whereas the M3 Coordinator is not and requires care to deploy and run in a highly available way. There is work remaining to make the aggregator more accessible to users without requiring them to write their own compatible producer and consumer.",
            "title": "M3 Aggregator"
        },
        {
            "location": "/overview/motivation/",
            "text": "Motivation\n\n\nWe decided to open source the M3 platform as a scalable remote storage backend for Prometheus and Graphite so that others may attempt to reuse our work and avoid building yet another scalable metrics platform. As documentation for Prometheus states, it is limited by single nodes in its scalability and durability. The M3 platform aims to provide a turnkey, scalable, and configurable multi-tenant store for Prometheus, Graphite and other standard metrics schemas.",
            "title": "Motivation"
        },
        {
            "location": "/overview/motivation/#motivation",
            "text": "We decided to open source the M3 platform as a scalable remote storage backend for Prometheus and Graphite so that others may attempt to reuse our work and avoid building yet another scalable metrics platform. As documentation for Prometheus states, it is limited by single nodes in its scalability and durability. The M3 platform aims to provide a turnkey, scalable, and configurable multi-tenant store for Prometheus, Graphite and other standard metrics schemas.",
            "title": "Motivation"
        },
        {
            "location": "/overview/media/",
            "text": "Media\n\n\nBlogs\n\n\n\n\n\n\nM3: Uber\u2019s Open Source, Large-scale Metrics Platform for Prometheus\n By Rob Skillington - Aug 7, 2018.\n\n\n\n\n\n\nBuilding a Query Engine for High Cardinality Time Series Data\n By Nikunj Aggarwal and Ben Raskin - Dec 10, 2018.\n\n\n\n\n\n\nRecorded Talks\n\n\n\n\n\n\nOSCON 2019: Large-Scale Automated Storage on Kubernetes\n By Matt Schallert - Jul 18, 2019. \nSlides\n\n\n\n\n\n\nHow to get the 30,000 ft view, 1 ft view and everything in between without breaking the bank\n By Martin Mao - June 5, 2019. \nSlides\n\n\n\n\n\n\nM3 and Prometheus, Monitoring at Planet Scale for Everyone\n By Rob Skillington - May 22, 2019. \nVideo\n\n\n\n\n\n\nBuilding Operators at Uber\n By Matt Schallert & Paul Schooss - Mar 11, 2019.\n\n\n\n\n\n\nM3 and a new age of metrics and monitoring in an increasingly complex world\n By Rob Skillington - Feb 3, 2019.\n\n\n\n\n\n\nKubeCon Seattle 2018 Keynote: Smooth Operator\u266a: Large Scale Automated Storage with Kubernetes\n By Celina Ward & Matt Schallert - Dec 13, 2018.\n\n\n\n\n\n\nHow to query millions of time series efficiently\n By Martin Mao - Dec 10, 2018. \nSlides\n\n\n\n\n\n\nLearnings, patterns and Uber\u2019s metrics platform M3, open sourced as a Prometheus long term storage backend\n By Rob Skillington - Nov 5, 2018. \nSlides\n\n\n\n\n\n\nAdventures in building a high-volume Time-Series Database\n By Richard Artoul & Prateek Rungta - Nov 4, 2018.\n\n\n\n\n\n\nPromCon 2018 Lightning Talk: M3 with Prometheus\n by Nikunj Aggarwal - Aug 9, 2018.\n\n\n\n\n\n\nPromCon 2018 Panel Discussion: Prometheus Long-Term Storage Approaches\n including highlights of the M3 stack by Nikunj Aggarwal - Aug 9, 2018.\n\n\n\n\n\n\nPutting billions of time series to work at Uber with autonomous monitoring\n By Prateek Rungta - Jun 6, 2018. \nSlides",
            "title": "Media"
        },
        {
            "location": "/overview/media/#media",
            "text": "",
            "title": "Media"
        },
        {
            "location": "/overview/media/#blogs",
            "text": "M3: Uber\u2019s Open Source, Large-scale Metrics Platform for Prometheus  By Rob Skillington - Aug 7, 2018.    Building a Query Engine for High Cardinality Time Series Data  By Nikunj Aggarwal and Ben Raskin - Dec 10, 2018.",
            "title": "Blogs"
        },
        {
            "location": "/overview/media/#recorded-talks",
            "text": "OSCON 2019: Large-Scale Automated Storage on Kubernetes  By Matt Schallert - Jul 18, 2019.  Slides    How to get the 30,000 ft view, 1 ft view and everything in between without breaking the bank  By Martin Mao - June 5, 2019.  Slides    M3 and Prometheus, Monitoring at Planet Scale for Everyone  By Rob Skillington - May 22, 2019.  Video    Building Operators at Uber  By Matt Schallert & Paul Schooss - Mar 11, 2019.    M3 and a new age of metrics and monitoring in an increasingly complex world  By Rob Skillington - Feb 3, 2019.    KubeCon Seattle 2018 Keynote: Smooth Operator\u266a: Large Scale Automated Storage with Kubernetes  By Celina Ward & Matt Schallert - Dec 13, 2018.    How to query millions of time series efficiently  By Martin Mao - Dec 10, 2018.  Slides    Learnings, patterns and Uber\u2019s metrics platform M3, open sourced as a Prometheus long term storage backend  By Rob Skillington - Nov 5, 2018.  Slides    Adventures in building a high-volume Time-Series Database  By Richard Artoul & Prateek Rungta - Nov 4, 2018.    PromCon 2018 Lightning Talk: M3 with Prometheus  by Nikunj Aggarwal - Aug 9, 2018.    PromCon 2018 Panel Discussion: Prometheus Long-Term Storage Approaches  including highlights of the M3 stack by Nikunj Aggarwal - Aug 9, 2018.    Putting billions of time series to work at Uber with autonomous monitoring  By Prateek Rungta - Jun 6, 2018.  Slides",
            "title": "Recorded Talks"
        },
        {
            "location": "/overview/roadmap/",
            "text": "Roadmap\n\n\nThis roadmap is open for suggestions and currently just a small snapshot of what is coming up.\n\n\nShort:\n- Add diagrams of what using M3 looks like (broken down by use case)\n- Improve operational guides for the aggregator\n- Add tutorials for a variety of use cases\n- Add design documentation of reverse index\n- Add design documentation of aggregator\n\n\nMedium:\n- Plan what a v1.0 release looks like",
            "title": "Roadmap"
        },
        {
            "location": "/overview/roadmap/#roadmap",
            "text": "This roadmap is open for suggestions and currently just a small snapshot of what is coming up.  Short:\n- Add diagrams of what using M3 looks like (broken down by use case)\n- Improve operational guides for the aggregator\n- Add tutorials for a variety of use cases\n- Add design documentation of reverse index\n- Add design documentation of aggregator  Medium:\n- Plan what a v1.0 release looks like",
            "title": "Roadmap"
        },
        {
            "location": "/m3db/",
            "text": "M3DB, a distributed time series database\n\n\nAbout\n\n\nM3DB, inspired by \nGorilla\n and \nCassandra\n, is a distributed time series database released as open source by \nUber Technologies\n. It can be used for storing realtime metrics at long retention.\n\n\nHere are some attributes of the project:\n\n\n\n\nDistributed time series storage, single nodes use a WAL commit log and persists time windows per shard independently\n\n\nCluster management built on top of \netcd\n\n\nBuilt-in synchronous replication with configurable durability and read consistency (one, majority, all, etc)\n\n\nM3TSZ float64 compression inspired by Gorilla TSZ compression, configurable as lossless or lossy\n\n\nArbitrary time precision configurable from seconds to nanoseconds precision, able to switch precision with any write\n\n\nConfigurable out of order writes, currently limited to the size of the configured time window's block size\n\n\n\n\nCurrent Limitations\n\n\nDue to the nature of the requirements for the project, which are primarily to reduce the cost of ingesting and storing billions of timeseries and providing fast scalable reads, there are a few limitations currently that make M3DB not suitable for use as a general purpose time series database.\n\n\nThe project has aimed to avoid compactions when at all possible, currently the only compactions M3DB performs are in-memory for the mutable compressed time series window (default configured at 2 hours).  As such out of order writes are limited to the size of a single compressed time series window.  Consequently backfilling large amounts of data is not currently possible.\n\n\nThe project has also optimized the storage and retrieval of float64 values, as such there is no way to use it as a general time series database of arbitrary data structures just yet.",
            "title": "Introduction"
        },
        {
            "location": "/m3db/#m3db-a-distributed-time-series-database",
            "text": "",
            "title": "M3DB, a distributed time series database"
        },
        {
            "location": "/m3db/#about",
            "text": "M3DB, inspired by  Gorilla  and  Cassandra , is a distributed time series database released as open source by  Uber Technologies . It can be used for storing realtime metrics at long retention.  Here are some attributes of the project:   Distributed time series storage, single nodes use a WAL commit log and persists time windows per shard independently  Cluster management built on top of  etcd  Built-in synchronous replication with configurable durability and read consistency (one, majority, all, etc)  M3TSZ float64 compression inspired by Gorilla TSZ compression, configurable as lossless or lossy  Arbitrary time precision configurable from seconds to nanoseconds precision, able to switch precision with any write  Configurable out of order writes, currently limited to the size of the configured time window's block size",
            "title": "About"
        },
        {
            "location": "/m3db/#current-limitations",
            "text": "Due to the nature of the requirements for the project, which are primarily to reduce the cost of ingesting and storing billions of timeseries and providing fast scalable reads, there are a few limitations currently that make M3DB not suitable for use as a general purpose time series database.  The project has aimed to avoid compactions when at all possible, currently the only compactions M3DB performs are in-memory for the mutable compressed time series window (default configured at 2 hours).  As such out of order writes are limited to the size of a single compressed time series window.  Consequently backfilling large amounts of data is not currently possible.  The project has also optimized the storage and retrieval of float64 values, as such there is no way to use it as a general time series database of arbitrary data structures just yet.",
            "title": "Current Limitations"
        },
        {
            "location": "/m3db/architecture/",
            "text": "Architecture\n\n\nOverview\n\n\nM3DB is written entirely in Go and does not have any required dependencies. For larger deployments, one may use an etcd cluster to manage M3DB cluster membership and topology definition.\n\n\nHigh Level Goals\n\n\nSome of the high level goals for the project are defined as:\n\n\n\n\n\n\nMonitoring support:\n M3DB was primarily developed for collecting a high volume of monitoring time series data, distributing the storage in a horizontally scalable manner and most efficiently leveraging the hardware.  As such time series that are not read frequently are not kept in memory.\n\n\n\n\n\n\nHighly configurable:\n Provide a high level of configuration to support a wide set of use cases and runtime environments.\n\n\n\n\n\n\nVariable durability:\n Providing variable durability guarantees for the write and read side of storing time series data enables a wider variety of applications to use M3DB. This is why replication is primarily synchronous and is provided with configurable consistency levels, to enable consistent writes and reads. It must be possible to use M3DB with strong guarantees that data was replicated to a quorum of nodes and that the data was durable if desired.",
            "title": "Overview"
        },
        {
            "location": "/m3db/architecture/#architecture",
            "text": "",
            "title": "Architecture"
        },
        {
            "location": "/m3db/architecture/#overview",
            "text": "M3DB is written entirely in Go and does not have any required dependencies. For larger deployments, one may use an etcd cluster to manage M3DB cluster membership and topology definition.",
            "title": "Overview"
        },
        {
            "location": "/m3db/architecture/#high-level-goals",
            "text": "Some of the high level goals for the project are defined as:    Monitoring support:  M3DB was primarily developed for collecting a high volume of monitoring time series data, distributing the storage in a horizontally scalable manner and most efficiently leveraging the hardware.  As such time series that are not read frequently are not kept in memory.    Highly configurable:  Provide a high level of configuration to support a wide set of use cases and runtime environments.    Variable durability:  Providing variable durability guarantees for the write and read side of storing time series data enables a wider variety of applications to use M3DB. This is why replication is primarily synchronous and is provided with configurable consistency levels, to enable consistent writes and reads. It must be possible to use M3DB with strong guarantees that data was replicated to a quorum of nodes and that the data was durable if desired.",
            "title": "High Level Goals"
        },
        {
            "location": "/m3db/architecture/engine/",
            "text": "Storage Engine Overview\n\n\nM3DB is a time series database that was primarily designed to be horizontally scalable and able to handle high data throughput.\n\n\nTime Series Compression\n\n\nOne of M3DB's biggest strengths as a time series database (as opposed to using a more general-purpose horizontally scalable, distributed database like Cassandra) is its ability to compress time series data resulting in huge memory and disk savings. There are two compression algorithms used in M3DB: M3TSZ and protobuf encoding.\n\n\nM3TSZ\n\n\nM3TSZ is used when values are floats. A variant of the streaming time series compression algorithm described in \nFacebook's Gorilla paper\n, it achieves a high compression ratio. The compression ratio will vary depending on the workload and configuration, but we found that we were able to achieve a compression ratio of 1.45 bytes/datapoint with Uber's production workloads. This was a 40% improvement over standard TSZ, which only gave us a compression ratio of 2.42 bytes/datapoint under the same conditions.\n\n\nProtobuf Encoding\n\n\nFor more complex value types, M3DB also supports generic Protobuf messages with \na few exceptions\n. The algorithm takes on a hybrid approach and uses different compression schemes depending on the field types within the Protobuf message.\n\n\nDetails on the encoding, marshaling and unmarshaling methods can be read \nhere\n.\n\n\nArchitecture\n\n\nM3DB is a persistent database with durable storage, but it is best understood via the boundary between its in-memory object layout and on-disk representations.\n\n\nIn-Memory Object Layout\n\n\n   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   \u2502                          Database                          \u2502\n   \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n   \u2502                                                            \u2502\n   \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n   \u2502   \u2502                     Namespaces                     \u2502   \u2502\n   \u2502   \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524   \u2502\n   \u2502   \u2502                                                    \u2502   \u2502\n   \u2502   \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502   \u2502\n   \u2502   \u2502   \u2502                   Shards                   \u2502   \u2502   \u2502\n   \u2502   \u2502   \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524   \u2502   \u2502\n   \u2502   \u2502   \u2502                                            \u2502   \u2502   \u2502\n   \u2502   \u2502   \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502   \u2502   \u2502\n   \u2502   \u2502   \u2502   \u2502               Series               \u2502   \u2502   \u2502   \u2502\n   \u2502   \u2502   \u2502   \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524   \u2502   \u2502   \u2502\n   \u2502   \u2502   \u2502   \u2502                                    \u2502   \u2502   \u2502   \u2502\n   \u2502   \u2502   \u2502   \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502   \u2502   \u2502   \u2502\n   \u2502   \u2502   \u2502   \u2502   \u2502           Buffer           \u2502   \u2502   \u2502   \u2502   \u2502\n   \u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502   \u2502   \u2502   \u2502\n   \u2502   \u2502   \u2502   \u2502                                    \u2502   \u2502   \u2502   \u2502\n   \u2502   \u2502   \u2502   \u2502                                    \u2502   \u2502   \u2502   \u2502\n   \u2502   \u2502   \u2502   \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502   \u2502   \u2502   \u2502\n   \u2502   \u2502   \u2502   \u2502   \u2502       Cached blocks        \u2502   \u2502   \u2502   \u2502   \u2502\n   \u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502   \u2502   \u2502   \u2502\n   \u2502   \u2502   \u2502   \u2502                ...                 \u2502   \u2502   \u2502   \u2502\n   \u2502   \u2502   \u2502   \u2502                                    \u2502   \u2502   \u2502   \u2502\n   \u2502   \u2502   \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502   \u2502   \u2502\n   \u2502   \u2502   \u2502                    ...                     \u2502   \u2502   \u2502\n   \u2502   \u2502   \u2502                                            \u2502   \u2502   \u2502\n   \u2502   \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502   \u2502\n   \u2502   \u2502                        ...                         \u2502   \u2502\n   \u2502   \u2502                                                    \u2502   \u2502\n   \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n   \u2502                            ...                             \u2502\n   \u2502                                                            \u2502\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\n\n\nThe in-memory portion of M3DB is implemented via a hierarchy of objects:\n\n\n\n\n\n\nA \ndatabase\n of which there is only one per M3DB process. The \ndatabase\n owns multiple \nnamespace\ns.\n\n\n\n\n\n\nA \nnamespace\n is similar to a table in other databases. Each \nnamespace\n has a unique name and a set of configuration options, such as data retention and block size (which we will discuss in more detail later). A namespace owns multiple \nshard\ns.\n\n\n\n\n\n\nA \nshard\n is effectively the same as a \"virtual shard\" in Cassandra in that it provides an arbitrary distribution of time series data via a simple hash of the series ID. A shard owns multiple \nseries\n.\n\n\n\n\n\n\nA \nseries\n represents a sequence of time series datapoints. For example, the CPU utilization for a host could be represented as a series with the ID \"host1.system.cpu.utilization\" and a vector of (TIMESTAMP, CPU_LEVEL) tuples. Visualizing this example in a graph, there would a single line with time on the x-axis and CPU utilization on the y-axis. A \nseries\n owns a \nbuffer\n and any cached \nblock\ns.\n\n\n\n\n\n\nThe \nbuffer\n is where all data that has yet to be written to disk gets stored in memory. This includes both new writes to M3DB and data obtained through bootstrapping. More details on the \nbuffer\n is explained below. Upon \nflushing\n, the buffer creates a \nblock\n of its data to be persisted to disk.\n\n\n\n\n\n\nA \nblock\n represents a stream of compressed time series data for a pre-configured block size, for example, a block could hold data for 6-8PM (block size of two hours). A \nblock\n can arrive directly into the series only as a result of getting cached after a read request. Since blocks are in a compressed format, individual datapoints cannot be read from it. In other words, in order to read a single datapoint, the entire block up to that datapoint needs to be decompressed beforehand.\n\n\n\n\n\n\nPersistent storage\n\n\nWhile in-memory databases can be useful (and M3DB supports operating in a memory-only mode), some form of persistence is required for durability. In other words, without a persistence strategy, it would be impossible for M3DB to restart (or recover from a crash) without losing all of its data.\n\n\nIn addition, with large volumes of data, it becomes prohibitively expensive to keep all of the data in memory. This is especially true for monitoring workloads which often follow a \"write-once, read-never\" pattern where less than a few percent of all the data that's stored is ever read. With that type of workload, it's wasteful to keep all of that data in memory when it could be persisted on disk and retrieved when required.\n\n\nM3DB takes a two-pronged approach to persistant storage that involves combining a \ncommit log\n for disaster recovery with periodic flushing (writing \nfileset files\n to disk) for efficient retrieval:\n\n\n\n\nAll writes are persisted to a commit log (the commit log can be configured to fsync every write, or optionally batch writes together which is much faster but leaves open the possibility of small amounts of data loss in the case of a catastrophic failure). The commit log is completely uncompressed and exists only to recover unflushed data in the case of a database shutdown (intentional or not) and is never used to satisfy a read request.\n\n\nPeriodically (based on the configured block size), all data in the buffer is flushed to disk as immutable \nfileset files\n. These files are highly compressed and can be indexed into via their complementary index files. Check out the \nflushing section\n to learn more about the background flushing process.\n\n\n\n\nThe block size parameter is the most important variable that needs to be tuned for a particular workload. A small block size will mean more frequent flushing and a smaller memory footprint for the data that is being actively compressed, but it will also reduce the compression ratio and data will take up more space on disk.\n\n\nIf the database is stopped for any reason in between flushes, then when the node is started back up those writes will be recovered by reading the commit log or streaming in the data from a peer responsible for the same shard (if the replication factor is larger than one).\n\n\nWhile the fileset files are designed to support efficient data retrieval via the series ID, there is still a heavy cost associated with any query that has to retrieve data from disk because going to disk is always much slower than accessing main memory. To compensate for that, M3DB supports various \ncaching policies\n which can significantly improve the performance of reads by caching data in memory.\n\n\nWrite Path\n\n\nWe now have enough context of M3DB's architecture to discuss the lifecycle of a write. A write begins when an M3DB client calls the \nwriteBatchRaw\n endpoint on M3DB's embedded thrift server. The write itself will contain the following information:\n\n\n\n\nThe namespace\n\n\nThe series ID (byte blob)\n\n\nThe timestamp\n\n\nThe value itself\n\n\n\n\nM3DB will consult the database object to check if the namespace exists, and if it does, then it will hash the series ID to determine which shard it belongs to. If the node receiving the write owns that shard, then it will lookup the series in the shard object. If the series exists, then an encoder in the buffer will encode the datapoint into the compressed stream. If the encoder doesn't exist (no writes for this series have occurred yet as part of this block) then a new encoder will be allocated and it will begin a compressed M3TSZ stream with that datapoint. There is also some additional logic for handling multiple encoders and filesets which is discussed in the \nbuffer\n section.\n\n\nAt the same time, the write will be appended to the commit log, which is periodically compacted via a snapshot process. Details of this is outlined in the \ncommit log\n page.\n\n\nNote:\n Regardless of the success or failure of the write in a single node, the client will return a success or failure to the caller for the write based on the configured \nconsistency level\n.\n\n\nRead Path\n\n\nA read begins when an M3DB client calls the \nFetchBatchResult\n or \nFetchBlocksRawResult\n endpoint on M3DB's embedded thrift server. The read request will contain the following information:\n\n\n\n\nThe namespace\n\n\nThe series ID (byte blob)\n\n\nThe period of time being requested (start and end)\n\n\n\n\nM3DB will consult the database object to check if the namespace exists, and if it does, it will hash the series ID to determine which shard it belongs to. If the node receiving the read owns that shard, then M3DB needs to determine two things:\n\n\n\n\nWhether the series exists and if it does,\n\n\nWhether the data exists in the buffer, cached in-memory, on disk, or some combination of all three.\n\n\n\n\nDetermining whether the series exists is simple. M3DB looks up the series in the shard object. If it exists, then the series exists. If it doesn't, then M3DB consults in-memory bloom filters(s) for all shard/block start combinations(s) that overlap the query range to determine if the series exists on disk.\n\n\nIf the series exists, then for every block that the request spans, M3DB needs to consolidate data from the buffer, in-memory cache, and fileset files (disk).\n\n\nLet's imagine a read for a given series that requests the last 6 hours worth of data, and an M3DB namespace that is configured with a block size of 2 hours, i.e. we need to find 3 different blocks.\n\n\nIf the current time is 8PM, then the location of the requested blocks might be as follows:\n\n\n[2PM - 4PM (fileset file)]                - Flushed block that isn't cached\n[4PM - 6PM (in-memory cache)]             - Flushed block that is cached\n[4PM - 6PM (cold write in active buffer)] - Cold write that hasn't been flushed yet\n[6PM - 8PM (active buffer)]               - Hasn't been flushed yet\n\n\n\n\nThen M3DB will need to consolidate:\n\n\n\n\nThe not-yet-sealed block from the buffer (located inside an internal lookup in the Series object) \n[6PM - 8PM]\n\n\nThe in-memory cached block (also located inside an internal lookup in the Series object). Since there are also cold writes in this block, the cold writes will be consolidated in memory with data found in the cached block before returning. \n[4PM - 6PM]\n\n\nThe block from disk (the block will be retrieved from disk and will then be cached according to the current \ncaching policy\n) \n[2PM - 4PM]\n\n\n\n\nRetrieving blocks from the buffer and in-memory cache is simple, the data is already present in memory and easily accessible via hashmaps keyed by series ID. Retrieving a block from disk is more complicated. The flow for retrieving a block from disk is as follows:\n\n\n\n\nConsult the in-memory bloom filter to determine if it's possible the series exists on disk.\n\n\nIf the bloom filter returns negative, we are sure that the series isn't there, so return that result. If the bloom filter returns positive, then binary search the in-memory index summaries to find the nearest index entry that is \nbefore\n the series ID that we're searching for. Review the \nindex_lookup.go\n file for implementation details.\n\n\nJump to the offset in the index file that we obtained from the binary search in the previous step, and begin scanning forward until we identify the index entry for the series ID we're looking for \nor\n we get far enough in the index file that it becomes clear that the ID we're looking for doesn't exist (this is possible because the index file is sorted by ID)\n\n\nJump to the offset in the data file that we obtained from scanning the index file in the previous step, and begin streaming data.\n\n\n\n\nOnce M3DB has retrieved the three blocks from their respective locations in memory / on-disk, it will transmit all of the data back to the client. Whether or not the client returns a success to the caller for the read is dependent on the configured \nconsistency level\n.\n\n\nNote:\n Since M3DB nodes return compressed blocks (the M3DB client decompresses them), it's not possible to return \"partial results\" for a given block. If any portion of a read request spans a given block, then that block in its entirety must be transmitted back to the client. In practice, this ends up being not much of an issue because of the high compression ratio that M3DB is able to achieve.\n\n\nBuffer\n\n\nEach series object contains a buffer, which is in charge of handling all data that has yet to be flushed - new writes and bootstrapped data. To accomplish this, it keeps mutable \"buckets\" of encoders (for new writes) and immutable blocks (for bootstrapped data). M3TSZ, the database's encoding scheme, is designed for compressing time series data in which each datapoint has a timestamp that is larger than the last encoded datapoint. For metrics workloads this works very well because every subsequent datapoint is almost always after the previous one. However, out of order writes will occasionally be received, for example due to clock skew. When this happens, M3DB will allocate a new encoder for the out of order datapoints. These encoders are contained in a bucket along with any blocks that got bootstrapped.\n\n\nUpon a flush (discussed further below), all data within a bucket gets merged and its version gets incremented - the specific version it gets set to depends on the number of times this block has previously been flushed. This bucket versioning allows the buffer to know which data has been flushed so that subsequent flushes will not try to flush it again. It also indicates to the clean up process (also discussed below) that that data can be evicted.\n\n\nGiven this complex, concurrent logic, this has been \nmodeled in TLA\n.\n\n\n           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n           \u2502          Buffer         \u2502\n           \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n           \u2502                         \u2502\n           \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n           \u2502   \u2502  2-4PM buckets  \u2502   \u2502\n           \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n           \u2502                         \u2502\n           \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n      \u250c\u2500\u2500\u2500\u2500\u2502\u2500\u2500\u2500\u2502  4-6PM buckets  \u2502   |\n      \u2502    \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n      \u2502    \u2502                         \u2502\n      \u2502    \u2502           ...           \u2502\n      \u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n      \u2502\n      \u2502\n      v                               After flush:\n   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   \u2502    4-6PM buckets    \u2502            \u2502    4-6PM buckets    \u2502\n   \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524            \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n   \u2502                     \u2502            \u2502                     \u2502\n   \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502            \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n   \u2502   \u2502  Bucket v0  \u2502<--\u2502--writes    \u2502   \u2502  Bucket v3  \u2502   \u2502\n   \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502            \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n   \u2502                     \u2502            \u2502                     \u2502\n   \u2502                     \u2502            \u2502                     \u2502\n   \u2502                     \u2502            \u2502                     \u2502\n   \u2502                     \u2502            \u2502                     \u2502\n   \u2502                     \u2502            \u2502                     \u2502\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\n   More writes after flush:           After clean up:\n   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   \u2502    4-6PM buckets    \u2502            \u2502    4-6PM buckets    \u2502\n   \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524            \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n   \u2502                     \u2502            \u2502                     \u2502\n   \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502            \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n   \u2502   \u2502  Bucket v3  \u2502   \u2502            \u2502   \u2502  Bucket v0  \u2502<--\u2502--writes\n   \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502            \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n   \u2502                     \u2502            \u2502                     \u2502\n   \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502            \u2502                     \u2502\n   \u2502   \u2502  Bucket v0  \u2502<--\u2502--writes    \u2502                     \u2502\n   \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502            \u2502                     \u2502\n   \u2502                     \u2502            \u2502                     \u2502\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\n\n\nBackground processes\n\n\nM3DB has a variety of processes that run in the background during normal operation.\n\n\nFlushing\n\n\nAs discussed in the \narchitecture\n section, writes are actively buffered / compressed in memory and the commit log is continuously being written to, but eventually data needs to be flushed to disk in the form of \nfileset files\n to facilitate efficient storage and retrieval.\n\n\nThis is where the configurable \"block size\" comes into play. The block size is simply a duration of time that dictates how long new writes will be compressed (in a streaming manner) in memory before being flushed to disk. Let's use a block size of two hours as an example.\n\n\nIf the block size is set to two hours, then all writes for all series for a given shard will be buffered in memory for two hours at a time. At the end of the two hour period all of the fileset files will be generated, written to disk, and then the in-memory objects can be released and replaced with new ones for the new block. The old objects will be removed from memory in the subsequent tick.\n\n\nIf a flush happens for a namespace/shard/series/block for which there is already a fileset, in-memory data will get merged with data on disk from the fileset. The resultant merged data will then be flushed as a separate fileset.\n\n\nTicking\n\n\nThe ticking process runs continously in the background and is responsible for a variety of tasks:\n\n\n\n\nMerging all encoders for a given series / block start combination\n\n\nRemoving expired / flushed series and blocks from memory\n\n\nClean up of expired data (fileset/commit log) from the filesystem\n\n\n\n\nMerging all encoders\n\n\nIf there are multiple encoders for a block, they need to be merged before flushing the data to disk. To prevent huge memory spikes during the flushing process we continuously merge out of order encoders in the background.\n\n\nRemoving expired / flushed series and blocks from memory\n\n\nDepending on the configured \ncaching policy\n, the \nin-memory object layout\n can end up with references to series or data blocks that are expired (have fallen out of the retention period) or no longer needed to be in memory (due to the data being flushed to disk or no longer needing to be cached). The background tick will identify these structures and release them from memory.\n\n\nClean up of expired data\n\n\nFileset files can become no longer necessary for two reasons:\n\n\n\n\nThe fileset files for a block that has fallen out of retention\n\n\nA flush occurred for a block that already has a fileset file. The new fileset will be a superset of the existing fileset with any new data that for that block, hence, the existing fileset is no longer required\n\n\n\n\nDuring the clean up process, these fileset files will get deleted.\n\n\nCaveats / Limitations\n\n\n\n\nCurrently M3DB does not support deletes.\n\n\nM3DB does not support storing data with an indefinite retention period, every namespace in M3DB is required to have a retention policy which specifies how long data in that namespace will be retained for. While there is no upper bound on that value, it's still required and generally speaking M3DB is optimized for workloads with a well-defined \nTTL\n.\n\n\nM3DB does not support either background data repair or Cassandra-style \nread repairs\n. Future versions of M3DB will support automatic repairs of data as an ongoing background process.\n\n\nM3DB does not support writing far into the future. Support for this will be added in future.",
            "title": "Storage Engine"
        },
        {
            "location": "/m3db/architecture/engine/#storage-engine-overview",
            "text": "M3DB is a time series database that was primarily designed to be horizontally scalable and able to handle high data throughput.",
            "title": "Storage Engine Overview"
        },
        {
            "location": "/m3db/architecture/engine/#time-series-compression",
            "text": "One of M3DB's biggest strengths as a time series database (as opposed to using a more general-purpose horizontally scalable, distributed database like Cassandra) is its ability to compress time series data resulting in huge memory and disk savings. There are two compression algorithms used in M3DB: M3TSZ and protobuf encoding.",
            "title": "Time Series Compression"
        },
        {
            "location": "/m3db/architecture/engine/#m3tsz",
            "text": "M3TSZ is used when values are floats. A variant of the streaming time series compression algorithm described in  Facebook's Gorilla paper , it achieves a high compression ratio. The compression ratio will vary depending on the workload and configuration, but we found that we were able to achieve a compression ratio of 1.45 bytes/datapoint with Uber's production workloads. This was a 40% improvement over standard TSZ, which only gave us a compression ratio of 2.42 bytes/datapoint under the same conditions.",
            "title": "M3TSZ"
        },
        {
            "location": "/m3db/architecture/engine/#protobuf-encoding",
            "text": "For more complex value types, M3DB also supports generic Protobuf messages with  a few exceptions . The algorithm takes on a hybrid approach and uses different compression schemes depending on the field types within the Protobuf message.  Details on the encoding, marshaling and unmarshaling methods can be read  here .",
            "title": "Protobuf Encoding"
        },
        {
            "location": "/m3db/architecture/engine/#architecture",
            "text": "M3DB is a persistent database with durable storage, but it is best understood via the boundary between its in-memory object layout and on-disk representations.",
            "title": "Architecture"
        },
        {
            "location": "/m3db/architecture/engine/#in-memory-object-layout",
            "text": "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   \u2502                          Database                          \u2502\n   \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n   \u2502                                                            \u2502\n   \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n   \u2502   \u2502                     Namespaces                     \u2502   \u2502\n   \u2502   \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524   \u2502\n   \u2502   \u2502                                                    \u2502   \u2502\n   \u2502   \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502   \u2502\n   \u2502   \u2502   \u2502                   Shards                   \u2502   \u2502   \u2502\n   \u2502   \u2502   \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524   \u2502   \u2502\n   \u2502   \u2502   \u2502                                            \u2502   \u2502   \u2502\n   \u2502   \u2502   \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502   \u2502   \u2502\n   \u2502   \u2502   \u2502   \u2502               Series               \u2502   \u2502   \u2502   \u2502\n   \u2502   \u2502   \u2502   \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524   \u2502   \u2502   \u2502\n   \u2502   \u2502   \u2502   \u2502                                    \u2502   \u2502   \u2502   \u2502\n   \u2502   \u2502   \u2502   \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502   \u2502   \u2502   \u2502\n   \u2502   \u2502   \u2502   \u2502   \u2502           Buffer           \u2502   \u2502   \u2502   \u2502   \u2502\n   \u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502   \u2502   \u2502   \u2502\n   \u2502   \u2502   \u2502   \u2502                                    \u2502   \u2502   \u2502   \u2502\n   \u2502   \u2502   \u2502   \u2502                                    \u2502   \u2502   \u2502   \u2502\n   \u2502   \u2502   \u2502   \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502   \u2502   \u2502   \u2502\n   \u2502   \u2502   \u2502   \u2502   \u2502       Cached blocks        \u2502   \u2502   \u2502   \u2502   \u2502\n   \u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502   \u2502   \u2502   \u2502\n   \u2502   \u2502   \u2502   \u2502                ...                 \u2502   \u2502   \u2502   \u2502\n   \u2502   \u2502   \u2502   \u2502                                    \u2502   \u2502   \u2502   \u2502\n   \u2502   \u2502   \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502   \u2502   \u2502\n   \u2502   \u2502   \u2502                    ...                     \u2502   \u2502   \u2502\n   \u2502   \u2502   \u2502                                            \u2502   \u2502   \u2502\n   \u2502   \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502   \u2502\n   \u2502   \u2502                        ...                         \u2502   \u2502\n   \u2502   \u2502                                                    \u2502   \u2502\n   \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n   \u2502                            ...                             \u2502\n   \u2502                                                            \u2502\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  The in-memory portion of M3DB is implemented via a hierarchy of objects:    A  database  of which there is only one per M3DB process. The  database  owns multiple  namespace s.    A  namespace  is similar to a table in other databases. Each  namespace  has a unique name and a set of configuration options, such as data retention and block size (which we will discuss in more detail later). A namespace owns multiple  shard s.    A  shard  is effectively the same as a \"virtual shard\" in Cassandra in that it provides an arbitrary distribution of time series data via a simple hash of the series ID. A shard owns multiple  series .    A  series  represents a sequence of time series datapoints. For example, the CPU utilization for a host could be represented as a series with the ID \"host1.system.cpu.utilization\" and a vector of (TIMESTAMP, CPU_LEVEL) tuples. Visualizing this example in a graph, there would a single line with time on the x-axis and CPU utilization on the y-axis. A  series  owns a  buffer  and any cached  block s.    The  buffer  is where all data that has yet to be written to disk gets stored in memory. This includes both new writes to M3DB and data obtained through bootstrapping. More details on the  buffer  is explained below. Upon  flushing , the buffer creates a  block  of its data to be persisted to disk.    A  block  represents a stream of compressed time series data for a pre-configured block size, for example, a block could hold data for 6-8PM (block size of two hours). A  block  can arrive directly into the series only as a result of getting cached after a read request. Since blocks are in a compressed format, individual datapoints cannot be read from it. In other words, in order to read a single datapoint, the entire block up to that datapoint needs to be decompressed beforehand.",
            "title": "In-Memory Object Layout"
        },
        {
            "location": "/m3db/architecture/engine/#persistent-storage",
            "text": "While in-memory databases can be useful (and M3DB supports operating in a memory-only mode), some form of persistence is required for durability. In other words, without a persistence strategy, it would be impossible for M3DB to restart (or recover from a crash) without losing all of its data.  In addition, with large volumes of data, it becomes prohibitively expensive to keep all of the data in memory. This is especially true for monitoring workloads which often follow a \"write-once, read-never\" pattern where less than a few percent of all the data that's stored is ever read. With that type of workload, it's wasteful to keep all of that data in memory when it could be persisted on disk and retrieved when required.  M3DB takes a two-pronged approach to persistant storage that involves combining a  commit log  for disaster recovery with periodic flushing (writing  fileset files  to disk) for efficient retrieval:   All writes are persisted to a commit log (the commit log can be configured to fsync every write, or optionally batch writes together which is much faster but leaves open the possibility of small amounts of data loss in the case of a catastrophic failure). The commit log is completely uncompressed and exists only to recover unflushed data in the case of a database shutdown (intentional or not) and is never used to satisfy a read request.  Periodically (based on the configured block size), all data in the buffer is flushed to disk as immutable  fileset files . These files are highly compressed and can be indexed into via their complementary index files. Check out the  flushing section  to learn more about the background flushing process.   The block size parameter is the most important variable that needs to be tuned for a particular workload. A small block size will mean more frequent flushing and a smaller memory footprint for the data that is being actively compressed, but it will also reduce the compression ratio and data will take up more space on disk.  If the database is stopped for any reason in between flushes, then when the node is started back up those writes will be recovered by reading the commit log or streaming in the data from a peer responsible for the same shard (if the replication factor is larger than one).  While the fileset files are designed to support efficient data retrieval via the series ID, there is still a heavy cost associated with any query that has to retrieve data from disk because going to disk is always much slower than accessing main memory. To compensate for that, M3DB supports various  caching policies  which can significantly improve the performance of reads by caching data in memory.",
            "title": "Persistent storage"
        },
        {
            "location": "/m3db/architecture/engine/#write-path",
            "text": "We now have enough context of M3DB's architecture to discuss the lifecycle of a write. A write begins when an M3DB client calls the  writeBatchRaw  endpoint on M3DB's embedded thrift server. The write itself will contain the following information:   The namespace  The series ID (byte blob)  The timestamp  The value itself   M3DB will consult the database object to check if the namespace exists, and if it does, then it will hash the series ID to determine which shard it belongs to. If the node receiving the write owns that shard, then it will lookup the series in the shard object. If the series exists, then an encoder in the buffer will encode the datapoint into the compressed stream. If the encoder doesn't exist (no writes for this series have occurred yet as part of this block) then a new encoder will be allocated and it will begin a compressed M3TSZ stream with that datapoint. There is also some additional logic for handling multiple encoders and filesets which is discussed in the  buffer  section.  At the same time, the write will be appended to the commit log, which is periodically compacted via a snapshot process. Details of this is outlined in the  commit log  page.  Note:  Regardless of the success or failure of the write in a single node, the client will return a success or failure to the caller for the write based on the configured  consistency level .",
            "title": "Write Path"
        },
        {
            "location": "/m3db/architecture/engine/#read-path",
            "text": "A read begins when an M3DB client calls the  FetchBatchResult  or  FetchBlocksRawResult  endpoint on M3DB's embedded thrift server. The read request will contain the following information:   The namespace  The series ID (byte blob)  The period of time being requested (start and end)   M3DB will consult the database object to check if the namespace exists, and if it does, it will hash the series ID to determine which shard it belongs to. If the node receiving the read owns that shard, then M3DB needs to determine two things:   Whether the series exists and if it does,  Whether the data exists in the buffer, cached in-memory, on disk, or some combination of all three.   Determining whether the series exists is simple. M3DB looks up the series in the shard object. If it exists, then the series exists. If it doesn't, then M3DB consults in-memory bloom filters(s) for all shard/block start combinations(s) that overlap the query range to determine if the series exists on disk.  If the series exists, then for every block that the request spans, M3DB needs to consolidate data from the buffer, in-memory cache, and fileset files (disk).  Let's imagine a read for a given series that requests the last 6 hours worth of data, and an M3DB namespace that is configured with a block size of 2 hours, i.e. we need to find 3 different blocks.  If the current time is 8PM, then the location of the requested blocks might be as follows:  [2PM - 4PM (fileset file)]                - Flushed block that isn't cached\n[4PM - 6PM (in-memory cache)]             - Flushed block that is cached\n[4PM - 6PM (cold write in active buffer)] - Cold write that hasn't been flushed yet\n[6PM - 8PM (active buffer)]               - Hasn't been flushed yet  Then M3DB will need to consolidate:   The not-yet-sealed block from the buffer (located inside an internal lookup in the Series object)  [6PM - 8PM]  The in-memory cached block (also located inside an internal lookup in the Series object). Since there are also cold writes in this block, the cold writes will be consolidated in memory with data found in the cached block before returning.  [4PM - 6PM]  The block from disk (the block will be retrieved from disk and will then be cached according to the current  caching policy )  [2PM - 4PM]   Retrieving blocks from the buffer and in-memory cache is simple, the data is already present in memory and easily accessible via hashmaps keyed by series ID. Retrieving a block from disk is more complicated. The flow for retrieving a block from disk is as follows:   Consult the in-memory bloom filter to determine if it's possible the series exists on disk.  If the bloom filter returns negative, we are sure that the series isn't there, so return that result. If the bloom filter returns positive, then binary search the in-memory index summaries to find the nearest index entry that is  before  the series ID that we're searching for. Review the  index_lookup.go  file for implementation details.  Jump to the offset in the index file that we obtained from the binary search in the previous step, and begin scanning forward until we identify the index entry for the series ID we're looking for  or  we get far enough in the index file that it becomes clear that the ID we're looking for doesn't exist (this is possible because the index file is sorted by ID)  Jump to the offset in the data file that we obtained from scanning the index file in the previous step, and begin streaming data.   Once M3DB has retrieved the three blocks from their respective locations in memory / on-disk, it will transmit all of the data back to the client. Whether or not the client returns a success to the caller for the read is dependent on the configured  consistency level .  Note:  Since M3DB nodes return compressed blocks (the M3DB client decompresses them), it's not possible to return \"partial results\" for a given block. If any portion of a read request spans a given block, then that block in its entirety must be transmitted back to the client. In practice, this ends up being not much of an issue because of the high compression ratio that M3DB is able to achieve.",
            "title": "Read Path"
        },
        {
            "location": "/m3db/architecture/engine/#buffer",
            "text": "Each series object contains a buffer, which is in charge of handling all data that has yet to be flushed - new writes and bootstrapped data. To accomplish this, it keeps mutable \"buckets\" of encoders (for new writes) and immutable blocks (for bootstrapped data). M3TSZ, the database's encoding scheme, is designed for compressing time series data in which each datapoint has a timestamp that is larger than the last encoded datapoint. For metrics workloads this works very well because every subsequent datapoint is almost always after the previous one. However, out of order writes will occasionally be received, for example due to clock skew. When this happens, M3DB will allocate a new encoder for the out of order datapoints. These encoders are contained in a bucket along with any blocks that got bootstrapped.  Upon a flush (discussed further below), all data within a bucket gets merged and its version gets incremented - the specific version it gets set to depends on the number of times this block has previously been flushed. This bucket versioning allows the buffer to know which data has been flushed so that subsequent flushes will not try to flush it again. It also indicates to the clean up process (also discussed below) that that data can be evicted.  Given this complex, concurrent logic, this has been  modeled in TLA .             \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n           \u2502          Buffer         \u2502\n           \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n           \u2502                         \u2502\n           \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n           \u2502   \u2502  2-4PM buckets  \u2502   \u2502\n           \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n           \u2502                         \u2502\n           \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n      \u250c\u2500\u2500\u2500\u2500\u2502\u2500\u2500\u2500\u2502  4-6PM buckets  \u2502   |\n      \u2502    \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n      \u2502    \u2502                         \u2502\n      \u2502    \u2502           ...           \u2502\n      \u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n      \u2502\n      \u2502\n      v                               After flush:\n   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   \u2502    4-6PM buckets    \u2502            \u2502    4-6PM buckets    \u2502\n   \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524            \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n   \u2502                     \u2502            \u2502                     \u2502\n   \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502            \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n   \u2502   \u2502  Bucket v0  \u2502<--\u2502--writes    \u2502   \u2502  Bucket v3  \u2502   \u2502\n   \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502            \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n   \u2502                     \u2502            \u2502                     \u2502\n   \u2502                     \u2502            \u2502                     \u2502\n   \u2502                     \u2502            \u2502                     \u2502\n   \u2502                     \u2502            \u2502                     \u2502\n   \u2502                     \u2502            \u2502                     \u2502\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\n   More writes after flush:           After clean up:\n   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   \u2502    4-6PM buckets    \u2502            \u2502    4-6PM buckets    \u2502\n   \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524            \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n   \u2502                     \u2502            \u2502                     \u2502\n   \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502            \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n   \u2502   \u2502  Bucket v3  \u2502   \u2502            \u2502   \u2502  Bucket v0  \u2502<--\u2502--writes\n   \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502            \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n   \u2502                     \u2502            \u2502                     \u2502\n   \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502            \u2502                     \u2502\n   \u2502   \u2502  Bucket v0  \u2502<--\u2502--writes    \u2502                     \u2502\n   \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502            \u2502                     \u2502\n   \u2502                     \u2502            \u2502                     \u2502\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518",
            "title": "Buffer"
        },
        {
            "location": "/m3db/architecture/engine/#background-processes",
            "text": "M3DB has a variety of processes that run in the background during normal operation.",
            "title": "Background processes"
        },
        {
            "location": "/m3db/architecture/engine/#flushing",
            "text": "As discussed in the  architecture  section, writes are actively buffered / compressed in memory and the commit log is continuously being written to, but eventually data needs to be flushed to disk in the form of  fileset files  to facilitate efficient storage and retrieval.  This is where the configurable \"block size\" comes into play. The block size is simply a duration of time that dictates how long new writes will be compressed (in a streaming manner) in memory before being flushed to disk. Let's use a block size of two hours as an example.  If the block size is set to two hours, then all writes for all series for a given shard will be buffered in memory for two hours at a time. At the end of the two hour period all of the fileset files will be generated, written to disk, and then the in-memory objects can be released and replaced with new ones for the new block. The old objects will be removed from memory in the subsequent tick.  If a flush happens for a namespace/shard/series/block for which there is already a fileset, in-memory data will get merged with data on disk from the fileset. The resultant merged data will then be flushed as a separate fileset.",
            "title": "Flushing"
        },
        {
            "location": "/m3db/architecture/engine/#ticking",
            "text": "The ticking process runs continously in the background and is responsible for a variety of tasks:   Merging all encoders for a given series / block start combination  Removing expired / flushed series and blocks from memory  Clean up of expired data (fileset/commit log) from the filesystem",
            "title": "Ticking"
        },
        {
            "location": "/m3db/architecture/engine/#merging-all-encoders",
            "text": "If there are multiple encoders for a block, they need to be merged before flushing the data to disk. To prevent huge memory spikes during the flushing process we continuously merge out of order encoders in the background.",
            "title": "Merging all encoders"
        },
        {
            "location": "/m3db/architecture/engine/#removing-expired-flushed-series-and-blocks-from-memory",
            "text": "Depending on the configured  caching policy , the  in-memory object layout  can end up with references to series or data blocks that are expired (have fallen out of the retention period) or no longer needed to be in memory (due to the data being flushed to disk or no longer needing to be cached). The background tick will identify these structures and release them from memory.",
            "title": "Removing expired / flushed series and blocks from memory"
        },
        {
            "location": "/m3db/architecture/engine/#clean-up-of-expired-data",
            "text": "Fileset files can become no longer necessary for two reasons:   The fileset files for a block that has fallen out of retention  A flush occurred for a block that already has a fileset file. The new fileset will be a superset of the existing fileset with any new data that for that block, hence, the existing fileset is no longer required   During the clean up process, these fileset files will get deleted.",
            "title": "Clean up of expired data"
        },
        {
            "location": "/m3db/architecture/engine/#caveats-limitations",
            "text": "Currently M3DB does not support deletes.  M3DB does not support storing data with an indefinite retention period, every namespace in M3DB is required to have a retention policy which specifies how long data in that namespace will be retained for. While there is no upper bound on that value, it's still required and generally speaking M3DB is optimized for workloads with a well-defined  TTL .  M3DB does not support either background data repair or Cassandra-style  read repairs . Future versions of M3DB will support automatic repairs of data as an ongoing background process.  M3DB does not support writing far into the future. Support for this will be added in future.",
            "title": "Caveats / Limitations"
        },
        {
            "location": "/m3db/architecture/sharding/",
            "text": "Sharding\n\n\nTimeseries keys are hashed to a fixed set of virtual shards. Virtual shards are then assigned to physical nodes. M3DB can be configured to use any hashing function and a configured number of shards. By default \nmurmur3\n is used as the hashing function and 4096 virtual shards are configured.\n\n\nBenefits\n\n\nShards provide a variety of benefits throughout the M3DB stack:\n\n\n\n\nThey make horizontal scaling easier and adding / removing nodes without downtime trivial at the cluster level.\n\n\nThey provide more fine grained lock granularity at the memory level.\n\n\nThey inform the filesystem organization in that data belonging to the same shard will be used / dropped together and can be kept in the same file.\n\n\n\n\nReplication\n\n\nLogical shards are placed per virtual shard per replica with configurable isolation (zone aware, rack aware, etc). For instance, when using rack aware isolation, the set of datacenter racks that locate a replica\u2019s data is distinct to the racks that locate all other replicas\u2019 data.\n\n\nReplication is synchronization during a write and depending on the consistency level configured will notify the client on whether a write succeeded or failed with respect to the consistency level and replication achieved.\n\n\nReplica\n\n\nEach replica has its own assignment of a single logical shard per virtual shard.\n\n\nConceptually it can be defined as:\n\n\nReplica {\n  id uint32\n  shards []Shard\n}\n\n\n\n\nShard state\n\n\nEach shard can be conceptually defined as:\n\n\nShard {\n  id uint32\n  assignments []ShardAssignment\n}\n\nShardAssignment {\n  host Host\n  state ShardState\n}\n\nenum ShardState {\n  INITIALIZING,\n  AVAILABLE,\n  LEAVING\n}\n\n\n\n\nShard assignment\n\n\nThe assignment of shards is stored in etcd. When adding, removing or replacing a node shard goal states are assigned for each shard assigned.\n\n\nFor a write to appear as successful for a given replica it must succeed against all assigned hosts for that shard.  That means if there is a given shard with a host assigned as \nLEAVING\n and another host assigned as \nINITIALIZING\n for a given replica writes to both these hosts must appear as successful to return success for a write to that given replica.  Currently however only \nAVAILABLE\n shards count towards consistency, the work to group the \nLEAVING\n and \nINITIALIZING\n shards together when calculating a write success/error is not complete, see \nissue 417\n.\n\n\nIt is up to the nodes themselves to bootstrap shards when the assignment of new shards to it are discovered in the \nINITIALIZING\n state and to transition the state to \nAVAILABLE\n once bootstrapped by calling the cluster management APIs when done.  Using a compare and set this atomically removes the \nLEAVING\n shard still assigned to the node that previously owned it and transitions the shard state on the new node from \nINITIALIZING\n state to \nAVAILABLE\n.\n\n\nNodes will not start serving reads for the new shard until it is \nAVAILABLE\n, meaning not until they have bootstrapped data for those shards.\n\n\nCluster operations\n\n\nNode add\n\n\nWhen a node is added to the cluster it is assigned shards that relieves load fairly from the existing nodes.  The shards assigned to the new node will become \nINITIALIZING\n, the nodes then discover they need to be bootstrapped and will begin bootstrapping the data using all replicas available.  The shards that will be removed from the existing nodes are marked as \nLEAVING\n.\n\n\nNode down\n\n\nA node needs to be explicitly taken out of the cluster.  If a node goes down and is unavailable the clients performing reads will be served an error from the replica for the shard range that the node owns.  During this time it will rely on reads from other replicas to continue uninterrupted operation.\n\n\nNode remove\n\n\nWhen a node is removed the shards it owns are assigned to existing nodes in the cluster.  Remaining servers discover they are now in possession of shards that are \nINITIALIZING\n and need to be bootstrapped and will begin bootstrapping the data using all replicas available.",
            "title": "Sharding and Replication"
        },
        {
            "location": "/m3db/architecture/sharding/#sharding",
            "text": "Timeseries keys are hashed to a fixed set of virtual shards. Virtual shards are then assigned to physical nodes. M3DB can be configured to use any hashing function and a configured number of shards. By default  murmur3  is used as the hashing function and 4096 virtual shards are configured.",
            "title": "Sharding"
        },
        {
            "location": "/m3db/architecture/sharding/#benefits",
            "text": "Shards provide a variety of benefits throughout the M3DB stack:   They make horizontal scaling easier and adding / removing nodes without downtime trivial at the cluster level.  They provide more fine grained lock granularity at the memory level.  They inform the filesystem organization in that data belonging to the same shard will be used / dropped together and can be kept in the same file.",
            "title": "Benefits"
        },
        {
            "location": "/m3db/architecture/sharding/#replication",
            "text": "Logical shards are placed per virtual shard per replica with configurable isolation (zone aware, rack aware, etc). For instance, when using rack aware isolation, the set of datacenter racks that locate a replica\u2019s data is distinct to the racks that locate all other replicas\u2019 data.  Replication is synchronization during a write and depending on the consistency level configured will notify the client on whether a write succeeded or failed with respect to the consistency level and replication achieved.",
            "title": "Replication"
        },
        {
            "location": "/m3db/architecture/sharding/#replica",
            "text": "Each replica has its own assignment of a single logical shard per virtual shard.  Conceptually it can be defined as:  Replica {\n  id uint32\n  shards []Shard\n}",
            "title": "Replica"
        },
        {
            "location": "/m3db/architecture/sharding/#shard-state",
            "text": "Each shard can be conceptually defined as:  Shard {\n  id uint32\n  assignments []ShardAssignment\n}\n\nShardAssignment {\n  host Host\n  state ShardState\n}\n\nenum ShardState {\n  INITIALIZING,\n  AVAILABLE,\n  LEAVING\n}",
            "title": "Shard state"
        },
        {
            "location": "/m3db/architecture/sharding/#shard-assignment",
            "text": "The assignment of shards is stored in etcd. When adding, removing or replacing a node shard goal states are assigned for each shard assigned.  For a write to appear as successful for a given replica it must succeed against all assigned hosts for that shard.  That means if there is a given shard with a host assigned as  LEAVING  and another host assigned as  INITIALIZING  for a given replica writes to both these hosts must appear as successful to return success for a write to that given replica.  Currently however only  AVAILABLE  shards count towards consistency, the work to group the  LEAVING  and  INITIALIZING  shards together when calculating a write success/error is not complete, see  issue 417 .  It is up to the nodes themselves to bootstrap shards when the assignment of new shards to it are discovered in the  INITIALIZING  state and to transition the state to  AVAILABLE  once bootstrapped by calling the cluster management APIs when done.  Using a compare and set this atomically removes the  LEAVING  shard still assigned to the node that previously owned it and transitions the shard state on the new node from  INITIALIZING  state to  AVAILABLE .  Nodes will not start serving reads for the new shard until it is  AVAILABLE , meaning not until they have bootstrapped data for those shards.",
            "title": "Shard assignment"
        },
        {
            "location": "/m3db/architecture/sharding/#cluster-operations",
            "text": "",
            "title": "Cluster operations"
        },
        {
            "location": "/m3db/architecture/sharding/#node-add",
            "text": "When a node is added to the cluster it is assigned shards that relieves load fairly from the existing nodes.  The shards assigned to the new node will become  INITIALIZING , the nodes then discover they need to be bootstrapped and will begin bootstrapping the data using all replicas available.  The shards that will be removed from the existing nodes are marked as  LEAVING .",
            "title": "Node add"
        },
        {
            "location": "/m3db/architecture/sharding/#node-down",
            "text": "A node needs to be explicitly taken out of the cluster.  If a node goes down and is unavailable the clients performing reads will be served an error from the replica for the shard range that the node owns.  During this time it will rely on reads from other replicas to continue uninterrupted operation.",
            "title": "Node down"
        },
        {
            "location": "/m3db/architecture/sharding/#node-remove",
            "text": "When a node is removed the shards it owns are assigned to existing nodes in the cluster.  Remaining servers discover they are now in possession of shards that are  INITIALIZING  and need to be bootstrapped and will begin bootstrapping the data using all replicas available.",
            "title": "Node remove"
        },
        {
            "location": "/m3db/architecture/consistencylevels/",
            "text": "Consistency Levels\n\n\nM3DB provides variable consistency levels for read and write operations, as well as cluster connection operations. These consistency levels are handled at the client level.\n\n\nWrite consistency levels\n\n\n\n\n\n\nOne:\n Corresponds to a single node succeeding for an operation to succeed.\n\n\n\n\n\n\nMajority:\n Corresponds to the majority of nodes succeeding for an operation to succeed.\n\n\n\n\n\n\nAll:\n Corresponds to all nodes succeeding for an operation to succeed.\n\n\n\n\n\n\nRead consistency levels\n\n\n\n\n\n\nOne\n: Corresponds to reading from a single node to designate success.\n\n\n\n\n\n\nUnstrictMajority\n: Corresponds to reading from the majority of nodes but relaxing the constraint when it cannot be met, falling back to returning success when reading from at least a single node after attempting reading from the majority of nodes.\n\n\n\n\n\n\nMajority\n: Corresponds to reading from the majority of nodes to designate success.\n\n\n\n\n\n\nAll:\n Corresponds to reading from all of the nodes to designate success.\n\n\n\n\n\n\nConnect consistency levels\n\n\nConnect consistency levels are used to determine when a client session is deemed as connected before operations can be attempted.\n\n\n\n\n\n\nAny:\n Corresponds to connecting to any number of nodes for all shards, this strategy will attempt to connect to all, then the majority, then one and then fallback to none and as such will always succeed.\n\n\n\n\n\n\nNone:\n Corresponds to connecting to no nodes for all shards and as such will always succeed.\n\n\n\n\n\n\nOne:\n Corresponds to connecting to a single node for all shards.\n\n\n\n\n\n\nMajority:\n Corresponds to connecting to the majority of nodes for all shards.\n\n\n\n\n\n\nAll:\n Corresponds to connecting to all of the nodes for all shards.",
            "title": "Consistency Levels"
        },
        {
            "location": "/m3db/architecture/consistencylevels/#consistency-levels",
            "text": "M3DB provides variable consistency levels for read and write operations, as well as cluster connection operations. These consistency levels are handled at the client level.",
            "title": "Consistency Levels"
        },
        {
            "location": "/m3db/architecture/consistencylevels/#write-consistency-levels",
            "text": "One:  Corresponds to a single node succeeding for an operation to succeed.    Majority:  Corresponds to the majority of nodes succeeding for an operation to succeed.    All:  Corresponds to all nodes succeeding for an operation to succeed.",
            "title": "Write consistency levels"
        },
        {
            "location": "/m3db/architecture/consistencylevels/#read-consistency-levels",
            "text": "One : Corresponds to reading from a single node to designate success.    UnstrictMajority : Corresponds to reading from the majority of nodes but relaxing the constraint when it cannot be met, falling back to returning success when reading from at least a single node after attempting reading from the majority of nodes.    Majority : Corresponds to reading from the majority of nodes to designate success.    All:  Corresponds to reading from all of the nodes to designate success.",
            "title": "Read consistency levels"
        },
        {
            "location": "/m3db/architecture/consistencylevels/#connect-consistency-levels",
            "text": "Connect consistency levels are used to determine when a client session is deemed as connected before operations can be attempted.    Any:  Corresponds to connecting to any number of nodes for all shards, this strategy will attempt to connect to all, then the majority, then one and then fallback to none and as such will always succeed.    None:  Corresponds to connecting to no nodes for all shards and as such will always succeed.    One:  Corresponds to connecting to a single node for all shards.    Majority:  Corresponds to connecting to the majority of nodes for all shards.    All:  Corresponds to connecting to all of the nodes for all shards.",
            "title": "Connect consistency levels"
        },
        {
            "location": "/m3db/architecture/storage/",
            "text": "Storage\n\n\nOverview\n\n\nThe primary unit of long-term storage for M3DB are fileset files which store compressed streams of time series values, one per shard block time window size.\n\n\nThey are flushed to disk after a block time window becomes unreachable, that is the end of the time window for which that block can no longer be written to.  If a process is killed before it has a chance to flush the data for the current time window to disk it must be restored from the commit log (or a peer that is responsible for the same shard if replication factor is larger than 1.)\n\n\nFileSets\n\n\nA fileset has the following files:\n\n\n\n\nInfo file:\n Stores the block time window start and size and other important metadata about the fileset volume.\n\n\nSummaries file:\n Stores a subset of the index file for purposes of keeping the contents in memory and jumping to section of the index file that within a few pages of linear scanning can find the series that is being looked up.\n\n\nIndex file:\n Stores the series metadata, including tags if indexing is enabled, and location of compressed stream in the data file for retrieval.\n\n\nData file:\n Stores the series compressed data streams.\n\n\nBloom filter file:\n Stores a bloom filter bitset of all series contained in this fileset for quick knowledge of whether to attempt retrieving a series for this fileset volume.\n\n\nDigests file:\n Stores the digest checksums of the info file, summaries file, index file, data file and bloom filter file in the fileset volume for integrity verification.\n\n\nCheckpoint file:\n Stores a digest of the digests file and written at the succesful completion of a fileset volume being persisted, allows for quickly checking if a volume was completed.\n\n\n\n\n                                                     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2502     Index File        \u2502\n\u2502      Info File      \u2502  \u2502   Summaries File    \u2502     \u2502   (sorted by ID)      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524  \u2502   (sorted by ID)    \u2502     \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502- Block Start        \u2502  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524  \u250c\u2500>\u2502- Idx                  \u2502\n\u2502- Block Size         \u2502  \u2502- Idx                \u2502  \u2502  \u2502- ID                   \u2502\n\u2502- Entries (Num)      \u2502  \u2502- ID                 \u2502  \u2502  \u2502- Size                 \u2502\n\u2502- Major Version      \u2502  \u2502- Index Entry Offset \u251c\u2500\u2500\u2518  \u2502- Checksum             \u2502\n\u2502- Summaries (Num)    \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502- Data Entry Offset    \u251c\u2500\u2500\u2510\n\u2502- BloomFilter (K/M)  \u2502                              \u2502- Encoded Tags         \u2502  \u2502\n\u2502- Snapshot Time      \u2502                              \u2502- Index Entry Checksum \u2502  \u2502\n\u2502- Type (Flush/Snap)  \u2502                              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502- Snapshot ID        \u2502                                                         \u2502\n\u2502- Volume Index       \u2502                                                         \u2502\n\u2502- Minor Version      \u2502                                                         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                                         \u2502\n                                                                                \u2502\n                         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502  Bloom Filter File  \u2502  \u2502\n\u2502    Digests File     \u2502  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524  \u2502- Bitset             \u2502  \u2502  \u2502      Data File      \u2502\n\u2502- Info file digest   \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502- Summaries digest   \u2502                           \u2502  \u2502List of:             \u2502\n\u2502- Index digest       \u2502                           \u2514\u2500>\u2502  - Marker (16 bytes)\u2502\n\u2502- Data digest        \u2502                              \u2502  - ID               \u2502\n\u2502- Bloom filter digest\u2502                              \u2502  - Data (size bytes)\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Checkpoint File   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502- Digests digest     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\n\n\nIn the diagram above you can see that the data file stores compressed blocks for a given shard / block start combination. The index file (which is sorted by ID and thus can be binary searched or scanned) can be used to find the offset of a specific ID.\n\n\nFileSet files will be kept for every shard / block start combination that is within the retention period. Once the files fall out of the period defined in the configurable namespace retention period they will be deleted.",
            "title": "Storage"
        },
        {
            "location": "/m3db/architecture/storage/#storage",
            "text": "",
            "title": "Storage"
        },
        {
            "location": "/m3db/architecture/storage/#overview",
            "text": "The primary unit of long-term storage for M3DB are fileset files which store compressed streams of time series values, one per shard block time window size.  They are flushed to disk after a block time window becomes unreachable, that is the end of the time window for which that block can no longer be written to.  If a process is killed before it has a chance to flush the data for the current time window to disk it must be restored from the commit log (or a peer that is responsible for the same shard if replication factor is larger than 1.)",
            "title": "Overview"
        },
        {
            "location": "/m3db/architecture/storage/#filesets",
            "text": "A fileset has the following files:   Info file:  Stores the block time window start and size and other important metadata about the fileset volume.  Summaries file:  Stores a subset of the index file for purposes of keeping the contents in memory and jumping to section of the index file that within a few pages of linear scanning can find the series that is being looked up.  Index file:  Stores the series metadata, including tags if indexing is enabled, and location of compressed stream in the data file for retrieval.  Data file:  Stores the series compressed data streams.  Bloom filter file:  Stores a bloom filter bitset of all series contained in this fileset for quick knowledge of whether to attempt retrieving a series for this fileset volume.  Digests file:  Stores the digest checksums of the info file, summaries file, index file, data file and bloom filter file in the fileset volume for integrity verification.  Checkpoint file:  Stores a digest of the digests file and written at the succesful completion of a fileset volume being persisted, allows for quickly checking if a volume was completed.                                                        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2502     Index File        \u2502\n\u2502      Info File      \u2502  \u2502   Summaries File    \u2502     \u2502   (sorted by ID)      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524  \u2502   (sorted by ID)    \u2502     \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502- Block Start        \u2502  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524  \u250c\u2500>\u2502- Idx                  \u2502\n\u2502- Block Size         \u2502  \u2502- Idx                \u2502  \u2502  \u2502- ID                   \u2502\n\u2502- Entries (Num)      \u2502  \u2502- ID                 \u2502  \u2502  \u2502- Size                 \u2502\n\u2502- Major Version      \u2502  \u2502- Index Entry Offset \u251c\u2500\u2500\u2518  \u2502- Checksum             \u2502\n\u2502- Summaries (Num)    \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502- Data Entry Offset    \u251c\u2500\u2500\u2510\n\u2502- BloomFilter (K/M)  \u2502                              \u2502- Encoded Tags         \u2502  \u2502\n\u2502- Snapshot Time      \u2502                              \u2502- Index Entry Checksum \u2502  \u2502\n\u2502- Type (Flush/Snap)  \u2502                              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502- Snapshot ID        \u2502                                                         \u2502\n\u2502- Volume Index       \u2502                                                         \u2502\n\u2502- Minor Version      \u2502                                                         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                                         \u2502\n                                                                                \u2502\n                         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502  Bloom Filter File  \u2502  \u2502\n\u2502    Digests File     \u2502  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524  \u2502- Bitset             \u2502  \u2502  \u2502      Data File      \u2502\n\u2502- Info file digest   \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502- Summaries digest   \u2502                           \u2502  \u2502List of:             \u2502\n\u2502- Index digest       \u2502                           \u2514\u2500>\u2502  - Marker (16 bytes)\u2502\n\u2502- Data digest        \u2502                              \u2502  - ID               \u2502\n\u2502- Bloom filter digest\u2502                              \u2502  - Data (size bytes)\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Checkpoint File   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502- Digests digest     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  In the diagram above you can see that the data file stores compressed blocks for a given shard / block start combination. The index file (which is sorted by ID and thus can be binary searched or scanned) can be used to find the offset of a specific ID.  FileSet files will be kept for every shard / block start combination that is within the retention period. Once the files fall out of the period defined in the configurable namespace retention period they will be deleted.",
            "title": "FileSets"
        },
        {
            "location": "/m3db/architecture/commitlogs/",
            "text": "Commit Logs And Snapshot Files\n\n\nOverview\n\n\nM3DB has a commit log that is equivalent to the commit log or write-ahead-log in other databases. The commit logs are completely uncompressed (no M3TSZ encoding), and there is one per database (multiple namespaces in a single process will share a commit log.)\n\n\nIntegrity Levels\n\n\nThere are two integrity levels available for commit logs:\n\n\n\n\nSynchronous:\n write operations must wait until it has finished writing an entry in the commit log to complete.\n\n\nBehind:\n write operations must finish enqueueing an entry to the commit log write queue to complete.\n\n\n\n\nDepending on the data loss requirements users can choose either integrity level.\n\n\nProperties\n\n\nCommit logs will be stamped by the start time, aligned and rotated by a configured time window size. To restore data for an entire block you will require the commit logs from all time commit logs that overlap the block size with buffer past subtracted from the bootstrap start range and buffer future extended onto the bootstrap end range.\n\n\nStructure\n\n\nCommit logs for a given time window are kept in a single file. An info structure keeping metadata is written to the header of the file and all consequent entries are a repeated log structure, optionally containing metadata describing the series if it's the first time a log entry for a given series appears.\n\n\nThe structures can be conceptually described as:\n\n\nCommitLogInfo {\n  start int64\n  duration int64\n  index int64\n}\n\nCommitLog {\n  created int64\n  index uint64\n  metadata bytes\n  timestamp int64\n  value float64\n  unit uint32\n  annotation bytes\n}\n\nCommitLogMetadata {\n  id bytes\n  namespace bytes\n  shard uint32\n}\n\n\n\n\nCompaction / Snapshotting\n\n\nCommit log files are compacted via the snapshotting proccess which (if enabled at the namespace level) will snapshot all data in memory into compressed files which have the same structure as the \nfileset files\n but are stored in a different location. Once these snapshot files are created, then all the commit log files whose data are captured by the snapshot files can be deleted. This can result in significant disk savings for M3DB nodes running with large block sizes and high write volume where the size of the (uncompressed) commit logs can quickly get out of hand.\n\n\nIn addition, since the snapshot files are already compressed, bootstrapping from them is much faster than bootstrapping from raw commit log files because the individual data points don't need to be decoded and then M3TSZ encoded. The M3DB node just needs to read the raw bytes off disk and load them into memory.\n\n\nCleanup\n\n\nCommit log files are automatically deleted once all the data they contain has been flushed to disk as immutable compressed filesets \nor\n all the data they contain has been captured by a compressed snapshot file. Similarly, snapshot files are deleted once all the data they contain has been flushed to disk as filesets.",
            "title": "Commit Logs"
        },
        {
            "location": "/m3db/architecture/commitlogs/#commit-logs-and-snapshot-files",
            "text": "",
            "title": "Commit Logs And Snapshot Files"
        },
        {
            "location": "/m3db/architecture/commitlogs/#overview",
            "text": "M3DB has a commit log that is equivalent to the commit log or write-ahead-log in other databases. The commit logs are completely uncompressed (no M3TSZ encoding), and there is one per database (multiple namespaces in a single process will share a commit log.)",
            "title": "Overview"
        },
        {
            "location": "/m3db/architecture/commitlogs/#integrity-levels",
            "text": "There are two integrity levels available for commit logs:   Synchronous:  write operations must wait until it has finished writing an entry in the commit log to complete.  Behind:  write operations must finish enqueueing an entry to the commit log write queue to complete.   Depending on the data loss requirements users can choose either integrity level.",
            "title": "Integrity Levels"
        },
        {
            "location": "/m3db/architecture/commitlogs/#properties",
            "text": "Commit logs will be stamped by the start time, aligned and rotated by a configured time window size. To restore data for an entire block you will require the commit logs from all time commit logs that overlap the block size with buffer past subtracted from the bootstrap start range and buffer future extended onto the bootstrap end range.",
            "title": "Properties"
        },
        {
            "location": "/m3db/architecture/commitlogs/#structure",
            "text": "Commit logs for a given time window are kept in a single file. An info structure keeping metadata is written to the header of the file and all consequent entries are a repeated log structure, optionally containing metadata describing the series if it's the first time a log entry for a given series appears.  The structures can be conceptually described as:  CommitLogInfo {\n  start int64\n  duration int64\n  index int64\n}\n\nCommitLog {\n  created int64\n  index uint64\n  metadata bytes\n  timestamp int64\n  value float64\n  unit uint32\n  annotation bytes\n}\n\nCommitLogMetadata {\n  id bytes\n  namespace bytes\n  shard uint32\n}",
            "title": "Structure"
        },
        {
            "location": "/m3db/architecture/commitlogs/#compaction-snapshotting",
            "text": "Commit log files are compacted via the snapshotting proccess which (if enabled at the namespace level) will snapshot all data in memory into compressed files which have the same structure as the  fileset files  but are stored in a different location. Once these snapshot files are created, then all the commit log files whose data are captured by the snapshot files can be deleted. This can result in significant disk savings for M3DB nodes running with large block sizes and high write volume where the size of the (uncompressed) commit logs can quickly get out of hand.  In addition, since the snapshot files are already compressed, bootstrapping from them is much faster than bootstrapping from raw commit log files because the individual data points don't need to be decoded and then M3TSZ encoded. The M3DB node just needs to read the raw bytes off disk and load them into memory.",
            "title": "Compaction / Snapshotting"
        },
        {
            "location": "/m3db/architecture/commitlogs/#cleanup",
            "text": "Commit log files are automatically deleted once all the data they contain has been flushed to disk as immutable compressed filesets  or  all the data they contain has been captured by a compressed snapshot file. Similarly, snapshot files are deleted once all the data they contain has been flushed to disk as filesets.",
            "title": "Cleanup"
        },
        {
            "location": "/m3db/architecture/peer_streaming/",
            "text": "Peer Streaming\n\n\nClient\n\n\nPeer streaming is managed by the M3DB client. It fetches all blocks from peers for a specified time range for bootstrapping purposes. It performs the following steps:\n\n\n\n\nFetch all metadata for blocks from all peers who own the specified shard\n\n\nCompares metadata from different peers and determines the best peer(s) from which to stream the actual data\n\n\nStreams the block data from peers\n\n\n\n\nSteps 1, 2 and 3 all happen concurrently. As metadata streams in, we begin determining which peer is the best source to stream a given block's data for a given series from, and then we begin streaming data from that peer while we continue to receive metadata. If the checksum for a given series block matches all three replicas then the least loaded (in terms of outstanding requests) and recently attempted will be selected to stream from. If the checksum differs for the series block across any of the peers then a fanout fetch of the series block is performed.\n\n\nIn terms of error handling, the client will respect the consistency level specified for bootstrap. This means that when fetching metadata, indefinite retry is performed until the consistency level is achieved, for instance for quorum a majority of peers must successfully return metadata. For fetching the block data, if checksum matches from all peers then one successful fetch must occur, unless bootstrap consistency level \"none\" is specified, and if checksum mismatches then the specified consistency level must be achieved when the series block fetch is fanned out to peers. Fetching block data as well will indefinitely retry until the consistency level is achieved.\n\n\nThe client supports dynamically changing the bootstrap consistency level, which is helpful in disaster scenarios where the consistency level cannot be achieved. To break the indefinite streaming attempt an operator can change the consistency level to \"none\" and a purely best-effort will be made to fetch the metadata and correspondingly to fetch the block data.\n\n\nThe diagram below depicts the control flow and concurrency (goroutines and channels) in detail:\n\n\n             \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n             \u2502                                               \u2502\n             \u2502         FetchBootstrapBlocksFromPeers         \u2502\n             \u2502                                               \u2502\n             \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                     \u2502\n                                     \u2502\n                \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                \u2502\n                \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502         Main routine          \u2502\n\u2502                               \u2502\n\u2502     1) Create metadataCh      \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 2) Spin up background routine \u2502                \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      Create with metadataCh\n                \u2502                                \u2502\n                \u2502                                \u25bc\n                \u2502                \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                \u2502                \u2502                               \u2502\n                \u2502                \u2502      Background routine       \u2502\n                \u2502                \u2502                               \u2502\n                \u2502                \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                \u2502                                \u2502\n                \u2502                          For each peer\n                \u2502                                \u2502\n                \u2502               \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                \u2502               \u2502                \u2502                 \u2502\n                \u2502               \u2502                \u2502                 \u2502\n                \u2502               \u25bc                \u25bc                 \u25bc\n                \u2502          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                \u2502          \u2502       StreamBlocksMetadataFromPeer        \u2502\n                \u2502          \u2502                                           \u2502\n                \u2502          \u2502  Stream paginated blocks metadata from a  \u2502\n                \u2502          \u2502        peer while pageToken != nil        \u2502\n                \u2502          \u2502                                           \u2502\n                \u2502          \u2502     For each blocks' metadata --> put     \u2502\n                \u2502          \u2502         metadata into metadataCh          \u2502\n                \u2502          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502           StreamBlocksFromPeers           \u2502\n\u2502                                           \u2502\n\u2502 1) Create a background goroutine (details \u2502\n\u2502               to the right)               \u2502\n\u2502                                           \u2502\n\u2502 2) Create a queue per-peer which each have\u2502\n\u2502   their own internal goroutine and will   \u2502\n\u2502   stream blocks back per-series from a    \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              specific peer                \u2502          \u2502\n\u2502                                           \u2502          \u2502\n\u2502 3) Loop through the enqueCh and pick an   \u2502 Creates with metadataCh\n\u2502appropriate peer(s) for each series (based \u2502     and enqueueCh\n\u2502on whether all the peers have the same data\u2502          \u2502\n\u2502 or not) and then put that into the queue  \u2502          \u2502\n\u2502for that peer so the data will be streamed \u2502          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2502\n                \u2502                                      \u25bc\n                \u2502    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                \u2502    \u2502   streamAndGroupCollectedBlocksMetadata (injected via    \u2502\n                \u2502    \u2502                streamMetadataFn variable)                \u2502\n                \u2502    \u2502                                                          \u2502\n                \u2502    \u2502 Loop through the metadataCh aggregating blocks metadata  \u2502\n                \u2502    \u2502per series/block combination from different peers until we\u2502\n                \u2502    \u2502   have them from all peers for a series/block metadata   \u2502\n                \u2502    \u2502   combination and then \"submit\" them to the enqueueCh    \u2502\n                \u2502    \u2502                                                          \u2502\n                \u2502    \u2502At the end, flush any remaining series/block combinations \u2502\n                \u2502    \u2502(that we received from less than N peers) into the enqueCh\u2502\n                \u2502    \u2502                         as well.                         \u2502\n                \u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                \u2502\n          For each peer\n                \u2502\n   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   \u2502            \u2502             \u2502\n   \u2502            \u2502             \u2502\n   \u25bc            \u25bc             \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 newPeerBlocksQueue (processFn = streamBlocksBatchFromPeer)  \u2502\n\u2502                                                             \u2502\n\u2502For each peer we're creating a new peerBlocksQueue which will\u2502\n\u2502     stream data blocks from a specific peer (using the      \u2502\n\u2502   streamBlocksBatchFromPeer function) and add them to the   \u2502\n\u2502                        blocksResult                         \u2502\n\u2502                                                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518",
            "title": "Peer Streaming"
        },
        {
            "location": "/m3db/architecture/peer_streaming/#peer-streaming",
            "text": "",
            "title": "Peer Streaming"
        },
        {
            "location": "/m3db/architecture/peer_streaming/#client",
            "text": "Peer streaming is managed by the M3DB client. It fetches all blocks from peers for a specified time range for bootstrapping purposes. It performs the following steps:   Fetch all metadata for blocks from all peers who own the specified shard  Compares metadata from different peers and determines the best peer(s) from which to stream the actual data  Streams the block data from peers   Steps 1, 2 and 3 all happen concurrently. As metadata streams in, we begin determining which peer is the best source to stream a given block's data for a given series from, and then we begin streaming data from that peer while we continue to receive metadata. If the checksum for a given series block matches all three replicas then the least loaded (in terms of outstanding requests) and recently attempted will be selected to stream from. If the checksum differs for the series block across any of the peers then a fanout fetch of the series block is performed.  In terms of error handling, the client will respect the consistency level specified for bootstrap. This means that when fetching metadata, indefinite retry is performed until the consistency level is achieved, for instance for quorum a majority of peers must successfully return metadata. For fetching the block data, if checksum matches from all peers then one successful fetch must occur, unless bootstrap consistency level \"none\" is specified, and if checksum mismatches then the specified consistency level must be achieved when the series block fetch is fanned out to peers. Fetching block data as well will indefinitely retry until the consistency level is achieved.  The client supports dynamically changing the bootstrap consistency level, which is helpful in disaster scenarios where the consistency level cannot be achieved. To break the indefinite streaming attempt an operator can change the consistency level to \"none\" and a purely best-effort will be made to fetch the metadata and correspondingly to fetch the block data.  The diagram below depicts the control flow and concurrency (goroutines and channels) in detail:               \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n             \u2502                                               \u2502\n             \u2502         FetchBootstrapBlocksFromPeers         \u2502\n             \u2502                                               \u2502\n             \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                     \u2502\n                                     \u2502\n                \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                \u2502\n                \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502         Main routine          \u2502\n\u2502                               \u2502\n\u2502     1) Create metadataCh      \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 2) Spin up background routine \u2502                \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      Create with metadataCh\n                \u2502                                \u2502\n                \u2502                                \u25bc\n                \u2502                \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                \u2502                \u2502                               \u2502\n                \u2502                \u2502      Background routine       \u2502\n                \u2502                \u2502                               \u2502\n                \u2502                \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                \u2502                                \u2502\n                \u2502                          For each peer\n                \u2502                                \u2502\n                \u2502               \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                \u2502               \u2502                \u2502                 \u2502\n                \u2502               \u2502                \u2502                 \u2502\n                \u2502               \u25bc                \u25bc                 \u25bc\n                \u2502          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                \u2502          \u2502       StreamBlocksMetadataFromPeer        \u2502\n                \u2502          \u2502                                           \u2502\n                \u2502          \u2502  Stream paginated blocks metadata from a  \u2502\n                \u2502          \u2502        peer while pageToken != nil        \u2502\n                \u2502          \u2502                                           \u2502\n                \u2502          \u2502     For each blocks' metadata --> put     \u2502\n                \u2502          \u2502         metadata into metadataCh          \u2502\n                \u2502          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502           StreamBlocksFromPeers           \u2502\n\u2502                                           \u2502\n\u2502 1) Create a background goroutine (details \u2502\n\u2502               to the right)               \u2502\n\u2502                                           \u2502\n\u2502 2) Create a queue per-peer which each have\u2502\n\u2502   their own internal goroutine and will   \u2502\n\u2502   stream blocks back per-series from a    \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              specific peer                \u2502          \u2502\n\u2502                                           \u2502          \u2502\n\u2502 3) Loop through the enqueCh and pick an   \u2502 Creates with metadataCh\n\u2502appropriate peer(s) for each series (based \u2502     and enqueueCh\n\u2502on whether all the peers have the same data\u2502          \u2502\n\u2502 or not) and then put that into the queue  \u2502          \u2502\n\u2502for that peer so the data will be streamed \u2502          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2502\n                \u2502                                      \u25bc\n                \u2502    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                \u2502    \u2502   streamAndGroupCollectedBlocksMetadata (injected via    \u2502\n                \u2502    \u2502                streamMetadataFn variable)                \u2502\n                \u2502    \u2502                                                          \u2502\n                \u2502    \u2502 Loop through the metadataCh aggregating blocks metadata  \u2502\n                \u2502    \u2502per series/block combination from different peers until we\u2502\n                \u2502    \u2502   have them from all peers for a series/block metadata   \u2502\n                \u2502    \u2502   combination and then \"submit\" them to the enqueueCh    \u2502\n                \u2502    \u2502                                                          \u2502\n                \u2502    \u2502At the end, flush any remaining series/block combinations \u2502\n                \u2502    \u2502(that we received from less than N peers) into the enqueCh\u2502\n                \u2502    \u2502                         as well.                         \u2502\n                \u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                \u2502\n          For each peer\n                \u2502\n   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   \u2502            \u2502             \u2502\n   \u2502            \u2502             \u2502\n   \u25bc            \u25bc             \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 newPeerBlocksQueue (processFn = streamBlocksBatchFromPeer)  \u2502\n\u2502                                                             \u2502\n\u2502For each peer we're creating a new peerBlocksQueue which will\u2502\n\u2502     stream data blocks from a specific peer (using the      \u2502\n\u2502   streamBlocksBatchFromPeer function) and add them to the   \u2502\n\u2502                        blocksResult                         \u2502\n\u2502                                                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518",
            "title": "Client"
        },
        {
            "location": "/m3db/architecture/caching/",
            "text": "Caching policies\n\n\nOverview\n\n\nBlocks that are still being actively compressed / M3TSZ encoded must be kept in memory until they are sealed and flushed to disk. Blocks that have already been sealed, however, don't need to remain in-memory. In order to support efficient reads, M3DB implements various caching policies which determine which flushed blocks are kept in memory, and which are not. The \"cache\" itself is not a separate datastructure in memory, cached blocks are simply stored in their respective \nin-memory objects\n with various different mechanisms (depending on the chosen cache policy) determining which series / blocks are evicted and which are retained.\n\n\nFor general purpose workloads, the \nlru\n caching policy is reccommended.\n\n\nNone Cache Policy\n\n\nThe \nnone\n cache policy is the simplest. As soon as a block is sealed, its flushed to disk and never retained in memory again. This cache policy will have the lowest memory consumption, but also the poorest read performance as every read for a block that is already flushed will require a disk read.\n\n\nAll Cache Policy\n\n\nThe \nall\n cache policy is the opposite of the \nnone\n cache policy. All blocks are kept in memory until their retention period is over. This policy can be useful for read-heavy workloads with small datasets, but is obviously limited by the amount of memory on the host machine. Also keep in mind that this cache policy may have unintended side-effects on write throughput as keeping every block in memory creates a lot of work for the Golang garbage collector.\n\n\nRecently Read Cache Policy\n\n\nThe \nrecently_read\n cache policy keeps all blocks that are read from disk in memory for a configurable duration of time. For example, if the \nrecently_read\n cache policy is set with a duration of 10 minutes, then everytime a block is read from disk it will be kept in memory for at least 10 minutes. This policy can be very effective if only a small portion of your overall dataset is ever read, and especially if that subset is read frequently (i.e as is common in the case of database backing an automatic alerting system), but it can cause very high memory usage during workloads that involve sequentially scanning all of the data.\n\n\nData eviction from memory is triggered by the \"ticking\" process described in the \nbackground processes section\n\n\nLeast Recently Used (LRU) Cache Policy\n\n\nThe \nlru\n cache policy uses an \nlru\n list with a configurable max size to keep track of which blocks have been read least recently, and evicts those blocks first when the capacity of the list is full and a new block needs to be read from disk. This cache policy strikes the best overall balance and is the recommended policy for general case workloads. Review the comments in \nwired_list.go\n for implementation details.",
            "title": "Caching"
        },
        {
            "location": "/m3db/architecture/caching/#caching-policies",
            "text": "",
            "title": "Caching policies"
        },
        {
            "location": "/m3db/architecture/caching/#overview",
            "text": "Blocks that are still being actively compressed / M3TSZ encoded must be kept in memory until they are sealed and flushed to disk. Blocks that have already been sealed, however, don't need to remain in-memory. In order to support efficient reads, M3DB implements various caching policies which determine which flushed blocks are kept in memory, and which are not. The \"cache\" itself is not a separate datastructure in memory, cached blocks are simply stored in their respective  in-memory objects  with various different mechanisms (depending on the chosen cache policy) determining which series / blocks are evicted and which are retained.  For general purpose workloads, the  lru  caching policy is reccommended.",
            "title": "Overview"
        },
        {
            "location": "/m3db/architecture/caching/#none-cache-policy",
            "text": "The  none  cache policy is the simplest. As soon as a block is sealed, its flushed to disk and never retained in memory again. This cache policy will have the lowest memory consumption, but also the poorest read performance as every read for a block that is already flushed will require a disk read.",
            "title": "None Cache Policy"
        },
        {
            "location": "/m3db/architecture/caching/#all-cache-policy",
            "text": "The  all  cache policy is the opposite of the  none  cache policy. All blocks are kept in memory until their retention period is over. This policy can be useful for read-heavy workloads with small datasets, but is obviously limited by the amount of memory on the host machine. Also keep in mind that this cache policy may have unintended side-effects on write throughput as keeping every block in memory creates a lot of work for the Golang garbage collector.",
            "title": "All Cache Policy"
        },
        {
            "location": "/m3db/architecture/caching/#recently-read-cache-policy",
            "text": "The  recently_read  cache policy keeps all blocks that are read from disk in memory for a configurable duration of time. For example, if the  recently_read  cache policy is set with a duration of 10 minutes, then everytime a block is read from disk it will be kept in memory for at least 10 minutes. This policy can be very effective if only a small portion of your overall dataset is ever read, and especially if that subset is read frequently (i.e as is common in the case of database backing an automatic alerting system), but it can cause very high memory usage during workloads that involve sequentially scanning all of the data.  Data eviction from memory is triggered by the \"ticking\" process described in the  background processes section",
            "title": "Recently Read Cache Policy"
        },
        {
            "location": "/m3db/architecture/caching/#least-recently-used-lru-cache-policy",
            "text": "The  lru  cache policy uses an  lru  list with a configurable max size to keep track of which blocks have been read least recently, and evicts those blocks first when the capacity of the list is full and a new block needs to be read from disk. This cache policy strikes the best overall balance and is the recommended policy for general case workloads. Review the comments in  wired_list.go  for implementation details.",
            "title": "Least Recently Used (LRU) Cache Policy"
        },
        {
            "location": "/m3coordinator/",
            "text": "M3 Coordinator, stateless API server for reading/writing metrics and M3 management\n\n\nM3 Coordinator is a service that coordinates reads and writes between upstream systems, such as Prometheus, and downstream systems, such as M3DB. \n\n\nIt also provides management APIs to setup and configure different parts of M3.\n\n\nThe coordinator is generally a bridge for read and writing different types of metrics formats and a management layer for M3.\n\n\nNote\n: M3DB by default includes the M3 Coordinator accessible on port 7201. \nFor production deployments it is recommended to deploy it as a \ndedicated service to ensure you can scale the write coordination role separately \nand independently to database nodes as an isolated application separate from \nthe M3DB database role.",
            "title": "Introduction"
        },
        {
            "location": "/m3coordinator/#m3-coordinator-stateless-api-server-for-readingwriting-metrics-and-m3-management",
            "text": "M3 Coordinator is a service that coordinates reads and writes between upstream systems, such as Prometheus, and downstream systems, such as M3DB.   It also provides management APIs to setup and configure different parts of M3.  The coordinator is generally a bridge for read and writing different types of metrics formats and a management layer for M3.  Note : M3DB by default includes the M3 Coordinator accessible on port 7201. \nFor production deployments it is recommended to deploy it as a \ndedicated service to ensure you can scale the write coordination role separately \nand independently to database nodes as an isolated application separate from \nthe M3DB database role.",
            "title": "M3 Coordinator, stateless API server for reading/writing metrics and M3 management"
        },
        {
            "location": "/m3coordinator/api/remote/",
            "text": "API\n\n\nThe M3 Coordinator implements the Prometheus Remote Read and Write HTTP endpoints, they also can be used however as general purpose metrics write and read APIs. Any metrics that are written to the remote write API can be queried using PromQL through the query APIs as well as being able to be read back by the Prometheus Remote Read endpoint.\n\n\nRemote Write\n\n\nWrite a Prometheus Remote write query to M3.\n\n\nURL\n\n\n/api/v1/prom/remote/write\n\n\nMethod\n\n\nPOST\n\n\nURL Params\n\n\nNone.\n\n\nHeader Params\n\n\nOptional\n\n\n\n\nM3-Metrics-Type\n:\n\n If this header is set, it determines what type of metric to store this metric value as. Otherwise by default, metrics will be stored in all namespaces that are configured. You can also disable this default behavior by setting \ndownsample\n options to \nall: false\n for a namespace in the coordinator config, for more see \ndisabling automatic aggregation\n.\n\n Must be one of:\n\n\nunaggregated\n: Write metrics directly to configured unaggregated namespace.\n\n\naggregated\n: Write metrics directly to a configured aggregated namespace (bypassing any aggregation), this requires the \nM3-Storage-Policy\n header to be set to resolve which namespace to write metrics to.\n\n\n\n\nM3-Storage-Policy\n:\n\n If this header is set, it determines which aggregated namespace to read/write metrics directly to/from (bypassing any aggregation).\n\n The value of the header must be in the format of \nresolution:retention\n in duration shorthand. e.g. \n1m:48h\n specifices 1 minute resolution and 48 hour retention. Valid time units are \"ns\", \"us\" (or \"\u00b5s\"), \"ms\", \"s\", \"m\", \"h\".\n\nHere is \nan example\n of querying metrics from a specific namespace.\n\n\nM3-Map-Tags-JSON\n:\n\n If this header is set it enables dynamically mutating tags in a Prometheus write request. See issue\n\n2254\n for further context.\n\nCurrently only \nwrite\n is supported. As an example, the following header would unconditionally cause\n\nglobaltag=somevalue\n to be added to all metrics in a write request:\n\nM3-Map-Tags-JSON: '{\"tagMappers\":[{\"write\":{\"tag\":\"globaltag\",\"value\":\"somevalue\"}}]}'\n\n\n\n\n\n\n\nData Params\n\n\nBinary \nsnappy compressed\n Prometheus \nWriteRequest protobuf message\n.\n\n\nAvailable Tuning Params\n\n\nRefer \nhere\n for an up to date list of remote tuning parameters. \n\n\nSample Call\n\n\nThere isn't a straightforward way to Snappy compress and marshal a Prometheus WriteRequest protobuf message using just shell, so this example uses a specific command line utility instead. \n\n\nThis sample call is made using \npromremotecli\n which is a command line tool that uses a \nGo client\n to Prometheus Remote endpoints. For more information visit the \nGitHub repository\n.\n\n\nThere is also a \nJava client\n that can be used to make requests to the endpoint.\n\n\nEach \n-t\n parameter specifies a label (dimension) to add to the metric.\n\n\nThe \n-h\n parameter can be used as many times as necessary to add headers to the outgoing request in the form of \"Header-Name: HeaderValue\".\n\n\nHere is an example of writing the datapoint at the current unix timestamp with value 123.456:\n\n\n\n\n\ndocker run -it --rm                                            \n\\\n\n  quay.io/m3db/prometheus_remote_client_golang:latest          \n\\\n\n  -u http://host.docker.internal:7201/api/v1/prom/remote/write \n\\\n\n  -t __name__:http_requests_total                              \n\\\n\n  -t code:200                                                  \n\\\n\n  -t handler:graph                                             \n\\\n\n  -t method:get                                                \n\\\n\n  -d \n$(\ndate +\n\"%s\"\n)\n,123.456\npromremotecli_log \n2019\n/06/25 \n04\n:13:56 writing datapoint \n[\n2019\n-06-25 \n04\n:13:55 +0000 UTC \n123\n.456\n]\n\npromremotecli_log \n2019\n/06/25 \n04\n:13:56 labelled \n[[\n__name__ http_requests_total\n]\n \n[\ncode \n200\n]\n \n[\nhandler graph\n]\n \n[\nmethod get\n]]\n\npromremotecli_log \n2019\n/06/25 \n04\n:13:56 writing to http://host.docker.internal:7201/api/v1/prom/remote/write\n\n{\n\"success\"\n:true,\n\"statusCode\"\n:200\n}\n\npromremotecli_log \n2019\n/06/25 \n04\n:13:56 write success\n\n\n# If you are paranoid about image tags being hijacked/replaced with nefarious code, you can use this SHA256 tag:\n\n\n# quay.io/m3db/prometheus_remote_client_golang@sha256:fc56df819bff9a5a087484804acf3a584dd4a78c68900c31a28896ed66ca7e7b\n\n\n\n\n\nFor more details on querying data in PromQL that was written using this endpoint, see the \nquery API documentation\n.\n\n\nRemote Read\n\n\nRead Prometheus metrics from M3.\n\n\nURL\n\n\n/api/v1/prom/remote/read\n\n\nMethod\n\n\nPOST\n\n\nURL Params\n\n\nNone.\n\n\nHeader Params\n\n\nOptional\n\n\n\n\nM3-Metrics-Type\n:\n\n If this header is set, it determines what type of metric to store this metric value as. Otherwise by default, metrics will be stored in all namespaces that are configured. You can also disable this default behavior by setting \ndownsample\n options to \nall: false\n for a namespace in the coordinator config, for more see \ndisabling automatic aggregation\n.\n\n Must be one of:\n\n\nunaggregated\n: Write metrics directly to configured unaggregated namespace.\n\n\naggregated\n: Write metrics directly to a configured aggregated namespace (bypassing any aggregation), this requires the \nM3-Storage-Policy\n header to be set to resolve which namespace to write metrics to.\n\n\n\n\nM3-Storage-Policy\n:\n\n If this header is set, it determines which aggregated namespace to read/write metrics directly to/from (bypassing any aggregation).\n\n The value of the header must be in the format of \nresolution:retention\n in duration shorthand. e.g. \n1m:48h\n specifices 1 minute resolution and 48 hour retention. Valid time units are \"ns\", \"us\" (or \"\u00b5s\"), \"ms\", \"s\", \"m\", \"h\".\n\nHere is \nan example\n of querying metrics from a specific namespace.\n\n\nM3-Limit-Max-Series\n:\n\n If this header is set it will override any configured per query time series limit. If the limit is hit, it will either return a partial result or an error based on the require exhaustive configuration set.\n\n\nM3-Limit-Max-Docs\n:\n\n If this header is set it will override any configured per query time series * blocks limit (docs limit). If the limit is hit, it will either return a partial result or an error based on the require exhaustive configuration set.\n\n\nM3-Limit-Require-Exhaustive\n:\n\n If this header is set it will override any configured require exhaustive setting. If \"true\" it will return an error if query hits a configured limit (such as series or docs limit) instead of a partial result. Otherwise if \"false\" it will return a partial result of the time series already matched with the response header \nM3-Results-Limited\n detailing the limit that was hit and a warning included in the response body.\n\n\nM3-Restrict-By-Tags-JSON\n:\n\n If this header is set it can ensure specific label matching is performed as part\nof every query including series metadata endpoints. As an example, the following \nheader would unconditionally cause \nglobaltag=somevalue\n to be a part of all queries\nissued regardless of if they include the label or not in a query and also strip the\n\"globaltag\" from appearing as a label in any of the resulting timeseries:\n\nM3-Restrict-By-Tags-JSON: '{\"match\":[{\"name\":\"globaltag\",\"type\":\"EQUAL\",\"value\":\"somevalue\"}],\"strip\":[\"globaltag\"]}'\n\n\n\n\n\n\n\nData Params\n\n\nBinary \nsnappy compressed\n Prometheus \nWriteRequest protobuf message\n.",
            "title": "Prometheus Remote Write/Read"
        },
        {
            "location": "/m3coordinator/api/remote/#api",
            "text": "The M3 Coordinator implements the Prometheus Remote Read and Write HTTP endpoints, they also can be used however as general purpose metrics write and read APIs. Any metrics that are written to the remote write API can be queried using PromQL through the query APIs as well as being able to be read back by the Prometheus Remote Read endpoint.",
            "title": "API"
        },
        {
            "location": "/m3coordinator/api/remote/#remote-write",
            "text": "Write a Prometheus Remote write query to M3.",
            "title": "Remote Write"
        },
        {
            "location": "/m3coordinator/api/remote/#url",
            "text": "/api/v1/prom/remote/write",
            "title": "URL"
        },
        {
            "location": "/m3coordinator/api/remote/#method",
            "text": "POST",
            "title": "Method"
        },
        {
            "location": "/m3coordinator/api/remote/#url-params",
            "text": "None.",
            "title": "URL Params"
        },
        {
            "location": "/m3coordinator/api/remote/#header-params",
            "text": "",
            "title": "Header Params"
        },
        {
            "location": "/m3coordinator/api/remote/#optional",
            "text": "M3-Metrics-Type : \n If this header is set, it determines what type of metric to store this metric value as. Otherwise by default, metrics will be stored in all namespaces that are configured. You can also disable this default behavior by setting  downsample  options to  all: false  for a namespace in the coordinator config, for more see  disabling automatic aggregation . \n Must be one of:  unaggregated : Write metrics directly to configured unaggregated namespace.  aggregated : Write metrics directly to a configured aggregated namespace (bypassing any aggregation), this requires the  M3-Storage-Policy  header to be set to resolve which namespace to write metrics to.   M3-Storage-Policy : \n If this header is set, it determines which aggregated namespace to read/write metrics directly to/from (bypassing any aggregation). \n The value of the header must be in the format of  resolution:retention  in duration shorthand. e.g.  1m:48h  specifices 1 minute resolution and 48 hour retention. Valid time units are \"ns\", \"us\" (or \"\u00b5s\"), \"ms\", \"s\", \"m\", \"h\". \nHere is  an example  of querying metrics from a specific namespace.  M3-Map-Tags-JSON : \n If this header is set it enables dynamically mutating tags in a Prometheus write request. See issue 2254  for further context. \nCurrently only  write  is supported. As an example, the following header would unconditionally cause globaltag=somevalue  to be added to all metrics in a write request: M3-Map-Tags-JSON: '{\"tagMappers\":[{\"write\":{\"tag\":\"globaltag\",\"value\":\"somevalue\"}}]}'",
            "title": "Optional"
        },
        {
            "location": "/m3coordinator/api/remote/#data-params",
            "text": "Binary  snappy compressed  Prometheus  WriteRequest protobuf message .",
            "title": "Data Params"
        },
        {
            "location": "/m3coordinator/api/remote/#available-tuning-params",
            "text": "Refer  here  for an up to date list of remote tuning parameters.",
            "title": "Available Tuning Params"
        },
        {
            "location": "/m3coordinator/api/remote/#sample-call",
            "text": "There isn't a straightforward way to Snappy compress and marshal a Prometheus WriteRequest protobuf message using just shell, so this example uses a specific command line utility instead.   This sample call is made using  promremotecli  which is a command line tool that uses a  Go client  to Prometheus Remote endpoints. For more information visit the  GitHub repository .  There is also a  Java client  that can be used to make requests to the endpoint.  Each  -t  parameter specifies a label (dimension) to add to the metric.  The  -h  parameter can be used as many times as necessary to add headers to the outgoing request in the form of \"Header-Name: HeaderValue\".  Here is an example of writing the datapoint at the current unix timestamp with value 123.456:   docker run -it --rm                                             \\ \n  quay.io/m3db/prometheus_remote_client_golang:latest           \\ \n  -u http://host.docker.internal:7201/api/v1/prom/remote/write  \\ \n  -t __name__:http_requests_total                               \\ \n  -t code:200                                                   \\ \n  -t handler:graph                                              \\ \n  -t method:get                                                 \\ \n  -d  $( date + \"%s\" ) ,123.456\npromremotecli_log  2019 /06/25  04 :13:56 writing datapoint  [ 2019 -06-25  04 :13:55 +0000 UTC  123 .456 ] \npromremotecli_log  2019 /06/25  04 :13:56 labelled  [[ __name__ http_requests_total ]   [ code  200 ]   [ handler graph ]   [ method get ]] \npromremotecli_log  2019 /06/25  04 :13:56 writing to http://host.docker.internal:7201/api/v1/prom/remote/write { \"success\" :true, \"statusCode\" :200 } \npromremotecli_log  2019 /06/25  04 :13:56 write success # If you are paranoid about image tags being hijacked/replaced with nefarious code, you can use this SHA256 tag:  # quay.io/m3db/prometheus_remote_client_golang@sha256:fc56df819bff9a5a087484804acf3a584dd4a78c68900c31a28896ed66ca7e7b   For more details on querying data in PromQL that was written using this endpoint, see the  query API documentation .",
            "title": "Sample Call"
        },
        {
            "location": "/m3coordinator/api/remote/#remote-read",
            "text": "Read Prometheus metrics from M3.",
            "title": "Remote Read"
        },
        {
            "location": "/m3coordinator/api/remote/#url_1",
            "text": "/api/v1/prom/remote/read",
            "title": "URL"
        },
        {
            "location": "/m3coordinator/api/remote/#method_1",
            "text": "POST",
            "title": "Method"
        },
        {
            "location": "/m3coordinator/api/remote/#url-params_1",
            "text": "None.",
            "title": "URL Params"
        },
        {
            "location": "/m3coordinator/api/remote/#header-params_1",
            "text": "",
            "title": "Header Params"
        },
        {
            "location": "/m3coordinator/api/remote/#optional_1",
            "text": "M3-Metrics-Type : \n If this header is set, it determines what type of metric to store this metric value as. Otherwise by default, metrics will be stored in all namespaces that are configured. You can also disable this default behavior by setting  downsample  options to  all: false  for a namespace in the coordinator config, for more see  disabling automatic aggregation . \n Must be one of:  unaggregated : Write metrics directly to configured unaggregated namespace.  aggregated : Write metrics directly to a configured aggregated namespace (bypassing any aggregation), this requires the  M3-Storage-Policy  header to be set to resolve which namespace to write metrics to.   M3-Storage-Policy : \n If this header is set, it determines which aggregated namespace to read/write metrics directly to/from (bypassing any aggregation). \n The value of the header must be in the format of  resolution:retention  in duration shorthand. e.g.  1m:48h  specifices 1 minute resolution and 48 hour retention. Valid time units are \"ns\", \"us\" (or \"\u00b5s\"), \"ms\", \"s\", \"m\", \"h\". \nHere is  an example  of querying metrics from a specific namespace.  M3-Limit-Max-Series : \n If this header is set it will override any configured per query time series limit. If the limit is hit, it will either return a partial result or an error based on the require exhaustive configuration set.  M3-Limit-Max-Docs : \n If this header is set it will override any configured per query time series * blocks limit (docs limit). If the limit is hit, it will either return a partial result or an error based on the require exhaustive configuration set.  M3-Limit-Require-Exhaustive : \n If this header is set it will override any configured require exhaustive setting. If \"true\" it will return an error if query hits a configured limit (such as series or docs limit) instead of a partial result. Otherwise if \"false\" it will return a partial result of the time series already matched with the response header  M3-Results-Limited  detailing the limit that was hit and a warning included in the response body.  M3-Restrict-By-Tags-JSON : \n If this header is set it can ensure specific label matching is performed as part\nof every query including series metadata endpoints. As an example, the following \nheader would unconditionally cause  globaltag=somevalue  to be a part of all queries\nissued regardless of if they include the label or not in a query and also strip the\n\"globaltag\" from appearing as a label in any of the resulting timeseries: M3-Restrict-By-Tags-JSON: '{\"match\":[{\"name\":\"globaltag\",\"type\":\"EQUAL\",\"value\":\"somevalue\"}],\"strip\":[\"globaltag\"]}'",
            "title": "Optional"
        },
        {
            "location": "/m3coordinator/api/remote/#data-params_1",
            "text": "Binary  snappy compressed  Prometheus  WriteRequest protobuf message .",
            "title": "Data Params"
        },
        {
            "location": "/m3query/",
            "text": "M3 Query, a statelees query server for M3DB and Prometheus\n\n\nM3 Query is a service that exposes all metrics query endpoints along with \nmetrics time series metadata APIs that return dimensions and labels of metrics\nthat reside in a M3DB cluster.\n\n\nNote\n: M3 Coordinator, and by proxy M3DB, by default includes the M3 \nQuery endpoints accessible on port 7201.\nFor production deployments it is recommended to deploy it as a \ndedicated service to ensure you can scale the memory heavy query role separately \nfrom the metrics ingestion write path of writes through M3 Coordinator to M3DB\ndatabase role nodes. This allows excessive queries to primarily affect the \ndedicated M3 Query service instead of interrupting service to the write \ningestion pipeline.",
            "title": "Introduction"
        },
        {
            "location": "/m3query/#m3-query-a-statelees-query-server-for-m3db-and-prometheus",
            "text": "M3 Query is a service that exposes all metrics query endpoints along with \nmetrics time series metadata APIs that return dimensions and labels of metrics\nthat reside in a M3DB cluster.  Note : M3 Coordinator, and by proxy M3DB, by default includes the M3 \nQuery endpoints accessible on port 7201.\nFor production deployments it is recommended to deploy it as a \ndedicated service to ensure you can scale the memory heavy query role separately \nfrom the metrics ingestion write path of writes through M3 Coordinator to M3DB\ndatabase role nodes. This allows excessive queries to primarily affect the \ndedicated M3 Query service instead of interrupting service to the write \ningestion pipeline.",
            "title": "M3 Query, a statelees query server for M3DB and Prometheus"
        },
        {
            "location": "/m3query/config/",
            "text": "Configuration\n\n\nDefault query engine\n\n\nBy default M3 runs two query engines:\n\n\n\n\nPrometheus (default) - robust and de-facto query language for metrics\n\n\nM3 Query Engine - high-performance query engine but doesn't support all the functions yet\n\n\n\n\nPrometheus Query Engine is the default one when calling query endpoint:\n\nhttp://localhost:7201/api/v1/query?query=count(http_requests)&time=1590147165\n\n\n\nBut you can switch between the two in the following ways:\n\n\n\n\nChanging default query engine in config file (see \ndefaultEngine\n parameter in \nConfiguration\n)\n\n\n\n\nPassing HTTP header \nM3-Engine\n:\n\n\ncurl -H \"M3-Engine: m3query\" \"http://localhost:7201/api/v1/query?query=count(http_requests)&time=1590147165\"\n\n\nor\n\n\ncurl -H \"M3-Engine: prometheus\" \"http://localhost:7201/api/v1/query?query=count(http_requests)&time=1590147165\"\n\n\n\n\n\n\nPassing HTTP query URL parameter \nengine\n:\n\n\ncurl \"http://localhost:7201/api/v1/query?engine=m3query&query=count(http_requests)&time=1590147165\"\n\n\nor\n\n\ncurl \"http://localhost:7201/api/v1/query?engine=prometheus&query=count(http_requests)&time=1590147165\"\n\n\n\n\n\n\nUsing different URLs:\n\n\n\n\n/prometheus/api/v1/*\n - to call Prometheus query engine\n\n\n/m3query/api/v1/*\n - to call M3 Query query engine\n\n\n\n\ncurl \"http://localhost:7201/m3query/api/v1/query?query=count(http_requests)&time=1590147165\"\n\n\nor\n\n\ncurl \"http://localhost:7201/prometheus/api/v1/query?query=count(http_requests)&time=1590147165\"",
            "title": "Query Engine"
        },
        {
            "location": "/m3query/config/#configuration",
            "text": "",
            "title": "Configuration"
        },
        {
            "location": "/m3query/config/#default-query-engine",
            "text": "By default M3 runs two query engines:   Prometheus (default) - robust and de-facto query language for metrics  M3 Query Engine - high-performance query engine but doesn't support all the functions yet   Prometheus Query Engine is the default one when calling query endpoint: http://localhost:7201/api/v1/query?query=count(http_requests)&time=1590147165  But you can switch between the two in the following ways:   Changing default query engine in config file (see  defaultEngine  parameter in  Configuration )   Passing HTTP header  M3-Engine :  curl -H \"M3-Engine: m3query\" \"http://localhost:7201/api/v1/query?query=count(http_requests)&time=1590147165\"  or  curl -H \"M3-Engine: prometheus\" \"http://localhost:7201/api/v1/query?query=count(http_requests)&time=1590147165\"    Passing HTTP query URL parameter  engine :  curl \"http://localhost:7201/api/v1/query?engine=m3query&query=count(http_requests)&time=1590147165\"  or  curl \"http://localhost:7201/api/v1/query?engine=prometheus&query=count(http_requests)&time=1590147165\"    Using different URLs:   /prometheus/api/v1/*  - to call Prometheus query engine  /m3query/api/v1/*  - to call M3 Query query engine   curl \"http://localhost:7201/m3query/api/v1/query?query=count(http_requests)&time=1590147165\"  or  curl \"http://localhost:7201/prometheus/api/v1/query?query=count(http_requests)&time=1590147165\"",
            "title": "Default query engine"
        },
        {
            "location": "/m3query/config/annotated_config/",
            "text": "Annotated Config\n\n\nPlease \nsee here\n for a link to the annotated config.",
            "title": "Annotated Config File"
        },
        {
            "location": "/m3query/config/annotated_config/#annotated-config",
            "text": "Please  see here  for a link to the annotated config.",
            "title": "Annotated Config"
        },
        {
            "location": "/m3query/api/",
            "text": "API\n\n\nPlease note:\n This documentation is a work in progress and more detail is required.\n\n\nQuery using PromQL\n\n\nQuery using PromQL and returns JSON datapoints compatible with the Prometheus Grafana plugin.\n\n\nURL\n\n\n/api/v1/query_range\n\n\nMethod\n\n\nGET\n\n\nURL Params\n\n\nRequired\n\n\n\n\nstart=[time in RFC3339Nano]\n\n\nend=[time in RFC3339Nano]\n\n\nstep=[time duration]\n\n\ntarget=[string]\n\n\n\n\nOptional\n\n\n\n\ndebug=[bool]\n\n\nlookback=[string|time duration]\n: This sets the per request lookback duration to something other than the default set in config, can either be a time duration or the string \"step\" which sets the lookback to the same as the \nstep\n request parameter.\n\n\n\n\nHeader Params\n\n\nOptional\n\n\n\n\nM3-Metrics-Type\n:\n\n If this header is set, it determines what type of metric to store this metric value as. Otherwise by default, metrics will be stored in all namespaces that are configured. You can also disable this default behavior by setting \ndownsample\n options to \nall: false\n for a namespace in the coordinator config, for more see \ndisabling automatic aggregation\n.\n\n Must be one of:\n\n\nunaggregated\n: Write metrics directly to configured unaggregated namespace.\n\n\naggregated\n: Write metrics directly to a configured aggregated namespace (bypassing any aggregation), this requires the \nM3-Storage-Policy\n header to be set to resolve which namespace to write metrics to.\n\n\n\n\nM3-Storage-Policy\n:\n\n If this header is set, it determines which aggregated namespace to read/write metrics directly to/from (bypassing any aggregation).\n\n The value of the header must be in the format of \nresolution:retention\n in duration shorthand. e.g. \n1m:48h\n specifices 1 minute resolution and 48 hour retention. Valid time units are \"ns\", \"us\" (or \"\u00b5s\"), \"ms\", \"s\", \"m\", \"h\".\n\nHere is \nan example\n of querying metrics from a specific namespace.\n\n\nM3-Limit-Max-Series\n:\n\n If this header is set it will override any configured per query time series limit. If the limit is hit, it will either return a partial result or an error based on the require exhaustive configuration set.\n\n\nM3-Limit-Max-Docs\n:\n\n If this header is set it will override any configured per query time series * blocks limit (docs limit). If the limit is hit, it will either return a partial result or an error based on the require exhaustive configuration set.\n\n\nM3-Limit-Require-Exhaustive\n:\n\n If this header is set it will override any configured require exhaustive setting. If \"true\" it will return an error if query hits a configured limit (such as series or docs limit) instead of a partial result. Otherwise if \"false\" it will return a partial result of the time series already matched with the response header \nM3-Results-Limited\n detailing the limit that was hit and a warning included in the response body.\n\n\nM3-Restrict-By-Tags-JSON\n:\n\n If this header is set it can ensure specific label matching is performed as part\nof every query including series metadata endpoints. As an example, the following \nheader would unconditionally cause \nglobaltag=somevalue\n to be a part of all queries\nissued regardless of if they include the label or not in a query and also strip the\n\"globaltag\" from appearing as a label in any of the resulting timeseries:\n\nM3-Restrict-By-Tags-JSON: '{\"match\":[{\"name\":\"globaltag\",\"type\":\"EQUAL\",\"value\":\"somevalue\"}],\"strip\":[\"globaltag\"]}'\n\n\n\n\n\n\n\nData Params\n\n\nNone.\n\n\nSample Call\n\n\n\n\n\ncurl \n'http://localhost:7201/api/v1/query_range?query=abs(http_requests_total)&start=1530220860&end=1530220900&step=15s'\n\n\n{\n\n  \n\"status\"\n: \n\"success\"\n,\n  \n\"data\"\n: \n{\n\n    \n\"resultType\"\n: \n\"matrix\"\n,\n    \n\"result\"\n: \n[\n\n      \n{\n\n        \n\"metric\"\n: \n{\n\n          \n\"code\"\n: \n\"200\"\n,\n          \n\"handler\"\n: \n\"graph\"\n,\n          \n\"method\"\n: \n\"get\"\n\n        \n}\n,\n        \n\"values\"\n: \n[\n\n          \n[\n\n            \n1530220860\n,\n            \n\"6\"\n\n          \n]\n,\n          \n[\n\n            \n1530220875\n,\n            \n\"6\"\n\n          \n]\n,\n          \n[\n\n            \n1530220890\n,\n            \n\"6\"\n\n          \n]\n\n        \n]\n\n      \n}\n,\n      \n{\n\n        \n\"metric\"\n: \n{\n\n          \n\"code\"\n: \n\"200\"\n,\n          \n\"handler\"\n: \n\"label_values\"\n,\n          \n\"method\"\n: \n\"get\"\n\n        \n}\n,\n        \n\"values\"\n: \n[\n\n          \n[\n\n            \n1530220860\n,\n            \n\"6\"\n\n          \n]\n,\n          \n[\n\n            \n1530220875\n,\n            \n\"6\"\n\n          \n]\n,\n          \n[\n\n            \n1530220890\n,\n            \n\"6\"\n\n          \n]\n\n        \n]\n\n      \n}\n\n    \n]\n\n  \n}\n\n\n}",
            "title": "Query"
        },
        {
            "location": "/m3query/api/#api",
            "text": "Please note:  This documentation is a work in progress and more detail is required.",
            "title": "API"
        },
        {
            "location": "/m3query/api/#query-using-promql",
            "text": "Query using PromQL and returns JSON datapoints compatible with the Prometheus Grafana plugin.",
            "title": "Query using PromQL"
        },
        {
            "location": "/m3query/api/#url",
            "text": "/api/v1/query_range",
            "title": "URL"
        },
        {
            "location": "/m3query/api/#method",
            "text": "GET",
            "title": "Method"
        },
        {
            "location": "/m3query/api/#url-params",
            "text": "",
            "title": "URL Params"
        },
        {
            "location": "/m3query/api/#required",
            "text": "start=[time in RFC3339Nano]  end=[time in RFC3339Nano]  step=[time duration]  target=[string]",
            "title": "Required"
        },
        {
            "location": "/m3query/api/#optional",
            "text": "debug=[bool]  lookback=[string|time duration] : This sets the per request lookback duration to something other than the default set in config, can either be a time duration or the string \"step\" which sets the lookback to the same as the  step  request parameter.",
            "title": "Optional"
        },
        {
            "location": "/m3query/api/#header-params",
            "text": "",
            "title": "Header Params"
        },
        {
            "location": "/m3query/api/#optional_1",
            "text": "M3-Metrics-Type : \n If this header is set, it determines what type of metric to store this metric value as. Otherwise by default, metrics will be stored in all namespaces that are configured. You can also disable this default behavior by setting  downsample  options to  all: false  for a namespace in the coordinator config, for more see  disabling automatic aggregation . \n Must be one of:  unaggregated : Write metrics directly to configured unaggregated namespace.  aggregated : Write metrics directly to a configured aggregated namespace (bypassing any aggregation), this requires the  M3-Storage-Policy  header to be set to resolve which namespace to write metrics to.   M3-Storage-Policy : \n If this header is set, it determines which aggregated namespace to read/write metrics directly to/from (bypassing any aggregation). \n The value of the header must be in the format of  resolution:retention  in duration shorthand. e.g.  1m:48h  specifices 1 minute resolution and 48 hour retention. Valid time units are \"ns\", \"us\" (or \"\u00b5s\"), \"ms\", \"s\", \"m\", \"h\". \nHere is  an example  of querying metrics from a specific namespace.  M3-Limit-Max-Series : \n If this header is set it will override any configured per query time series limit. If the limit is hit, it will either return a partial result or an error based on the require exhaustive configuration set.  M3-Limit-Max-Docs : \n If this header is set it will override any configured per query time series * blocks limit (docs limit). If the limit is hit, it will either return a partial result or an error based on the require exhaustive configuration set.  M3-Limit-Require-Exhaustive : \n If this header is set it will override any configured require exhaustive setting. If \"true\" it will return an error if query hits a configured limit (such as series or docs limit) instead of a partial result. Otherwise if \"false\" it will return a partial result of the time series already matched with the response header  M3-Results-Limited  detailing the limit that was hit and a warning included in the response body.  M3-Restrict-By-Tags-JSON : \n If this header is set it can ensure specific label matching is performed as part\nof every query including series metadata endpoints. As an example, the following \nheader would unconditionally cause  globaltag=somevalue  to be a part of all queries\nissued regardless of if they include the label or not in a query and also strip the\n\"globaltag\" from appearing as a label in any of the resulting timeseries: M3-Restrict-By-Tags-JSON: '{\"match\":[{\"name\":\"globaltag\",\"type\":\"EQUAL\",\"value\":\"somevalue\"}],\"strip\":[\"globaltag\"]}'",
            "title": "Optional"
        },
        {
            "location": "/m3query/api/#data-params",
            "text": "None.",
            "title": "Data Params"
        },
        {
            "location": "/m3query/api/#sample-call",
            "text": "curl  'http://localhost:7201/api/v1/query_range?query=abs(http_requests_total)&start=1530220860&end=1530220900&step=15s'  { \n   \"status\" :  \"success\" ,\n   \"data\" :  { \n     \"resultType\" :  \"matrix\" ,\n     \"result\" :  [ \n       { \n         \"metric\" :  { \n           \"code\" :  \"200\" ,\n           \"handler\" :  \"graph\" ,\n           \"method\" :  \"get\" \n         } ,\n         \"values\" :  [ \n           [ \n             1530220860 ,\n             \"6\" \n           ] ,\n           [ \n             1530220875 ,\n             \"6\" \n           ] ,\n           [ \n             1530220890 ,\n             \"6\" \n           ] \n         ] \n       } ,\n       { \n         \"metric\" :  { \n           \"code\" :  \"200\" ,\n           \"handler\" :  \"label_values\" ,\n           \"method\" :  \"get\" \n         } ,\n         \"values\" :  [ \n           [ \n             1530220860 ,\n             \"6\" \n           ] ,\n           [ \n             1530220875 ,\n             \"6\" \n           ] ,\n           [ \n             1530220890 ,\n             \"6\" \n           ] \n         ] \n       } \n     ] \n   }  }",
            "title": "Sample Call"
        },
        {
            "location": "/m3query/architecture/",
            "text": "Architecture\n\n\nPlease note:\n This documentation is a work in progress and more detail is required.\n\n\nOverview\n\n\nM3 Query and M3 Coordinator are written entirely in Go, M3 Query is as a query engine for \nM3DB\n and M3 Coordinator is a remote read/write endpoint for Prometheus and M3DB. To learn more about Prometheus's remote endpoints and storage, \nsee here\n.",
            "title": "Overview"
        },
        {
            "location": "/m3query/architecture/#architecture",
            "text": "Please note:  This documentation is a work in progress and more detail is required.",
            "title": "Architecture"
        },
        {
            "location": "/m3query/architecture/#overview",
            "text": "M3 Query and M3 Coordinator are written entirely in Go, M3 Query is as a query engine for  M3DB  and M3 Coordinator is a remote read/write endpoint for Prometheus and M3DB. To learn more about Prometheus's remote endpoints and storage,  see here .",
            "title": "Overview"
        },
        {
            "location": "/m3query/architecture/blocks/",
            "text": "Blocks\n\n\nPlease note:\n This documentation is a work in progress and more detail is required.\n\n\nOverview\n\n\nThe fundamental data structures that M3 Query uses are \nBlocks\n. \nBlocks\n are what get created from the series iterators that M3DB returns. A \nBlock\n is associated with a start and end time. It contains data from multiple time series stored in columnar format.\n\n\nMost transformations within M3 Query will be applied across different series for each time interval. Therefore, having data stored in columnar format helps with the memory locality of the data. Moreover, most transformations within M3 Query can work in parallel on different blocks which can significantly increase the computation speed.\n\n\nDiagram\n\n\nBelow is a visual representation of a set of \nBlocks\n. On top is the M3QL query that gets executed, and on the bottom, are the results of the query containing 3 different Blocks.\n\n\n                              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                              \u2502                                                                       \u2502\n                              \u2502     fetch name:sign_up city_id:{new_york,san_diego,toronto} os:*     \u2502\n                              \u2502                                                                       \u2502\n                              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                         \u2502                        \u2502                         \u2502\n                                         \u2502                        \u2502                         \u2502\n                                         \u2502                        \u2502                         \u2502\n                                         \u25bc                        \u25bc                         \u25bc\n                                  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                  \u2502  Block One \u2502            \u2502  Block Two \u2502           \u2502 Block Three \u2502\n                                  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                              \u2502   t  \u2502 t+1  \u2502 t+2  \u2502    \u2502  t+3 \u2502 t+4  \u2502 t+5  \u2502   \u2502  t+6 \u2502 t+7  \u2502 t+8  \u2502\n                              \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u25b6    \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u25b6   \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502      \u2502      \u2502      \u2502    \u2502      \u2502      \u2502      \u2502   \u2502      \u2502      \u2502      \u2502\n\u2502       name:sign_up        \u2502 \u2502      \u2502      \u2502      \u2502    \u2502      \u2502      \u2502      \u2502   \u2502      \u2502      \u2502      \u2502\n\u2502  city_id:new_york os:ios  \u2502 \u2502  5   \u2502  2   \u2502  10  \u2502    \u2502  10  \u2502  2   \u2502  10  \u2502   \u2502  5   \u2502  3   \u2502  5   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502      \u2502      \u2502      \u2502    \u2502      \u2502      \u2502      \u2502   \u2502      \u2502      \u2502      \u2502\n                              \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u25b6    \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u25b6   \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502      \u2502      \u2502      \u2502    \u2502      \u2502      \u2502      \u2502   \u2502      \u2502      \u2502      \u2502\n\u2502       name:sign_up        \u2502 \u2502      \u2502      \u2502      \u2502    \u2502      \u2502      \u2502      \u2502   \u2502      \u2502      \u2502      \u2502\n\u2502city_id:new_york os:android\u2502 \u2502  10  \u2502  8   \u2502  5   \u2502    \u2502  20  \u2502  4   \u2502  5   \u2502   \u2502  10  \u2502  8   \u2502  5   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502      \u2502      \u2502      \u2502    \u2502      \u2502      \u2502      \u2502   \u2502      \u2502      \u2502      \u2502\n                              \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u25b6    \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u25b6   \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502      \u2502      \u2502      \u2502    \u2502      \u2502      \u2502      \u2502   \u2502      \u2502      \u2502      \u2502\n\u2502       name:sign_up        \u2502 \u2502      \u2502      \u2502      \u2502    \u2502      \u2502      \u2502      \u2502   \u2502      \u2502      \u2502      \u2502\n\u2502 city_id:san_diego os:ios  \u2502 \u2502  10  \u2502  5   \u2502  10  \u2502    \u2502  2   \u2502  5   \u2502  10  \u2502   \u2502  8   \u2502  6   \u2502  6   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502      \u2502      \u2502      \u2502    \u2502      \u2502      \u2502      \u2502   \u2502      \u2502      \u2502      \u2502\n                              \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u25b6    \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u25b6   \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502      \u2502      \u2502      \u2502    \u2502      \u2502      \u2502      \u2502   \u2502      \u2502      \u2502      \u2502\n\u2502       name:sign_up        \u2502 \u2502      \u2502      \u2502      \u2502    \u2502      \u2502      \u2502      \u2502   \u2502      \u2502      \u2502      \u2502\n\u2502  city_id:toronto os:ios   \u2502 \u2502  2   \u2502  5   \u2502  10  \u2502    \u2502  2   \u2502  5   \u2502  10  \u2502   \u2502  2   \u2502  5   \u2502  10  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502      \u2502      \u2502      \u2502    \u2502      \u2502      \u2502      \u2502   \u2502      \u2502      \u2502      \u2502\n                              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\n\n\nM3DB => M3 Query Blocks\n\n\nIn order to convert M3DB blocks into M3 Query blocks, we need to consolidate across different namespaces. In short, M3DB namespaces are essentially different resolutions that metrics are stored at. For example, a metric might be stored at both 1min and 10min resolutions- meaning this metric is found in two namespaces.\n\n\nAt a high level, M3DB returns to M3 Query \nSeriesBlocks\n that contain a list of \nSeriesIterators\n for a given timeseries per namespace. M3 Query then aligns the blocks across common time bounds before applying consolidation.\n\n\nFor example, let's say we have a query that returns two timeseries from two different namespaces- 1min and 10min. When we create the M3 Query \nBlock\n, in order to accurately consolidate results from these two namespaces, we need to convert everything to have a 10min resolution. Otherwise it will not be possible to perform correctly apply functions.\n\n\n\n\nComing Soon: More documentation on how M3 Query applies consolidation.",
            "title": "Blocks"
        },
        {
            "location": "/m3query/architecture/blocks/#blocks",
            "text": "Please note:  This documentation is a work in progress and more detail is required.",
            "title": "Blocks"
        },
        {
            "location": "/m3query/architecture/blocks/#overview",
            "text": "The fundamental data structures that M3 Query uses are  Blocks .  Blocks  are what get created from the series iterators that M3DB returns. A  Block  is associated with a start and end time. It contains data from multiple time series stored in columnar format.  Most transformations within M3 Query will be applied across different series for each time interval. Therefore, having data stored in columnar format helps with the memory locality of the data. Moreover, most transformations within M3 Query can work in parallel on different blocks which can significantly increase the computation speed.",
            "title": "Overview"
        },
        {
            "location": "/m3query/architecture/blocks/#diagram",
            "text": "Below is a visual representation of a set of  Blocks . On top is the M3QL query that gets executed, and on the bottom, are the results of the query containing 3 different Blocks.                                \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                              \u2502                                                                       \u2502\n                              \u2502     fetch name:sign_up city_id:{new_york,san_diego,toronto} os:*     \u2502\n                              \u2502                                                                       \u2502\n                              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                         \u2502                        \u2502                         \u2502\n                                         \u2502                        \u2502                         \u2502\n                                         \u2502                        \u2502                         \u2502\n                                         \u25bc                        \u25bc                         \u25bc\n                                  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                  \u2502  Block One \u2502            \u2502  Block Two \u2502           \u2502 Block Three \u2502\n                                  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                              \u2502   t  \u2502 t+1  \u2502 t+2  \u2502    \u2502  t+3 \u2502 t+4  \u2502 t+5  \u2502   \u2502  t+6 \u2502 t+7  \u2502 t+8  \u2502\n                              \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u25b6    \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u25b6   \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502      \u2502      \u2502      \u2502    \u2502      \u2502      \u2502      \u2502   \u2502      \u2502      \u2502      \u2502\n\u2502       name:sign_up        \u2502 \u2502      \u2502      \u2502      \u2502    \u2502      \u2502      \u2502      \u2502   \u2502      \u2502      \u2502      \u2502\n\u2502  city_id:new_york os:ios  \u2502 \u2502  5   \u2502  2   \u2502  10  \u2502    \u2502  10  \u2502  2   \u2502  10  \u2502   \u2502  5   \u2502  3   \u2502  5   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502      \u2502      \u2502      \u2502    \u2502      \u2502      \u2502      \u2502   \u2502      \u2502      \u2502      \u2502\n                              \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u25b6    \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u25b6   \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502      \u2502      \u2502      \u2502    \u2502      \u2502      \u2502      \u2502   \u2502      \u2502      \u2502      \u2502\n\u2502       name:sign_up        \u2502 \u2502      \u2502      \u2502      \u2502    \u2502      \u2502      \u2502      \u2502   \u2502      \u2502      \u2502      \u2502\n\u2502city_id:new_york os:android\u2502 \u2502  10  \u2502  8   \u2502  5   \u2502    \u2502  20  \u2502  4   \u2502  5   \u2502   \u2502  10  \u2502  8   \u2502  5   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502      \u2502      \u2502      \u2502    \u2502      \u2502      \u2502      \u2502   \u2502      \u2502      \u2502      \u2502\n                              \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u25b6    \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u25b6   \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502      \u2502      \u2502      \u2502    \u2502      \u2502      \u2502      \u2502   \u2502      \u2502      \u2502      \u2502\n\u2502       name:sign_up        \u2502 \u2502      \u2502      \u2502      \u2502    \u2502      \u2502      \u2502      \u2502   \u2502      \u2502      \u2502      \u2502\n\u2502 city_id:san_diego os:ios  \u2502 \u2502  10  \u2502  5   \u2502  10  \u2502    \u2502  2   \u2502  5   \u2502  10  \u2502   \u2502  8   \u2502  6   \u2502  6   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502      \u2502      \u2502      \u2502    \u2502      \u2502      \u2502      \u2502   \u2502      \u2502      \u2502      \u2502\n                              \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u25b6    \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u25b6   \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502      \u2502      \u2502      \u2502    \u2502      \u2502      \u2502      \u2502   \u2502      \u2502      \u2502      \u2502\n\u2502       name:sign_up        \u2502 \u2502      \u2502      \u2502      \u2502    \u2502      \u2502      \u2502      \u2502   \u2502      \u2502      \u2502      \u2502\n\u2502  city_id:toronto os:ios   \u2502 \u2502  2   \u2502  5   \u2502  10  \u2502    \u2502  2   \u2502  5   \u2502  10  \u2502   \u2502  2   \u2502  5   \u2502  10  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502      \u2502      \u2502      \u2502    \u2502      \u2502      \u2502      \u2502   \u2502      \u2502      \u2502      \u2502\n                              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518",
            "title": "Diagram"
        },
        {
            "location": "/m3query/architecture/blocks/#m3db-m3-query-blocks",
            "text": "In order to convert M3DB blocks into M3 Query blocks, we need to consolidate across different namespaces. In short, M3DB namespaces are essentially different resolutions that metrics are stored at. For example, a metric might be stored at both 1min and 10min resolutions- meaning this metric is found in two namespaces.  At a high level, M3DB returns to M3 Query  SeriesBlocks  that contain a list of  SeriesIterators  for a given timeseries per namespace. M3 Query then aligns the blocks across common time bounds before applying consolidation.  For example, let's say we have a query that returns two timeseries from two different namespaces- 1min and 10min. When we create the M3 Query  Block , in order to accurately consolidate results from these two namespaces, we need to convert everything to have a 10min resolution. Otherwise it will not be possible to perform correctly apply functions.   Coming Soon: More documentation on how M3 Query applies consolidation.",
            "title": "M3DB =&gt; M3 Query Blocks"
        },
        {
            "location": "/m3query/architecture/fanout/",
            "text": "Fetching and querying\n\n\nFetch fanout\n\n\nSince m3query does not currently have a view into the M3DB index, fanout to multiple clusters is rather complicated. Since not every metric is necessarily in every cluster (as an example, carbon metrics routed to a certain resolution), it is not trivial to determine which namespaces should be queried to return a fully correct set of recorded metrics.\n\n\nThe general approach is therefore to attempt to fanout to any namespace which has a complete view of all metrics, for example, \nUnaggregated\n, and take that if it fulfills the query range; if not, m3query will attempt to stitch together namespaces with longer retentions to try and build the most complete possible view of stored metrics.\n\n\nFor further details, please ask questions on \nSlack\n, and we'll be happy to help!",
            "title": "Query Fanout"
        },
        {
            "location": "/m3query/architecture/fanout/#fetching-and-querying",
            "text": "",
            "title": "Fetching and querying"
        },
        {
            "location": "/m3query/architecture/fanout/#fetch-fanout",
            "text": "Since m3query does not currently have a view into the M3DB index, fanout to multiple clusters is rather complicated. Since not every metric is necessarily in every cluster (as an example, carbon metrics routed to a certain resolution), it is not trivial to determine which namespaces should be queried to return a fully correct set of recorded metrics.  The general approach is therefore to attempt to fanout to any namespace which has a complete view of all metrics, for example,  Unaggregated , and take that if it fulfills the query range; if not, m3query will attempt to stitch together namespaces with longer retentions to try and build the most complete possible view of stored metrics.  For further details, please ask questions on  Slack , and we'll be happy to help!",
            "title": "Fetch fanout"
        },
        {
            "location": "/m3query/architecture/functions/",
            "text": "Function Processing\n\n\nSupported Functions\n\n\n\n\n\n\n\n\nM3QL\n\n\nPrometheus\n\n\nGraphite\n\n\n\n\n\n\n\n\n\n\nabs/absolute\n\n\nabs()\n\n\nabsolute(seriesList)\n\n\n\n\n\n\nalias [alias]\n\n\n\n\nalias(seriesList, newName)\n\n\n\n\n\n\naliasByTags [tag]\n\n\n\n\naliasByTags(seriesList, *tags)\n\n\n\n\n\n\naliasByBucket/aliasByHistogramBucket [tag]\n\n\n\n\n\n\n\n\n\n\nanomalies [flags]\n\n\n\n\n\n\n\n\n\n\nasPercent\n\n\n/\n\n\nasPercent(seriesList, total=None, *nodes)\n\n\n\n\n\n\navg/averageSeries [tag]\n\n\navg()\n\n\naverageSeries(*seriesLists)\n\n\n\n\n\n\nchanged\n\n\n\n\nchanged(seriesList)\n\n\n\n\n\n\nconstantLine [value]\n\n\n\n\nconstantLine(value)\n\n\n\n\n\n\ncount\n\n\ncount()\n\n\ncountSeries(*seriesLists)\n\n\n\n\n\n\nderivative\n\n\n\n\nderivative(seriesList)\n\n\n\n\n\n\ndiff\n\n\n-\n\n\ndiffSeries(*seriesLists)\n\n\n\n\n\n\ndivideSeries\n\n\n/\n\n\ndivideSeries(dividendSeriesList, divisorSeries)\n\n\n\n\n\n\neq/== [value]\n\n\n==\n\n\nremoveBelowValue(seriesList, n)/removeAboveValue(seriesList, n)\n\n\n\n\n\n\nne/!= [value]\n\n\n!=\n\n\nremoveBelowValue(seriesList, n)/removeAboveValue(seriesList, n)\n\n\n\n\n\n\nexcludeByTag [tag, pattern]\n\n\n\n\nexclude(seriesList, pattern)\n\n\n\n\n\n\nexecute/exec [fetch]\n\n\n\n\n\n\n\n\n\n\nfallbackSeries [replacement]\n\n\n\n\nfallbackSeries(seriesList, fallback)\n\n\n\n\n\n\nfetch\n\n\n\n\n\n\n\n\n\n\nge/=> [value]\n\n\n>=\n\n\nremoveBelowValue(seriesList, n)\n\n\n\n\n\n\ngt/> [value]\n\n\n>\n\n\nremoveBelowValue(seriesList, n)\n\n\n\n\n\n\nhead [limit]\n\n\ntopk()\n\n\nhighest(seriesList, n=1, func='average')\n\n\n\n\n\n\nhistogramCDF [idTag, rangeTag, value]\n\n\n\n\n\n\n\n\n\n\nhistogramPercentile [idTag, rangeTag, percentileValue]\n\n\n\n\n\n\n\n\n\n\nidentity [name]\n\n\n\n\nidentity(name)\n\n\n\n\n\n\nintegral\n\n\n\n\nintegral(seriesList)\n\n\n\n\n\n\nintersect [tags]\n\n\nand/or\n\n\n\n\n\n\n\n\nisNonNull\n\n\n\n\nisNonNull(seriesList)\n\n\n\n\n\n\njainCP\n\n\n\n\n\n\n\n\n\n\nkeepLastValue\n\n\n\n\nkeepLastValue(seriesList, limit=inf)\n\n\n\n\n\n\nle/<= [value]\n\n\n<=\n\n\nremoveAboveValue(seriesList, n)\n\n\n\n\n\n\nlogarithm\n\n\nln()\n\n\nlogarithm(seriesList, base=10)\n\n\n\n\n\n\nlt/< [value]\n\n\n<\n\n\nremoveAboveValue(seriesList, n)\n\n\n\n\n\n\nmax/maxSeries [tag]\n\n\nmax()\n\n\nmaxSeries(*seriesLists)\n\n\n\n\n\n\nmin/minSeries [tag]\n\n\nmin()\n\n\nminSeries(*seriesLists)\n\n\n\n\n\n\nmoving [interval, func]\n\n\n_over_time()\n\n\nmovingMax, movingMin, movingMedian, movingAverage, etc.\n\n\n\n\n\n\nmultiply/multiplySeries [tag]\n\n\n*\n\n\nmultiplySeries(*seriesLists)\n\n\n\n\n\n\nnonNegativeDerivative [maxValue]\n\n\n\n\nnonNegativeDerivative(seriesList, maxValue=None)\n\n\n\n\n\n\nnPercentile [percentile]\n\n\n\n\nnPercentile(seriesList, n)\n\n\n\n\n\n\noffset [amount]\n\n\n\n\noffset(seriesList, factor)\n\n\n\n\n\n\npercentileOfSeries [n, true/false, tag]\n\n\n\n\npercentileOfSeries(seriesList, n, interpolate=False)\n\n\n\n\n\n\nperSecond\n\n\nrate()\n\n\nperSecond(seriesList, maxValue=None)\n\n\n\n\n\n\npromHistogramPercentile [percentileValue]\n\n\n\n\n\n\n\n\n\n\nrange [tag]\n\n\n\n\nrangeOfSeries(*seriesLists)\n\n\n\n\n\n\nremoveAbovePercentile [percentile]\n\n\n\n\nremoveAbovePercentile(seriesList, n)\n\n\n\n\n\n\nremoveBelowPercentile [percentile]\n\n\n\n\nremoveBelowPercentile(seriesList, n)\n\n\n\n\n\n\nremoveAboveValue [value]\n\n\n\n\nremoveAboveValue(seriesList, n)\n\n\n\n\n\n\nremoveBelowValue [value]\n\n\n\n\nremoveBelowValue(seriesList, n)\n\n\n\n\n\n\nremoveEmpty\n\n\n\n\nremoveEmptySeries(seriesList, xFilesFactor=None)\n\n\n\n\n\n\nscale [factor]\n\n\n\n\nscale(seriesList, factor)\n\n\n\n\n\n\nscaleToSeconds [seconds]\n\n\n\n\nscaleToSeconds(seriesList, seconds)\n\n\n\n\n\n\nsetDiff [tags]\n\n\n\n\n\n\n\n\n\n\nshowAnomalyThresholds [level, model]\n\n\n\n\n\n\n\n\n\n\nshowTags [true/false, tagName(s)]\n\n\n\n\n\n\n\n\n\n\nsort/sortSeries [avg, current, max, stddev, sum]\n\n\nsort()\n\n\nsortBy(seriesList, func='average', reverse=False)\n\n\n\n\n\n\nstdev [points, windowTolerance]\n\n\nstddev()\n\n\nstdev(seriesList, points, windowTolerance=0.1)\n\n\n\n\n\n\nsqrt/squareRoot\n\n\nsqrt()\n\n\nsquareRoot(seriesList)\n\n\n\n\n\n\nsummarize [interval, func, alignToFrom]\n\n\n\n\nsummarize(seriesList, intervalString, func='sum', alignToFrom=False)\n\n\n\n\n\n\nsum/sumSeries [tag]\n\n\nsum()\n\n\nsumSeries(*seriesLists)\n\n\n\n\n\n\nsustain [duration]\n\n\n\n\n\n\n\n\n\n\nsustainedAbove & sustainedBelow\n\n\n\n\n\n\n\n\n\n\ntail [limit]\n\n\nbottomk()\n\n\nlowest(seriesList, n=1, func='average')\n\n\n\n\n\n\ntimeshift [duration]\n\n\n\n\ntimeShift(seriesList, timeShift, resetEnd=True, alignDST=False)\n\n\n\n\n\n\ntimestamp\n\n\ntimestamp()\n\n\n\n\n\n\n\n\ntransformNull [value]\n\n\n\n\ntransformNull(seriesList, default=0, referenceSeries=None)",
            "title": "Function Processing"
        },
        {
            "location": "/m3query/architecture/functions/#function-processing",
            "text": "",
            "title": "Function Processing"
        },
        {
            "location": "/m3query/architecture/functions/#supported-functions",
            "text": "M3QL  Prometheus  Graphite      abs/absolute  abs()  absolute(seriesList)    alias [alias]   alias(seriesList, newName)    aliasByTags [tag]   aliasByTags(seriesList, *tags)    aliasByBucket/aliasByHistogramBucket [tag]      anomalies [flags]      asPercent  /  asPercent(seriesList, total=None, *nodes)    avg/averageSeries [tag]  avg()  averageSeries(*seriesLists)    changed   changed(seriesList)    constantLine [value]   constantLine(value)    count  count()  countSeries(*seriesLists)    derivative   derivative(seriesList)    diff  -  diffSeries(*seriesLists)    divideSeries  /  divideSeries(dividendSeriesList, divisorSeries)    eq/== [value]  ==  removeBelowValue(seriesList, n)/removeAboveValue(seriesList, n)    ne/!= [value]  !=  removeBelowValue(seriesList, n)/removeAboveValue(seriesList, n)    excludeByTag [tag, pattern]   exclude(seriesList, pattern)    execute/exec [fetch]      fallbackSeries [replacement]   fallbackSeries(seriesList, fallback)    fetch      ge/=> [value]  >=  removeBelowValue(seriesList, n)    gt/> [value]  >  removeBelowValue(seriesList, n)    head [limit]  topk()  highest(seriesList, n=1, func='average')    histogramCDF [idTag, rangeTag, value]      histogramPercentile [idTag, rangeTag, percentileValue]      identity [name]   identity(name)    integral   integral(seriesList)    intersect [tags]  and/or     isNonNull   isNonNull(seriesList)    jainCP      keepLastValue   keepLastValue(seriesList, limit=inf)    le/<= [value]  <=  removeAboveValue(seriesList, n)    logarithm  ln()  logarithm(seriesList, base=10)    lt/< [value]  <  removeAboveValue(seriesList, n)    max/maxSeries [tag]  max()  maxSeries(*seriesLists)    min/minSeries [tag]  min()  minSeries(*seriesLists)    moving [interval, func]  _over_time()  movingMax, movingMin, movingMedian, movingAverage, etc.    multiply/multiplySeries [tag]  *  multiplySeries(*seriesLists)    nonNegativeDerivative [maxValue]   nonNegativeDerivative(seriesList, maxValue=None)    nPercentile [percentile]   nPercentile(seriesList, n)    offset [amount]   offset(seriesList, factor)    percentileOfSeries [n, true/false, tag]   percentileOfSeries(seriesList, n, interpolate=False)    perSecond  rate()  perSecond(seriesList, maxValue=None)    promHistogramPercentile [percentileValue]      range [tag]   rangeOfSeries(*seriesLists)    removeAbovePercentile [percentile]   removeAbovePercentile(seriesList, n)    removeBelowPercentile [percentile]   removeBelowPercentile(seriesList, n)    removeAboveValue [value]   removeAboveValue(seriesList, n)    removeBelowValue [value]   removeBelowValue(seriesList, n)    removeEmpty   removeEmptySeries(seriesList, xFilesFactor=None)    scale [factor]   scale(seriesList, factor)    scaleToSeconds [seconds]   scaleToSeconds(seriesList, seconds)    setDiff [tags]      showAnomalyThresholds [level, model]      showTags [true/false, tagName(s)]      sort/sortSeries [avg, current, max, stddev, sum]  sort()  sortBy(seriesList, func='average', reverse=False)    stdev [points, windowTolerance]  stddev()  stdev(seriesList, points, windowTolerance=0.1)    sqrt/squareRoot  sqrt()  squareRoot(seriesList)    summarize [interval, func, alignToFrom]   summarize(seriesList, intervalString, func='sum', alignToFrom=False)    sum/sumSeries [tag]  sum()  sumSeries(*seriesLists)    sustain [duration]      sustainedAbove & sustainedBelow      tail [limit]  bottomk()  lowest(seriesList, n=1, func='average')    timeshift [duration]   timeShift(seriesList, timeShift, resetEnd=True, alignDST=False)    timestamp  timestamp()     transformNull [value]   transformNull(seriesList, default=0, referenceSeries=None)",
            "title": "Supported Functions"
        },
        {
            "location": "/how_to/single_node/",
            "text": "M3DB Single Node Deployment\n\n\nDeploying a single-node cluster is a great way to experiment with M3DB and get a feel for what it\nhas to offer. Our Docker image by default configures a single M3DB instance as one binary\ncontaining:\n\n\n\n\nAn M3DB storage instance (\nm3dbnode\n) for timeseries storage. This includes an embedded tag-based\n  metrics index, as well as as an embedded etcd server for storing the above mentioned cluster\n  topology and runtime configuration.\n\n\nA \"coordinator\" instance (\nm3coordinator\n) for writing and querying tagged metrics, as well as\n  managing cluster topology and runtime configuration.\n\n\n\n\nTo begin, first start up a Docker container with port \n7201\n (used to manage the cluster topology), port \n7203\n which is where Prometheus scrapes metrics produced by \nM3DB\n and \nM3Coordinator\n, and port \n9003\n (used to read and write metrics) exposed. We recommend you create a persistent data\ndirectory on your host for durability:\n\n\ndocker pull quay.io/m3db/m3dbnode:latest\ndocker run -p 7201:7201 -p 7203:7203 -p 9003:9003 --name m3db -v $(pwd)/m3db_data:/var/lib/m3db quay.io/m3db/m3dbnode:latest\n\n\n\n\nNote:\n For the single node case, we use this \nsample config file\n. If you inspect the file, you'll see that all the configuration is namespaced by \ncoordinator\n or \ndb\n. That's because this setup runs \nM3DB\n and \nM3Coordinator\n as one application. While this is convenient for testing and development, you'll want to run clustered \nM3DB\n with a separate \nM3Coordinator\n in production. You can read more about that \nhere.\n.\n\n\nNext, create an initial namespace for your metrics in the database using the cURL below. Keep in mind that the provided \nnamespaceName\n must match the namespace in the \nlocal\n section of the \nM3Coordinator\n YAML configuration, and if you choose to \nadd any additional namespaces\n you'll need to add them to the \nlocal\n section of \nM3Coordinator\n's YAML config as well.\n\n\ncurl\n \n-X\n \nPOST\n \nhttp://localhost:\n7201\n/api/v\n1\n/database/create\n \n-d\n \n'\n{\n\n  \n\"type\"\n:\n \n\"local\"\n,\n\n  \n\"namespaceName\"\n:\n \n\"default\"\n,\n\n  \n\"retentionTime\"\n:\n \n\"12h\"\n\n\n}\n'\n\n\n\n\n\nNote\n: The \napi/v1/database/create\n endpoint is abstraction over two concepts in M3DB called \nplacements\n and \nnamespaces\n. If a placement doesn't exist, it will create one based on the \ntype\n argument, otherwise if the placement already exists, it just creates the specified namespace. For now it's enough to just understand that it creates M3DB namespaces (tables), but if you're going to run a clustered M3 setup in production, make sure you familiarize yourself with the links above.\n\n\nPlacement initialization may take a minute or two and you can check on the status of this by running the following:\n\n\ncurl http://localhost:7201/api/v1/placement | jq .\n\n\n\n\nOnce all of the shards become \nAVAILABLE\n, you should see your node complete bootstrapping! Don't worry if you see warnings or errors related to a local cache file, such as \n[W] could not load cache from file\n/var/lib/m3kv/m3db_embedded.json\n. Those are expected for a local instance and in general any\nwarn-level errors (prefixed with \n[W]\n) should not block bootstrapping.\n\n\n02:28:30.008072[I] updating database namespaces [{adds [default]} {updates []} {removals []}]\n02:28:30.270681[I] node tchannelthrift: listening on 0.0.0.0:9000\n02:28:30.271909[I] cluster tchannelthrift: listening on 0.0.0.0:9001\n02:28:30.519468[I] node httpjson: listening on 0.0.0.0:9002\n02:28:30.520061[I] cluster httpjson: listening on 0.0.0.0:9003\n02:28:30.520652[I] bootstrap finished [{namespace metrics} {duration 55.4\u00b5s}]\n02:28:30.520909[I] bootstrapped\n\n\n\n\nThe node also self-hosts its OpenAPI docs, outlining available endpoints. You can access this by\ngoing to \nlocalhost:7201/api/v1/openapi\n in your browser.\n\n\n\n\nNow you can experiment with writing tagged metrics:\n\ncurl\n \n-sS\n \n-X\n \nPOST\n \nhttp://localhost:\n9003\n/writetagged\n \n-d\n \n'\n{\n\n  \n\"namespace\"\n:\n \n\"default\"\n,\n\n  \n\"id\"\n:\n \n\"foo\"\n,\n\n  \n\"tags\"\n:\n \n[\n\n    \n{\n\n      \n\"name\"\n:\n \n\"__name__\"\n,\n\n      \n\"value\"\n:\n \n\"user_login\"\n\n    \n},\n\n    \n{\n\n      \n\"name\"\n:\n \n\"city\"\n,\n\n      \n\"value\"\n:\n \n\"new_york\"\n\n    \n},\n\n    \n{\n\n      \n\"name\"\n:\n \n\"endpoint\"\n,\n\n      \n\"value\"\n:\n \n\"/request\"\n\n    \n}\n\n  \n],\n\n  \n\"datapoint\"\n:\n \n{\n\n    \n\"timestamp\"\n:\n \n'\n\"$(date \"\n+%s\n\")\"\n'\n,\n\n    \n\"value\"\n:\n \n42.123456789\n\n  \n}\n\n\n}\n\n\n'\n\n\n\n\nNote:\n In the above example we include the tag \n__name__\n. This is because \n__name__\n is a\nreserved tag in Prometheus and will make querying the metric much easier. For example, if you have\n\nM3Query\n setup as a Prometheus datasource in Grafana, you can then query for the metric\nusing the following PromQL query:\n\n\nuser_login{city=\"new_york\",endpoint=\"/request\"}\n\n\n\n\nAnd reading the metrics you've written using the M3DB \n/query\n endpoint:\n\ncurl\n \n-sS\n \n-X\n \nPOST\n \nhttp://localhost:\n9003\n/query\n \n-d\n \n'\n{\n\n  \n\"namespace\"\n:\n \n\"default\"\n,\n\n  \n\"query\"\n:\n \n{\n\n    \n\"regexp\"\n:\n \n{\n\n      \n\"field\"\n:\n \n\"city\"\n,\n\n      \n\"regexp\"\n:\n \n\".*\"\n\n    \n}\n\n  \n},\n\n  \n\"rangeStart\"\n:\n \n0\n,\n\n  \n\"rangeEnd\"\n:\n \n'\n\"$(date \"\n+%s\n\")\"\n'\n\n\n}\n'\n \n|\n \njq\n \n.\n\n\n\n{\n\n  \n\"results\"\n:\n \n[\n\n    \n{\n\n      \n\"id\"\n:\n \n\"foo\"\n,\n\n      \n\"tags\"\n:\n \n[\n\n        \n{\n\n          \n\"name\"\n:\n \n\"__name__\"\n,\n\n          \n\"value\"\n:\n \n\"user_login\"\n\n        \n},\n\n        \n{\n\n          \n\"name\"\n:\n \n\"city\"\n,\n\n          \n\"value\"\n:\n \n\"new_york\"\n\n        \n},\n\n        \n{\n\n          \n\"name\"\n:\n \n\"endpoint\"\n,\n\n          \n\"value\"\n:\n \n\"/request\"\n\n        \n}\n\n      \n],\n\n      \n\"datapoints\"\n:\n \n[\n\n        \n{\n\n          \n\"timestamp\"\n:\n \n1527039389\n,\n\n          \n\"value\"\n:\n \n42.123456789\n\n        \n}\n\n      \n]\n\n    \n}\n\n  \n],\n\n  \n\"exhaustive\"\n:\n \ntrue\n\n\n}\n\n\n\n\nNow that you've got the M3 stack up and running, take a look at the rest of our documentation to see how you can integrate with \nPrometheus\n and \nGraphite",
            "title": "M3DB Single Node Deployment"
        },
        {
            "location": "/how_to/single_node/#m3db-single-node-deployment",
            "text": "Deploying a single-node cluster is a great way to experiment with M3DB and get a feel for what it\nhas to offer. Our Docker image by default configures a single M3DB instance as one binary\ncontaining:   An M3DB storage instance ( m3dbnode ) for timeseries storage. This includes an embedded tag-based\n  metrics index, as well as as an embedded etcd server for storing the above mentioned cluster\n  topology and runtime configuration.  A \"coordinator\" instance ( m3coordinator ) for writing and querying tagged metrics, as well as\n  managing cluster topology and runtime configuration.   To begin, first start up a Docker container with port  7201  (used to manage the cluster topology), port  7203  which is where Prometheus scrapes metrics produced by  M3DB  and  M3Coordinator , and port  9003  (used to read and write metrics) exposed. We recommend you create a persistent data\ndirectory on your host for durability:  docker pull quay.io/m3db/m3dbnode:latest\ndocker run -p 7201:7201 -p 7203:7203 -p 9003:9003 --name m3db -v $(pwd)/m3db_data:/var/lib/m3db quay.io/m3db/m3dbnode:latest  Note:  For the single node case, we use this  sample config file . If you inspect the file, you'll see that all the configuration is namespaced by  coordinator  or  db . That's because this setup runs  M3DB  and  M3Coordinator  as one application. While this is convenient for testing and development, you'll want to run clustered  M3DB  with a separate  M3Coordinator  in production. You can read more about that  here. .  Next, create an initial namespace for your metrics in the database using the cURL below. Keep in mind that the provided  namespaceName  must match the namespace in the  local  section of the  M3Coordinator  YAML configuration, and if you choose to  add any additional namespaces  you'll need to add them to the  local  section of  M3Coordinator 's YAML config as well.  curl   -X   POST   http://localhost: 7201 /api/v 1 /database/create   -d   ' { \n   \"type\" :   \"local\" , \n   \"namespaceName\" :   \"default\" , \n   \"retentionTime\" :   \"12h\"  } '   Note : The  api/v1/database/create  endpoint is abstraction over two concepts in M3DB called  placements  and  namespaces . If a placement doesn't exist, it will create one based on the  type  argument, otherwise if the placement already exists, it just creates the specified namespace. For now it's enough to just understand that it creates M3DB namespaces (tables), but if you're going to run a clustered M3 setup in production, make sure you familiarize yourself with the links above.  Placement initialization may take a minute or two and you can check on the status of this by running the following:  curl http://localhost:7201/api/v1/placement | jq .  Once all of the shards become  AVAILABLE , you should see your node complete bootstrapping! Don't worry if you see warnings or errors related to a local cache file, such as  [W] could not load cache from file\n/var/lib/m3kv/m3db_embedded.json . Those are expected for a local instance and in general any\nwarn-level errors (prefixed with  [W] ) should not block bootstrapping.  02:28:30.008072[I] updating database namespaces [{adds [default]} {updates []} {removals []}]\n02:28:30.270681[I] node tchannelthrift: listening on 0.0.0.0:9000\n02:28:30.271909[I] cluster tchannelthrift: listening on 0.0.0.0:9001\n02:28:30.519468[I] node httpjson: listening on 0.0.0.0:9002\n02:28:30.520061[I] cluster httpjson: listening on 0.0.0.0:9003\n02:28:30.520652[I] bootstrap finished [{namespace metrics} {duration 55.4\u00b5s}]\n02:28:30.520909[I] bootstrapped  The node also self-hosts its OpenAPI docs, outlining available endpoints. You can access this by\ngoing to  localhost:7201/api/v1/openapi  in your browser.   Now you can experiment with writing tagged metrics: curl   -sS   -X   POST   http://localhost: 9003 /writetagged   -d   ' { \n   \"namespace\" :   \"default\" , \n   \"id\" :   \"foo\" , \n   \"tags\" :   [ \n     { \n       \"name\" :   \"__name__\" , \n       \"value\" :   \"user_login\" \n     }, \n     { \n       \"name\" :   \"city\" , \n       \"value\" :   \"new_york\" \n     }, \n     { \n       \"name\" :   \"endpoint\" , \n       \"value\" :   \"/request\" \n     } \n   ], \n   \"datapoint\" :   { \n     \"timestamp\" :   ' \"$(date \" +%s \")\" ' , \n     \"value\" :   42.123456789 \n   }  }  '   Note:  In the above example we include the tag  __name__ . This is because  __name__  is a\nreserved tag in Prometheus and will make querying the metric much easier. For example, if you have M3Query  setup as a Prometheus datasource in Grafana, you can then query for the metric\nusing the following PromQL query:  user_login{city=\"new_york\",endpoint=\"/request\"}  And reading the metrics you've written using the M3DB  /query  endpoint: curl   -sS   -X   POST   http://localhost: 9003 /query   -d   ' { \n   \"namespace\" :   \"default\" , \n   \"query\" :   { \n     \"regexp\" :   { \n       \"field\" :   \"city\" , \n       \"regexp\" :   \".*\" \n     } \n   }, \n   \"rangeStart\" :   0 , \n   \"rangeEnd\" :   ' \"$(date \" +%s \")\" '  } '   |   jq   .  { \n   \"results\" :   [ \n     { \n       \"id\" :   \"foo\" , \n       \"tags\" :   [ \n         { \n           \"name\" :   \"__name__\" , \n           \"value\" :   \"user_login\" \n         }, \n         { \n           \"name\" :   \"city\" , \n           \"value\" :   \"new_york\" \n         }, \n         { \n           \"name\" :   \"endpoint\" , \n           \"value\" :   \"/request\" \n         } \n       ], \n       \"datapoints\" :   [ \n         { \n           \"timestamp\" :   1527039389 , \n           \"value\" :   42.123456789 \n         } \n       ] \n     } \n   ], \n   \"exhaustive\" :   true  }   Now that you've got the M3 stack up and running, take a look at the rest of our documentation to see how you can integrate with  Prometheus  and  Graphite",
            "title": "M3DB Single Node Deployment"
        },
        {
            "location": "/how_to/cluster_hard_way/",
            "text": "M3DB Cluster Deployment, Manually (The Hard Way)\n\n\nIntroduction\n\n\nThis document lists the manual steps involved in deploying a M3DB cluster. In practice, you'd be automating this using Terraform or using Kubernetes rather than doing this by hand; guides for doing so are available under the How-To section.\n\n\nPrimer Architecture\n\n\nA quick primer on M3DB architecture. Here\u2019s what a typical deployment looks like:\n\n\n\n\nA few different things to highlight about the diagram:\n\n\nRole Type\n\n\nThere are three \u2018role types\u2019 for a m3db deployment -\n\n\n\n\n\n\nCoordinator: \nm3coordinator\n serves to coordinate reads and writes across all hosts in the cluster. It\u2019s a lightweight process, and does not store any data. This role would typically be run alongside a Prometheus instance, or be baked into a collector agent.\n\n\n\n\n\n\nStorage Node: \nm3dbnode\n processes running on these hosts are the workhorses of the database, they store data; and serve reads and writes.\n\n\n\n\n\n\nSeed Node: First and foremost, these hosts are storage nodes themselves. In addition to that responsibility, they run an embedded ETCD server. This is to allow the various M3DB processes running across the cluster to reason about the topology/configuration of the cluster in a consistent manner.\n\n\n\n\n\n\nNote: In very large deployments, you\u2019d use a dedicated ETCD cluster, and only use M3DB Storage and Coordinator Nodes\n\n\nProvisioning\n\n\nEnough background, lets get you going with a real cluster! Provision your host (be it VMs from AWS/GCP/etc) or bare-metal servers in your DC with the latest and greatest flavour of Linux you favor. M3DB works on all popular distributions - Ubuntu/RHEL/CentOS, let us know if you run into issues on another platform and we\u2019ll be happy to assist.\n\n\nNetwork\n\n\nIf you\u2019re using AWS or GCP it is highly advised to use static IPs so that if you need to replace a host, you don\u2019t have to update your configuration files on all the hosts, you simply decomission the old seed node and provision a new seed node with the same host ID and static IP that the old seed node had.  For AWS you can use a \nElastic Network Interface\n on a VPC and for GCP you can simply use an \ninternal static IP address\n.\n\n\nIn this example you will be creating three static IP addresses for the three seed nodes.\n\n\nFurther, we assume you have hostnames configured correctly too. i.e. running \nhostname\n on a host in the cluster returns the host ID you'll be using when specifying instance host IDs when creating the M3DB cluster placement. E.g. running \nhostname\n on a node \nm3db001\n should return it's host ID \nm3db001\n.\n\n\nIn GCP the name of your instance when you create it will automatically be it's hostname. When you create an instance click \"Management, disks, networking, SSH keys\" and under \"Networking\" click the default interface and click the \"Primary internal IP\" drop down and select \"Reserve a static internal IP address\" and give it a name, i.e. \nm3db001\n, a description that describes it's a seed node IP address and use \"Assign automatically\".\n\n\nIn AWS it might be simpler to just use whatever the hostname you get for the provisioned VM as your host ID when specifying M3DB placement.  Either that or use the \nenvironment\n host ID resolver and pass your host ID when launching the database process with an environment variable.  You can set to the host ID and specify the environment variable name in config as \nenvVarName: M3DB_HOST_ID\n if you are using an environment variable named \nM3DB_HOST_ID\n.\n\n\nRelevant config snippet:\n\nhostID:\n  resolver: environment\n  envVarName: M3DB_HOST_ID\n\n\n\nThen start your process with:\n\nM3DB_HOST_ID=m3db001 m3dbnode -f config.yml\n\n\n\nKernel\n\n\nEnsure you review our \nrecommended kernel configuration\n before running M3DB in production as M3DB may exceed the default limits for some default kernel values.\n\n\nConfig files\n\n\nWe wouldn\u2019t feel right to call this guide, \u201cThe Hard Way\u201d and not require you to change some configs by hand.\n\n\nNote: the steps that follow assume you have the following 3 seed nodes - make necessary adjustment if you have more or are using a dedicated ETCD cluster. Example seed nodes:\n\n\n\n\nm3db001 (Region=us-east1, Zone=us-east1-a, Static IP=10.142.0.1)\n\n\nm3db002 (Region=us-east1, Zone=us-east1-b, Static IP=10.142.0.2)\n\n\nm3db003 (Region=us-east1, Zone=us-east1-c, Static IP=10.142.0.3)\n\n\n\n\nWe\u2019re going to start with the M3DB config template and modify it to work for your cluster. Start by downloading the \nconfig\n. Update the config \u2018service\u2019 and 'seedNodes' sections to read as follows:\n\n\nconfig:\n  service:\n    env: default_env\n    zone: embedded\n    service: m3db\n    cacheDir: /var/lib/m3kv\n    etcdClusters:\n      - zone: embedded\n        endpoints:\n          - 10.142.0.1:2379\n          - 10.142.0.2:2379\n          - 10.142.0.3:2379\n  seedNodes:\n    initialCluster:\n      - hostID: m3db001\n        endpoint: http://10.142.0.1:2380\n      - hostID: m3db002\n        endpoint: http://10.142.0.2:2380\n      - hostID: m3db003\n        endpoint: http://10.142.0.3:2380\n\n\n\n\nStart the seed nodes\n\n\nTransfer the config you just crafted to each host in the cluster. And then starting with the seed nodes, start up the m3dbnode process:\n\n\nm3dbnode -f <config-name.yml>\n\n\n\n\nNote, remember to daemon-ize this using your favourite utility: systemd/init.d/supervisor/etc\n\n\nCreate Namespace and Initialize Topology\n\n\nThe recommended way to create a namespace and initialize a topology is to use the \n/api/v1/database/create\n api. Below is an example.\n\n\nNote:\n In order to create a more custom setup, please refer to the \nnamespace configuration\n and \n\nplacement configuration\n guides, though this is discouraged.\n\n\ncurl\n \n-X\n \nPOST\n \nhttp://localhost:\n7201\n/api/v\n1\n/database/create\n \n-d\n \n'\n{\n\n  \n\"type\"\n:\n \n\"cluster\"\n,\n\n  \n\"namespaceName\"\n:\n \n\"1week_namespace\"\n,\n\n  \n\"retentionTime\"\n:\n \n\"168h\"\n,\n\n  \n\"numShards\"\n:\n \n\"1024\"\n,\n\n  \n\"replicationFactor\"\n:\n \n\"3\"\n,\n\n  \n\"hosts\"\n:\n \n[\n\n        \n{\n\n            \n\"id\"\n:\n \n\"m3db001\"\n,\n\n            \n\"isolationGroup\"\n:\n \n\"us-east1-a\"\n,\n\n            \n\"zone\"\n:\n \n\"embedded\"\n,\n\n            \n\"weight\"\n:\n \n100\n,\n\n            \n\"address\"\n:\n \n\"10.142.0.1\"\n,\n\n            \n\"port\"\n:\n \n9000\n\n        \n},\n\n        \n{\n\n            \n\"id\"\n:\n \n\"m3db002\"\n,\n\n            \n\"isolationGroup\"\n:\n \n\"us-east1-b\"\n,\n\n            \n\"zone\"\n:\n \n\"embedded\"\n,\n\n            \n\"weight\"\n:\n \n100\n,\n\n            \n\"address\"\n:\n \n\"10.142.0.2\"\n,\n\n            \n\"port\"\n:\n \n9000\n\n        \n},\n\n        \n{\n\n            \n\"id\"\n:\n \n\"m3db003\"\n,\n\n            \n\"isolationGroup\"\n:\n \n\"us-east1-c\"\n,\n\n            \n\"zone\"\n:\n \n\"embedded\"\n,\n\n            \n\"weight\"\n:\n \n100\n,\n\n            \n\"address\"\n:\n \n\"10.142.0.3\"\n,\n\n            \n\"port\"\n:\n \n9000\n\n        \n}\n\n    \n]\n\n\n}\n'\n\n\n\n\n\nNote:\n Isolation group specifies how the cluster places shards to avoid more than one replica of a shard appearing in the same replica group. As such you must be using at least as many isolation groups as your replication factor. In this example we use the availibity zones \nus-east1-a\n, \nus-east1-b\n, \nus-east1-c\n as our isolation groups which matches our replication factor of 3.\n\n\nShortly after, you should see your node complete bootstrapping:\n\n\n20:10:12.911218[I] updating database namespaces [{adds [default]} {updates []} {removals []}]\n20:10:13.462798[I] node tchannelthrift: listening on 0.0.0.0:9000\n20:10:13.463107[I] cluster tchannelthrift: listening on 0.0.0.0:9001\n20:10:13.747173[I] node httpjson: listening on 0.0.0.0:9002\n20:10:13.747506[I] cluster httpjson: listening on 0.0.0.0:9003\n20:10:13.747763[I] bootstrapping shards for range starting ...\n...\n20:10:13.757834[I] bootstrap finished [{namespace metrics} {duration 10.1261ms}]\n20:10:13.758001[I] bootstrapped\n20:10:14.764771[I] successfully updated topology to 3 hosts\n\n\n\n\nIf you need to setup multiple namespaces, you can run the above \n/api/v1/database/create\n command multiple times with different namespace configurations.\n\n\nReplication factor (RF)\n\n\nRecommended is RF3, where each replica is spread across failure domains such as a rack, data center or availability zone. See \nReplication Factor Recommendations\n for more specifics.\n\n\nShards\n\n\nSee \nplacement configuration\n to determine the appropriate number of shards to specify.\n\n\nTest it out\n\n\nNow you can experiment with writing tagged metrics:\n\n\ncurl\n \n-sS\n \n-X\n \nPOST\n \nlocalhost:\n9003\n/writetagged\n \n-d\n \n'\n{\n\n  \n\"namespace\"\n:\n \n\"metrics\"\n,\n\n  \n\"id\"\n:\n \n\"foo\"\n,\n\n  \n\"tags\"\n:\n \n[\n\n    \n{\n\n      \n\"name\"\n:\n \n\"city\"\n,\n\n      \n\"value\"\n:\n \n\"new_york\"\n\n    \n},\n\n    \n{\n\n      \n\"name\"\n:\n \n\"endpoint\"\n,\n\n      \n\"value\"\n:\n \n\"/request\"\n\n    \n}\n\n  \n],\n\n  \n\"datapoint\"\n:\n \n{\n\n    \n\"timestamp\"\n:\n \n'\n\"$(date \"\n+%s\n\")\"\n'\n,\n\n    \n\"value\"\n:\n \n42.123456789\n\n  \n}\n\n\n}\n'\n\n\n\n\n\nAnd reading the metrics you've written:\n\n\ncurl\n \n-sS\n \n-X\n \nPOST\n \nhttp://localhost:\n9003\n/query\n \n-d\n \n'\n{\n\n  \n\"namespace\"\n:\n \n\"metrics\"\n,\n\n  \n\"query\"\n:\n \n{\n\n    \n\"regexp\"\n:\n \n{\n\n      \n\"field\"\n:\n \n\"city\"\n,\n\n      \n\"regexp\"\n:\n \n\".*\"\n\n    \n}\n\n  \n},\n\n  \n\"rangeStart\"\n:\n \n0\n,\n\n  \n\"rangeEnd\"\n:\n \n'\n\"$(date \"\n+%s\n\")\"\n'\n\n\n}\n'\n \n|\n \njq\n \n.\n\n\n\n\n\nIntegrations\n\n\nPrometheus as a long term storage remote read/write endpoint\n.",
            "title": "M3DB Cluster Deployment, Manually"
        },
        {
            "location": "/how_to/cluster_hard_way/#m3db-cluster-deployment-manually-the-hard-way",
            "text": "",
            "title": "M3DB Cluster Deployment, Manually (The Hard Way)"
        },
        {
            "location": "/how_to/cluster_hard_way/#introduction",
            "text": "This document lists the manual steps involved in deploying a M3DB cluster. In practice, you'd be automating this using Terraform or using Kubernetes rather than doing this by hand; guides for doing so are available under the How-To section.",
            "title": "Introduction"
        },
        {
            "location": "/how_to/cluster_hard_way/#primer-architecture",
            "text": "A quick primer on M3DB architecture. Here\u2019s what a typical deployment looks like:   A few different things to highlight about the diagram:",
            "title": "Primer Architecture"
        },
        {
            "location": "/how_to/cluster_hard_way/#role-type",
            "text": "There are three \u2018role types\u2019 for a m3db deployment -    Coordinator:  m3coordinator  serves to coordinate reads and writes across all hosts in the cluster. It\u2019s a lightweight process, and does not store any data. This role would typically be run alongside a Prometheus instance, or be baked into a collector agent.    Storage Node:  m3dbnode  processes running on these hosts are the workhorses of the database, they store data; and serve reads and writes.    Seed Node: First and foremost, these hosts are storage nodes themselves. In addition to that responsibility, they run an embedded ETCD server. This is to allow the various M3DB processes running across the cluster to reason about the topology/configuration of the cluster in a consistent manner.    Note: In very large deployments, you\u2019d use a dedicated ETCD cluster, and only use M3DB Storage and Coordinator Nodes",
            "title": "Role Type"
        },
        {
            "location": "/how_to/cluster_hard_way/#provisioning",
            "text": "Enough background, lets get you going with a real cluster! Provision your host (be it VMs from AWS/GCP/etc) or bare-metal servers in your DC with the latest and greatest flavour of Linux you favor. M3DB works on all popular distributions - Ubuntu/RHEL/CentOS, let us know if you run into issues on another platform and we\u2019ll be happy to assist.",
            "title": "Provisioning"
        },
        {
            "location": "/how_to/cluster_hard_way/#network",
            "text": "If you\u2019re using AWS or GCP it is highly advised to use static IPs so that if you need to replace a host, you don\u2019t have to update your configuration files on all the hosts, you simply decomission the old seed node and provision a new seed node with the same host ID and static IP that the old seed node had.  For AWS you can use a  Elastic Network Interface  on a VPC and for GCP you can simply use an  internal static IP address .  In this example you will be creating three static IP addresses for the three seed nodes.  Further, we assume you have hostnames configured correctly too. i.e. running  hostname  on a host in the cluster returns the host ID you'll be using when specifying instance host IDs when creating the M3DB cluster placement. E.g. running  hostname  on a node  m3db001  should return it's host ID  m3db001 .  In GCP the name of your instance when you create it will automatically be it's hostname. When you create an instance click \"Management, disks, networking, SSH keys\" and under \"Networking\" click the default interface and click the \"Primary internal IP\" drop down and select \"Reserve a static internal IP address\" and give it a name, i.e.  m3db001 , a description that describes it's a seed node IP address and use \"Assign automatically\".  In AWS it might be simpler to just use whatever the hostname you get for the provisioned VM as your host ID when specifying M3DB placement.  Either that or use the  environment  host ID resolver and pass your host ID when launching the database process with an environment variable.  You can set to the host ID and specify the environment variable name in config as  envVarName: M3DB_HOST_ID  if you are using an environment variable named  M3DB_HOST_ID .  Relevant config snippet: hostID:\n  resolver: environment\n  envVarName: M3DB_HOST_ID  Then start your process with: M3DB_HOST_ID=m3db001 m3dbnode -f config.yml",
            "title": "Network"
        },
        {
            "location": "/how_to/cluster_hard_way/#kernel",
            "text": "Ensure you review our  recommended kernel configuration  before running M3DB in production as M3DB may exceed the default limits for some default kernel values.",
            "title": "Kernel"
        },
        {
            "location": "/how_to/cluster_hard_way/#config-files",
            "text": "We wouldn\u2019t feel right to call this guide, \u201cThe Hard Way\u201d and not require you to change some configs by hand.  Note: the steps that follow assume you have the following 3 seed nodes - make necessary adjustment if you have more or are using a dedicated ETCD cluster. Example seed nodes:   m3db001 (Region=us-east1, Zone=us-east1-a, Static IP=10.142.0.1)  m3db002 (Region=us-east1, Zone=us-east1-b, Static IP=10.142.0.2)  m3db003 (Region=us-east1, Zone=us-east1-c, Static IP=10.142.0.3)   We\u2019re going to start with the M3DB config template and modify it to work for your cluster. Start by downloading the  config . Update the config \u2018service\u2019 and 'seedNodes' sections to read as follows:  config:\n  service:\n    env: default_env\n    zone: embedded\n    service: m3db\n    cacheDir: /var/lib/m3kv\n    etcdClusters:\n      - zone: embedded\n        endpoints:\n          - 10.142.0.1:2379\n          - 10.142.0.2:2379\n          - 10.142.0.3:2379\n  seedNodes:\n    initialCluster:\n      - hostID: m3db001\n        endpoint: http://10.142.0.1:2380\n      - hostID: m3db002\n        endpoint: http://10.142.0.2:2380\n      - hostID: m3db003\n        endpoint: http://10.142.0.3:2380",
            "title": "Config files"
        },
        {
            "location": "/how_to/cluster_hard_way/#start-the-seed-nodes",
            "text": "Transfer the config you just crafted to each host in the cluster. And then starting with the seed nodes, start up the m3dbnode process:  m3dbnode -f <config-name.yml>  Note, remember to daemon-ize this using your favourite utility: systemd/init.d/supervisor/etc",
            "title": "Start the seed nodes"
        },
        {
            "location": "/how_to/cluster_hard_way/#create-namespace-and-initialize-topology",
            "text": "The recommended way to create a namespace and initialize a topology is to use the  /api/v1/database/create  api. Below is an example.  Note:  In order to create a more custom setup, please refer to the  namespace configuration  and  placement configuration  guides, though this is discouraged.  curl   -X   POST   http://localhost: 7201 /api/v 1 /database/create   -d   ' { \n   \"type\" :   \"cluster\" , \n   \"namespaceName\" :   \"1week_namespace\" , \n   \"retentionTime\" :   \"168h\" , \n   \"numShards\" :   \"1024\" , \n   \"replicationFactor\" :   \"3\" , \n   \"hosts\" :   [ \n         { \n             \"id\" :   \"m3db001\" , \n             \"isolationGroup\" :   \"us-east1-a\" , \n             \"zone\" :   \"embedded\" , \n             \"weight\" :   100 , \n             \"address\" :   \"10.142.0.1\" , \n             \"port\" :   9000 \n         }, \n         { \n             \"id\" :   \"m3db002\" , \n             \"isolationGroup\" :   \"us-east1-b\" , \n             \"zone\" :   \"embedded\" , \n             \"weight\" :   100 , \n             \"address\" :   \"10.142.0.2\" , \n             \"port\" :   9000 \n         }, \n         { \n             \"id\" :   \"m3db003\" , \n             \"isolationGroup\" :   \"us-east1-c\" , \n             \"zone\" :   \"embedded\" , \n             \"weight\" :   100 , \n             \"address\" :   \"10.142.0.3\" , \n             \"port\" :   9000 \n         } \n     ]  } '   Note:  Isolation group specifies how the cluster places shards to avoid more than one replica of a shard appearing in the same replica group. As such you must be using at least as many isolation groups as your replication factor. In this example we use the availibity zones  us-east1-a ,  us-east1-b ,  us-east1-c  as our isolation groups which matches our replication factor of 3.  Shortly after, you should see your node complete bootstrapping:  20:10:12.911218[I] updating database namespaces [{adds [default]} {updates []} {removals []}]\n20:10:13.462798[I] node tchannelthrift: listening on 0.0.0.0:9000\n20:10:13.463107[I] cluster tchannelthrift: listening on 0.0.0.0:9001\n20:10:13.747173[I] node httpjson: listening on 0.0.0.0:9002\n20:10:13.747506[I] cluster httpjson: listening on 0.0.0.0:9003\n20:10:13.747763[I] bootstrapping shards for range starting ...\n...\n20:10:13.757834[I] bootstrap finished [{namespace metrics} {duration 10.1261ms}]\n20:10:13.758001[I] bootstrapped\n20:10:14.764771[I] successfully updated topology to 3 hosts  If you need to setup multiple namespaces, you can run the above  /api/v1/database/create  command multiple times with different namespace configurations.",
            "title": "Create Namespace and Initialize Topology"
        },
        {
            "location": "/how_to/cluster_hard_way/#replication-factor-rf",
            "text": "Recommended is RF3, where each replica is spread across failure domains such as a rack, data center or availability zone. See  Replication Factor Recommendations  for more specifics.",
            "title": "Replication factor (RF)"
        },
        {
            "location": "/how_to/cluster_hard_way/#shards",
            "text": "See  placement configuration  to determine the appropriate number of shards to specify.",
            "title": "Shards"
        },
        {
            "location": "/how_to/cluster_hard_way/#test-it-out",
            "text": "Now you can experiment with writing tagged metrics:  curl   -sS   -X   POST   localhost: 9003 /writetagged   -d   ' { \n   \"namespace\" :   \"metrics\" , \n   \"id\" :   \"foo\" , \n   \"tags\" :   [ \n     { \n       \"name\" :   \"city\" , \n       \"value\" :   \"new_york\" \n     }, \n     { \n       \"name\" :   \"endpoint\" , \n       \"value\" :   \"/request\" \n     } \n   ], \n   \"datapoint\" :   { \n     \"timestamp\" :   ' \"$(date \" +%s \")\" ' , \n     \"value\" :   42.123456789 \n   }  } '   And reading the metrics you've written:  curl   -sS   -X   POST   http://localhost: 9003 /query   -d   ' { \n   \"namespace\" :   \"metrics\" , \n   \"query\" :   { \n     \"regexp\" :   { \n       \"field\" :   \"city\" , \n       \"regexp\" :   \".*\" \n     } \n   }, \n   \"rangeStart\" :   0 , \n   \"rangeEnd\" :   ' \"$(date \" +%s \")\" '  } '   |   jq   .",
            "title": "Test it out"
        },
        {
            "location": "/how_to/cluster_hard_way/#integrations",
            "text": "Prometheus as a long term storage remote read/write endpoint .",
            "title": "Integrations"
        },
        {
            "location": "/how_to/kubernetes/",
            "text": "M3DB on Kubernetes\n\n\nPlease note:\n If possible \nPLEASE USE THE OPERATOR\n to deploy to Kubernetes if you\ncan. It is a considerly more streamlined setup.\n\n\nThe \noperator\n leverages \ncustom resource definitions\n\n(CRDs) to automatically handle operations such as managing cluster topology.\n\n\nThe guide below provides static manifests to bootstrap a cluster on Kubernetes and should be considered\nas a guide to running M3 on Kubernetes, if and only if you have significant custom requirements not satisified by\nthe operator.\n\n\nPrerequisites\n\n\nM3DB performs better when it has access to fast disks. Every incoming write is written to a commit log, which at high\nvolumes of writes can be sensitive to spikes in disk latency. Additionally the random seeks into files when loading cold\nfiles benefit from lower random read latency.\n\n\nBecause of this, the included manifests reference a\n\nStorageClass\n named \nfast\n. Manifests are\nprovided to provide such a StorageClass on AWS / Azure / GCP using the respective cloud provider's premium disk class.\n\n\nIf you do not already have a StorageClass named \nfast\n, create one using one of the provided manifests:\n\n# AWS EBS (class io1)\nkubectl apply -f https://raw.githubusercontent.com/m3db/m3/master/kube/storage-fast-aws.yaml\n\n# Azure premium LRS\nkubectl apply -f https://raw.githubusercontent.com/m3db/m3/master/kube/storage-fast-azure.yaml\n\n# GCE Persistent SSD\nkubectl apply -f https://raw.githubusercontent.com/m3db/m3/master/kube/storage-fast-gcp.yaml\n\n\n\nIf you wish to use your cloud provider's default remote disk, or another disk class entirely, you'll have to modify them\nmanifests.\n\n\nIf your Kubernetes cluster spans multiple availability zones, it's important to specify a \nVolume Binding Mode\n of \nWaitForFirstConsumer\n in your StorageClass to delay the binding of the PersistentVolume until the Pod is created.\n\n\nKernel Configuration\n\n\nWe provide a Kubernetes daemonset that can make setting host-level sysctls easier. Please see the \nkernel\n docs\nfor more.\n\n\nNote that our default StatefulSet spec will give the M3DB container \nCAP_SYS_RESOURCE\n so it may raise its file limits.\nUncomment the \nsecurityContext\n on the \nm3db\n container in the StatefulSet if running with a Pod Security Policy or\nsimilar enforcement mechanism that prevents adding capabilities to containers.\n\n\nDeploying\n\n\nApply the following manifest to create your cluster:\n\nkubectl apply -f https://raw.githubusercontent.com/m3db/m3/master/kube/bundle.yaml\n\n\n\nApplying this bundle will create the following resources:\n\n\n\n\nAn \nm3db\n \nNamespace\n for\n   all M3DB-related resources.\n\n\nA 3-node etcd cluster in the form of a\n   \nStatefulSet\n backed by persistent\n   remote SSDs. This cluster stores the DB topology and other runtime configuration data.\n\n\nA 3-node M3DB cluster in the form of a StatefulSet.\n\n\nHeadless services\n for\n   the etcd and m3db StatefulSets to provide stable DNS hostnames per-pod.\n\n\n\n\nWait until all created pods are listed as ready:\n\n$ kubectl -n m3db get po\nNAME         READY     STATUS    RESTARTS   AGE\netcd-0       1/1       Running   0          22m\netcd-1       1/1       Running   0          22m\netcd-2       1/1       Running   0          22m\nm3dbnode-0   1/1       Running   0          22m\nm3dbnode-1   1/1       Running   0          22m\nm3dbnode-2   1/1       Running   0          22m\n\n\n\nYou can now proceed to initialize a namespace and placement for the cluster the same as you would for our other how-to\nguides:\n\n# Open a local connection to the coordinator service:\n$ kubectl -n m3db port-forward svc/m3coordinator 7201\nForwarding from 127.0.0.1:7201 -> 7201\nForwarding from [::1]:7201 -> 7201\n\n\n\n#\n \nCreate\n \nan\n \ninitial\n \ncluster\n \ntopology\n\n\ncurl\n \n-sSf\n \n-X\n \nPOST\n \nlocalhost:\n7201\n/api/v\n1\n/placement/init\n \n-d\n \n'\n{\n\n    \n\"num_shards\"\n:\n \n1024\n,\n\n    \n\"replication_factor\"\n:\n \n3\n,\n\n    \n\"instances\"\n:\n \n[\n\n        \n{\n\n            \n\"id\"\n:\n \n\"m3dbnode-0\"\n,\n\n            \n\"isolation_group\"\n:\n \n\"pod0\"\n,\n\n            \n\"zone\"\n:\n \n\"embedded\"\n,\n\n            \n\"weight\"\n:\n \n100\n,\n\n            \n\"endpoint\"\n:\n \n\"m3dbnode-0.m3dbnode:9000\"\n,\n\n            \n\"hostname\"\n:\n \n\"m3dbnode-0.m3dbnode\"\n,\n\n            \n\"port\"\n:\n \n9000\n\n        \n},\n\n        \n{\n\n            \n\"id\"\n:\n \n\"m3dbnode-1\"\n,\n\n            \n\"isolation_group\"\n:\n \n\"pod1\"\n,\n\n            \n\"zone\"\n:\n \n\"embedded\"\n,\n\n            \n\"weight\"\n:\n \n100\n,\n\n            \n\"endpoint\"\n:\n \n\"m3dbnode-1.m3dbnode:9000\"\n,\n\n            \n\"hostname\"\n:\n \n\"m3dbnode-1.m3dbnode\"\n,\n\n            \n\"port\"\n:\n \n9000\n\n        \n},\n\n        \n{\n\n            \n\"id\"\n:\n \n\"m3dbnode-2\"\n,\n\n            \n\"isolation_group\"\n:\n \n\"pod2\"\n,\n\n            \n\"zone\"\n:\n \n\"embedded\"\n,\n\n            \n\"weight\"\n:\n \n100\n,\n\n            \n\"endpoint\"\n:\n \n\"m3dbnode-2.m3dbnode:9000\"\n,\n\n            \n\"hostname\"\n:\n \n\"m3dbnode-2.m3dbnode\"\n,\n\n            \n\"port\"\n:\n \n9000\n\n        \n}\n\n    \n]\n\n\n}\n'\n\n\n\n\n\n#\n \nCreate\n \na\n \nnamespace\n \nto\n \nhold\n \nyour\n \nmetrics\n\n\ncurl\n \n-X\n \nPOST\n \nlocalhost:\n7201\n/api/v\n1\n/namespace\n \n-d\n \n'\n{\n\n  \n\"name\"\n:\n \n\"default\"\n,\n\n  \n\"options\"\n:\n \n{\n\n    \n\"bootstrapEnabled\"\n:\n \ntrue\n,\n\n    \n\"flushEnabled\"\n:\n \ntrue\n,\n\n    \n\"writesToCommitLog\"\n:\n \ntrue\n,\n\n    \n\"cleanupEnabled\"\n:\n \ntrue\n,\n\n    \n\"snapshotEnabled\"\n:\n \ntrue\n,\n\n    \n\"repairEnabled\"\n:\n \nfalse\n,\n\n    \n\"retentionOptions\"\n:\n \n{\n\n      \n\"retentionPeriodDuration\"\n:\n \n\"720h\"\n,\n\n      \n\"blockSizeDuration\"\n:\n \n\"12h\"\n,\n\n      \n\"bufferFutureDuration\"\n:\n \n\"1h\"\n,\n\n      \n\"bufferPastDuration\"\n:\n \n\"1h\"\n,\n\n      \n\"blockDataExpiry\"\n:\n \ntrue\n,\n\n      \n\"blockDataExpiryAfterNotAccessPeriodDuration\"\n:\n \n\"5m\"\n\n    \n},\n\n    \n\"indexOptions\"\n:\n \n{\n\n      \n\"enabled\"\n:\n \ntrue\n,\n\n      \n\"blockSizeDuration\"\n:\n \n\"12h\"\n\n    \n}\n\n  \n}\n\n\n}\n'\n\n\n\n\n\nShortly after you should see your nodes finish bootstrapping:\n\n$ kubectl -n m3db logs -f m3dbnode-0\n21:36:54.831698[I] cluster database initializing topology\n21:36:54.831732[I] cluster database resolving topology\n21:37:22.821740[I] resolving namespaces with namespace watch\n21:37:22.821813[I] updating database namespaces [{adds [metrics]} {updates []} {removals []}]\n21:37:23.008109[I] node tchannelthrift: listening on 0.0.0.0:9000\n21:37:23.008384[I] cluster tchannelthrift: listening on 0.0.0.0:9001\n21:37:23.217090[I] node httpjson: listening on 0.0.0.0:9002\n21:37:23.217240[I] cluster httpjson: listening on 0.0.0.0:9003\n21:37:23.217526[I] bootstrapping shards for range starting [{run bootstrap-data} {bootstrapper filesystem} ...\n...\n21:37:23.239534[I] bootstrap data fetched now initializing shards with series blocks [{namespace metrics} {numShards 256} {numSeries 0}]\n21:37:23.240778[I] bootstrap finished [{namespace metrics} {duration 23.325194ms}]\n21:37:23.240856[I] bootstrapped\n21:37:29.733025[I] successfully updated topology to 3 hosts\n\n\n\nYou can now write and read metrics using the API on the DB nodes:\n\n$ kubectl -n m3db port-forward svc/m3dbnode 9003\nForwarding from 127.0.0.1:9003 -> 9003\nForwarding from [::1]:9003 -> 9003\n\n\n\ncurl\n \n-sSf\n \n-X\n \nPOST\n \nlocalhost:\n9003\n/writetagged\n \n-d\n \n'\n{\n\n  \n\"namespace\"\n:\n \n\"default\"\n,\n\n  \n\"id\"\n:\n \n\"foo\"\n,\n\n  \n\"tags\"\n:\n \n[\n\n    \n{\n\n      \n\"name\"\n:\n \n\"city\"\n,\n\n      \n\"value\"\n:\n \n\"new_york\"\n\n    \n},\n\n    \n{\n\n      \n\"name\"\n:\n \n\"endpoint\"\n,\n\n      \n\"value\"\n:\n \n\"/request\"\n\n    \n}\n\n  \n],\n\n  \n\"datapoint\"\n:\n \n{\n\n    \n\"timestamp\"\n:\n \n'\n\"$(date \"\n+%s\n\")\"\n'\n,\n\n    \n\"value\"\n:\n \n42.123456789\n\n  \n}\n\n\n}\n'\n\n\n\n\n\n$\n \ncurl\n \n-sSf\n \n-X\n \nPOST\n \nhttp://localhost:\n9003\n/query\n \n-d\n \n'\n{\n\n  \n\"namespace\"\n:\n \n\"default\"\n,\n\n  \n\"query\"\n:\n \n{\n\n    \n\"regexp\"\n:\n \n{\n\n      \n\"field\"\n:\n \n\"city\"\n,\n\n      \n\"regexp\"\n:\n \n\".*\"\n\n    \n}\n\n  \n},\n\n  \n\"rangeStart\"\n:\n \n0\n,\n\n  \n\"rangeEnd\"\n:\n \n'\n\"$(date \"\n+%s\n\")\"\n'\n\n\n}\n'\n \n|\n \njq\n \n.\n\n\n\n{\n\n  \n\"results\"\n:\n \n[\n\n    \n{\n\n      \n\"id\"\n:\n \n\"foo\"\n,\n\n      \n\"tags\"\n:\n \n[\n\n        \n{\n\n          \n\"name\"\n:\n \n\"city\"\n,\n\n          \n\"value\"\n:\n \n\"new_york\"\n\n        \n},\n\n        \n{\n\n          \n\"name\"\n:\n \n\"endpoint\"\n,\n\n          \n\"value\"\n:\n \n\"/request\"\n\n        \n}\n\n      \n],\n\n      \n\"datapoints\"\n:\n \n[\n\n        \n{\n\n          \n\"timestamp\"\n:\n \n1527630053\n,\n\n          \n\"value\"\n:\n \n42.123456789\n\n        \n}\n\n      \n]\n\n    \n}\n\n  \n],\n\n  \n\"exhaustive\"\n:\n \ntrue\n\n\n}\n\n\n\n\n\nAdding nodes\n\n\nYou can easily scale your M3DB cluster by scaling the StatefulSet and informing the cluster topology of the change:\n\nkubectl -n m3db scale --replicas=4 statefulset/m3dbnode\n\n\n\nOnce the pod is ready you can modify the cluster topology:\n\nkubectl -n m3db port-forward svc/m3coordinator 7201\nForwarding from 127.0.0.1:7201 -> 7201\nForwarding from [::1]:7201 -> 7201\n\n\n\ncurl\n \n-sSf\n \n-X\n \nPOST\n \nlocalhost:\n7201\n/api/v\n1\n/placement\n \n-d\n \n'\n{\n\n    \n\"instances\"\n:\n \n[\n\n        \n{\n\n            \n\"id\"\n:\n \n\"m3dbnode-3\"\n,\n\n            \n\"isolation_group\"\n:\n \n\"pod3\"\n,\n\n            \n\"zone\"\n:\n \n\"embedded\"\n,\n\n            \n\"weight\"\n:\n \n100\n,\n\n            \n\"endpoint\"\n:\n \n\"m3dbnode-3.m3dbnode:9000\"\n,\n\n            \n\"hostname\"\n:\n \n\"m3dbnode-3.m3dbnode\"\n,\n\n            \n\"port\"\n:\n \n9000\n\n        \n}\n\n    \n]\n\n\n}\n'\n\n\n\n\n\nIntegrations\n\n\nPrometheus\n\n\nAs mentioned in our integrations \nguide\n, M3DB can be used as a \nremote read/write\nendpoint\n for Prometheus.\n\n\nIf you run Prometheus on your Kubernetes cluster you can easily point it at M3DB in your Prometheus server config:\n\n\nremote_read:\n  - url: \"http://m3coordinator.m3db.svc.cluster.local:7201/api/v1/prom/remote/read\"\n    # To test reading even when local Prometheus has the data\n    read_recent: true\n\nremote_write:\n  - url: \"http://m3coordinator.m3db.svc.cluster.local:7201/api/v1/prom/remote/write\"\n    # To differentiate between local and remote storage we will add a storage label\n    write_relabel_configs:\n      - target_label: metrics_storage\n        replacement: m3db_remote\n\n\n\n\nScheduling\n\n\nIn some cases, you might prefer M3DB to run on certain nodes in your cluster. For example: if your cluster is comprised\nof different instance types and some have more memory than others then you'd like M3DB to run on those nodes if\npossible. To accommodate this, the pods created by the StatefulSets use \npod\naffinities\n and\n\ntolerations\n to prefer to run on\ncertain nodes. Specifically:\n\n\n\n\nThe pods tolerate the taint \n\"dedicated-m3db\"\n to run on nodes that are specifically dedicated to m3db if you so\n   choose.\n\n\nVia \nnodeAffinity\n the pods prefer to run on nodes with the label \nm3db.io/dedicated-m3db=\"true\"\n.",
            "title": "M3DB on Kubernetes"
        },
        {
            "location": "/how_to/kubernetes/#m3db-on-kubernetes",
            "text": "Please note:  If possible  PLEASE USE THE OPERATOR  to deploy to Kubernetes if you\ncan. It is a considerly more streamlined setup.  The  operator  leverages  custom resource definitions \n(CRDs) to automatically handle operations such as managing cluster topology.  The guide below provides static manifests to bootstrap a cluster on Kubernetes and should be considered\nas a guide to running M3 on Kubernetes, if and only if you have significant custom requirements not satisified by\nthe operator.",
            "title": "M3DB on Kubernetes"
        },
        {
            "location": "/how_to/kubernetes/#prerequisites",
            "text": "M3DB performs better when it has access to fast disks. Every incoming write is written to a commit log, which at high\nvolumes of writes can be sensitive to spikes in disk latency. Additionally the random seeks into files when loading cold\nfiles benefit from lower random read latency.  Because of this, the included manifests reference a StorageClass  named  fast . Manifests are\nprovided to provide such a StorageClass on AWS / Azure / GCP using the respective cloud provider's premium disk class.  If you do not already have a StorageClass named  fast , create one using one of the provided manifests: # AWS EBS (class io1)\nkubectl apply -f https://raw.githubusercontent.com/m3db/m3/master/kube/storage-fast-aws.yaml\n\n# Azure premium LRS\nkubectl apply -f https://raw.githubusercontent.com/m3db/m3/master/kube/storage-fast-azure.yaml\n\n# GCE Persistent SSD\nkubectl apply -f https://raw.githubusercontent.com/m3db/m3/master/kube/storage-fast-gcp.yaml  If you wish to use your cloud provider's default remote disk, or another disk class entirely, you'll have to modify them\nmanifests.  If your Kubernetes cluster spans multiple availability zones, it's important to specify a  Volume Binding Mode  of  WaitForFirstConsumer  in your StorageClass to delay the binding of the PersistentVolume until the Pod is created.",
            "title": "Prerequisites"
        },
        {
            "location": "/how_to/kubernetes/#kernel-configuration",
            "text": "We provide a Kubernetes daemonset that can make setting host-level sysctls easier. Please see the  kernel  docs\nfor more.  Note that our default StatefulSet spec will give the M3DB container  CAP_SYS_RESOURCE  so it may raise its file limits.\nUncomment the  securityContext  on the  m3db  container in the StatefulSet if running with a Pod Security Policy or\nsimilar enforcement mechanism that prevents adding capabilities to containers.",
            "title": "Kernel Configuration"
        },
        {
            "location": "/how_to/kubernetes/#deploying",
            "text": "Apply the following manifest to create your cluster: kubectl apply -f https://raw.githubusercontent.com/m3db/m3/master/kube/bundle.yaml  Applying this bundle will create the following resources:   An  m3db   Namespace  for\n   all M3DB-related resources.  A 3-node etcd cluster in the form of a\n    StatefulSet  backed by persistent\n   remote SSDs. This cluster stores the DB topology and other runtime configuration data.  A 3-node M3DB cluster in the form of a StatefulSet.  Headless services  for\n   the etcd and m3db StatefulSets to provide stable DNS hostnames per-pod.   Wait until all created pods are listed as ready: $ kubectl -n m3db get po\nNAME         READY     STATUS    RESTARTS   AGE\netcd-0       1/1       Running   0          22m\netcd-1       1/1       Running   0          22m\netcd-2       1/1       Running   0          22m\nm3dbnode-0   1/1       Running   0          22m\nm3dbnode-1   1/1       Running   0          22m\nm3dbnode-2   1/1       Running   0          22m  You can now proceed to initialize a namespace and placement for the cluster the same as you would for our other how-to\nguides: # Open a local connection to the coordinator service:\n$ kubectl -n m3db port-forward svc/m3coordinator 7201\nForwarding from 127.0.0.1:7201 -> 7201\nForwarding from [::1]:7201 -> 7201  #   Create   an   initial   cluster   topology  curl   -sSf   -X   POST   localhost: 7201 /api/v 1 /placement/init   -d   ' { \n     \"num_shards\" :   1024 , \n     \"replication_factor\" :   3 , \n     \"instances\" :   [ \n         { \n             \"id\" :   \"m3dbnode-0\" , \n             \"isolation_group\" :   \"pod0\" , \n             \"zone\" :   \"embedded\" , \n             \"weight\" :   100 , \n             \"endpoint\" :   \"m3dbnode-0.m3dbnode:9000\" , \n             \"hostname\" :   \"m3dbnode-0.m3dbnode\" , \n             \"port\" :   9000 \n         }, \n         { \n             \"id\" :   \"m3dbnode-1\" , \n             \"isolation_group\" :   \"pod1\" , \n             \"zone\" :   \"embedded\" , \n             \"weight\" :   100 , \n             \"endpoint\" :   \"m3dbnode-1.m3dbnode:9000\" , \n             \"hostname\" :   \"m3dbnode-1.m3dbnode\" , \n             \"port\" :   9000 \n         }, \n         { \n             \"id\" :   \"m3dbnode-2\" , \n             \"isolation_group\" :   \"pod2\" , \n             \"zone\" :   \"embedded\" , \n             \"weight\" :   100 , \n             \"endpoint\" :   \"m3dbnode-2.m3dbnode:9000\" , \n             \"hostname\" :   \"m3dbnode-2.m3dbnode\" , \n             \"port\" :   9000 \n         } \n     ]  } '   #   Create   a   namespace   to   hold   your   metrics  curl   -X   POST   localhost: 7201 /api/v 1 /namespace   -d   ' { \n   \"name\" :   \"default\" , \n   \"options\" :   { \n     \"bootstrapEnabled\" :   true , \n     \"flushEnabled\" :   true , \n     \"writesToCommitLog\" :   true , \n     \"cleanupEnabled\" :   true , \n     \"snapshotEnabled\" :   true , \n     \"repairEnabled\" :   false , \n     \"retentionOptions\" :   { \n       \"retentionPeriodDuration\" :   \"720h\" , \n       \"blockSizeDuration\" :   \"12h\" , \n       \"bufferFutureDuration\" :   \"1h\" , \n       \"bufferPastDuration\" :   \"1h\" , \n       \"blockDataExpiry\" :   true , \n       \"blockDataExpiryAfterNotAccessPeriodDuration\" :   \"5m\" \n     }, \n     \"indexOptions\" :   { \n       \"enabled\" :   true , \n       \"blockSizeDuration\" :   \"12h\" \n     } \n   }  } '   Shortly after you should see your nodes finish bootstrapping: $ kubectl -n m3db logs -f m3dbnode-0\n21:36:54.831698[I] cluster database initializing topology\n21:36:54.831732[I] cluster database resolving topology\n21:37:22.821740[I] resolving namespaces with namespace watch\n21:37:22.821813[I] updating database namespaces [{adds [metrics]} {updates []} {removals []}]\n21:37:23.008109[I] node tchannelthrift: listening on 0.0.0.0:9000\n21:37:23.008384[I] cluster tchannelthrift: listening on 0.0.0.0:9001\n21:37:23.217090[I] node httpjson: listening on 0.0.0.0:9002\n21:37:23.217240[I] cluster httpjson: listening on 0.0.0.0:9003\n21:37:23.217526[I] bootstrapping shards for range starting [{run bootstrap-data} {bootstrapper filesystem} ...\n...\n21:37:23.239534[I] bootstrap data fetched now initializing shards with series blocks [{namespace metrics} {numShards 256} {numSeries 0}]\n21:37:23.240778[I] bootstrap finished [{namespace metrics} {duration 23.325194ms}]\n21:37:23.240856[I] bootstrapped\n21:37:29.733025[I] successfully updated topology to 3 hosts  You can now write and read metrics using the API on the DB nodes: $ kubectl -n m3db port-forward svc/m3dbnode 9003\nForwarding from 127.0.0.1:9003 -> 9003\nForwarding from [::1]:9003 -> 9003  curl   -sSf   -X   POST   localhost: 9003 /writetagged   -d   ' { \n   \"namespace\" :   \"default\" , \n   \"id\" :   \"foo\" , \n   \"tags\" :   [ \n     { \n       \"name\" :   \"city\" , \n       \"value\" :   \"new_york\" \n     }, \n     { \n       \"name\" :   \"endpoint\" , \n       \"value\" :   \"/request\" \n     } \n   ], \n   \"datapoint\" :   { \n     \"timestamp\" :   ' \"$(date \" +%s \")\" ' , \n     \"value\" :   42.123456789 \n   }  } '   $   curl   -sSf   -X   POST   http://localhost: 9003 /query   -d   ' { \n   \"namespace\" :   \"default\" , \n   \"query\" :   { \n     \"regexp\" :   { \n       \"field\" :   \"city\" , \n       \"regexp\" :   \".*\" \n     } \n   }, \n   \"rangeStart\" :   0 , \n   \"rangeEnd\" :   ' \"$(date \" +%s \")\" '  } '   |   jq   .  { \n   \"results\" :   [ \n     { \n       \"id\" :   \"foo\" , \n       \"tags\" :   [ \n         { \n           \"name\" :   \"city\" , \n           \"value\" :   \"new_york\" \n         }, \n         { \n           \"name\" :   \"endpoint\" , \n           \"value\" :   \"/request\" \n         } \n       ], \n       \"datapoints\" :   [ \n         { \n           \"timestamp\" :   1527630053 , \n           \"value\" :   42.123456789 \n         } \n       ] \n     } \n   ], \n   \"exhaustive\" :   true  }",
            "title": "Deploying"
        },
        {
            "location": "/how_to/kubernetes/#adding-nodes",
            "text": "You can easily scale your M3DB cluster by scaling the StatefulSet and informing the cluster topology of the change: kubectl -n m3db scale --replicas=4 statefulset/m3dbnode  Once the pod is ready you can modify the cluster topology: kubectl -n m3db port-forward svc/m3coordinator 7201\nForwarding from 127.0.0.1:7201 -> 7201\nForwarding from [::1]:7201 -> 7201  curl   -sSf   -X   POST   localhost: 7201 /api/v 1 /placement   -d   ' { \n     \"instances\" :   [ \n         { \n             \"id\" :   \"m3dbnode-3\" , \n             \"isolation_group\" :   \"pod3\" , \n             \"zone\" :   \"embedded\" , \n             \"weight\" :   100 , \n             \"endpoint\" :   \"m3dbnode-3.m3dbnode:9000\" , \n             \"hostname\" :   \"m3dbnode-3.m3dbnode\" , \n             \"port\" :   9000 \n         } \n     ]  } '",
            "title": "Adding nodes"
        },
        {
            "location": "/how_to/kubernetes/#integrations",
            "text": "",
            "title": "Integrations"
        },
        {
            "location": "/how_to/kubernetes/#prometheus",
            "text": "As mentioned in our integrations  guide , M3DB can be used as a  remote read/write\nendpoint  for Prometheus.  If you run Prometheus on your Kubernetes cluster you can easily point it at M3DB in your Prometheus server config:  remote_read:\n  - url: \"http://m3coordinator.m3db.svc.cluster.local:7201/api/v1/prom/remote/read\"\n    # To test reading even when local Prometheus has the data\n    read_recent: true\n\nremote_write:\n  - url: \"http://m3coordinator.m3db.svc.cluster.local:7201/api/v1/prom/remote/write\"\n    # To differentiate between local and remote storage we will add a storage label\n    write_relabel_configs:\n      - target_label: metrics_storage\n        replacement: m3db_remote",
            "title": "Prometheus"
        },
        {
            "location": "/how_to/kubernetes/#scheduling",
            "text": "In some cases, you might prefer M3DB to run on certain nodes in your cluster. For example: if your cluster is comprised\nof different instance types and some have more memory than others then you'd like M3DB to run on those nodes if\npossible. To accommodate this, the pods created by the StatefulSets use  pod\naffinities  and tolerations  to prefer to run on\ncertain nodes. Specifically:   The pods tolerate the taint  \"dedicated-m3db\"  to run on nodes that are specifically dedicated to m3db if you so\n   choose.  Via  nodeAffinity  the pods prefer to run on nodes with the label  m3db.io/dedicated-m3db=\"true\" .",
            "title": "Scheduling"
        },
        {
            "location": "/how_to/query/",
            "text": "Setting up m3query\n\n\nIntroduction\n\n\nm3query is used to query data that is stored in M3DB. For instance, if you are using the Prometheus remote write endpoint with \nm3coordinator\n, you can use m3query instead of the Prometheus remote read endpoint. By doing so, you get all of the benefits of m3query's engine such as \nblock processing\n. Furthermore, since m3query provides a Prometheus compatible API, you can use 3\nrd\n party graphing and alerting solutions like Grafana.\n\n\nConfiguration\n\n\nBefore setting up m3query, make sure that you have at least \none M3DB node running\n. In order to start m3query, you need to configure a \nyaml\n file, that will be used to connect to M3DB. Here is a link to a \nsample config\n file that is used for an embedded etcd cluster within M3DB.\n\n\nRunning\n\n\nYou can run m3query by either building and running the binary yourself:\n\n\nmake m3query\n./bin/m3query -f ./src/query/config/m3query-local-etcd.yml\n\n\n\n\nOr you can run it with Docker using the Docker file located at \n$GOPATH/src/github.com/m3db/m3/docker/m3query/Dockerfile\n.\n\n\nNamespaces\n\n\nAll namespaces that you wish to query from must be configured when \nsetting up M3DB\n. If you wish to add or change an existing namespace, please follow the namespace operational guide \nhere\n.\n\n\netcd\n\n\nThe configuration file linked above uses an embedded etcd cluster, which is fine for development purposes. However, if you wish to use this in production, you will want an \nexternal etcd\n cluster.\n\n\n\n\n\nAggregation\n\n\nYou will notice that in the setup linked above, M3DB has just one unaggregated namespace configured. If you want aggregated metrics, you will need to set up an aggregated namespace in M3DB \nand\n in the m3query configuration. It is important to note that all writes go to all namespaces so as long as you include all namespaces in your query config, you will be querying all namespaces. Aggregation is done strictly by the query service. For example if you have an aggregated namespace setup in M3DB named \nmetrics_10s_48h\n, you can add the following to the query config:\n\n\n-\n \nnamespace\n:\n \nmetrics_10s_48h\n\n  \ntype\n:\n \naggregated\n\n  \nretention\n:\n \n48h\n\n  \nresolution\n:\n \n10s\n\n\n\n\n\nDisabling automatic aggregation\n\n\nIf you run Statsite, m3agg, or some other aggregation tier, you will want to set the \nall\n flag under \ndownsample\n to \nfalse\n. Otherwise, you will be aggregating metrics that have already been aggregated.\n\n\n-\n \nnamespace\n:\n \nmetrics_10s_48h\n\n  \ntype\n:\n \naggregated\n\n  \nretention\n:\n \n48h\n\n  \nresolution\n:\n \n10s\n\n  \ndownsample\n:\n\n    \nall\n:\n \nfalse\n\n\n\n\n\nID generation\n\n\nThe default generation scheme for IDs, \nlegacy\n, is unfortunately prone to collisions, but remains the default for backwards compatibility reasons. It is suggested to set the ID generation scheme to one of either \nquoted\n or \nprepend_meta\n. \nquoted\n generation scheme yields the most human-readable IDs, whereas \nprepend_meta\n is better for more compact IDs, or if tags are expected to contain non-ASCII characters. To set the ID generation scheme, add the following to your m3coordinator configuration yaml file:\n\n\ntagOptions\n:\n\n  \nidScheme\n:\n \n<name>\n\n\n\n\n\nAs an example of how these schemes generate IDs, consider a series with the following 4 tags,\n\n[{\"t1\":v1}, {t2:\"v2\"}, {t3:v3}, {t4:v4}]\n. The following is an example of how different schemes will generate IDs.\n\n\nlegacy: \"t1\"=v1,t2=\"v2\",t3=v3,t4=v4,\nprepend_meta: 4,2,2,4,2,2,2,2!\"t1\"v1t2\"v2\"t3v3t4v4\nquoted: {\\\"t1\\\"=\"v1\",t2=\"\\\"v2\\\"\",t3=\"v3\",t4=\"v4\"}\n\n\n\n\nIf there is a chance that your metric tags will contain \"control\" characters, specifically \n,\n and \n=\n, it is highly recommended that one of either the \nquoted\n or \nprepend_meta\n schemes are specified, as the \nlegacy\n scheme may cause ID collisions. As a general guideline, we suggest \nquoted\n, as it mirrors the more familiar Prometheus style IDs.\n\n\nWe technically have a fourth ID generation scheme that is used for Graphite IDs, but it is exclusive to the Graphite ingestion path and is not selectable as a general scheme.\n\n\nWARNING:\n Once a scheme is selected, be very careful about changing it. If changed, all incoming metrics will resolve to a new ID, effectively doubling the metric cardinality until all of the older-style metric IDs fall out of retention.\n\n\nMigration\n\n\nWe recently updated our ID generation scheme in m3coordinator to avoid the collision issues discussed above. To ease migration, we're temporarily enforcing that an ID generation scheme be explicitly provided in the m3coordinator configuration files.\n\n\nIf you have been running m3query or m3coordinator already, you may want to counterintuitively select the collision-prone \nlegacy\n scheme, as all the IDs for all of your current metrics would have already been generated with this scheme, and choosing another will effectively double your index size. If the twofold increase in cardinality is an acceptable increase (and unfortunately, this is likely to mean doubled cardinality until your longest retention cluster rotates out), it's suggested to choose a collision-resistant scheme instead.\n\n\nAn example of a configuration file for a standalone m3query instance with the ID generation scheme can be found \nhere\n. If you're running m3query or m3coordinator embedded, these configuration options should be nested under the \ncoordinator:\n heading, as seen \nhere\n.\n\n\nIf none of these options work for you, or you would like further clarification, please stop by our \nSlack\n and we'll be happy to help you.\n\n\nGrafana\n\n\nYou can also set up m3query as a \ndatasource in Grafana\n. To do this, add a new datasource with a type of \nPrometheus\n. The URL should point to the host/port running m3query. By default, m3query runs on port \n7201\n.",
            "title": "M3Query"
        },
        {
            "location": "/how_to/query/#setting-up-m3query",
            "text": "",
            "title": "Setting up m3query"
        },
        {
            "location": "/how_to/query/#introduction",
            "text": "m3query is used to query data that is stored in M3DB. For instance, if you are using the Prometheus remote write endpoint with  m3coordinator , you can use m3query instead of the Prometheus remote read endpoint. By doing so, you get all of the benefits of m3query's engine such as  block processing . Furthermore, since m3query provides a Prometheus compatible API, you can use 3 rd  party graphing and alerting solutions like Grafana.",
            "title": "Introduction"
        },
        {
            "location": "/how_to/query/#configuration",
            "text": "Before setting up m3query, make sure that you have at least  one M3DB node running . In order to start m3query, you need to configure a  yaml  file, that will be used to connect to M3DB. Here is a link to a  sample config  file that is used for an embedded etcd cluster within M3DB.",
            "title": "Configuration"
        },
        {
            "location": "/how_to/query/#running",
            "text": "You can run m3query by either building and running the binary yourself:  make m3query\n./bin/m3query -f ./src/query/config/m3query-local-etcd.yml  Or you can run it with Docker using the Docker file located at  $GOPATH/src/github.com/m3db/m3/docker/m3query/Dockerfile .",
            "title": "Running"
        },
        {
            "location": "/how_to/query/#namespaces",
            "text": "All namespaces that you wish to query from must be configured when  setting up M3DB . If you wish to add or change an existing namespace, please follow the namespace operational guide  here .",
            "title": "Namespaces"
        },
        {
            "location": "/how_to/query/#etcd",
            "text": "The configuration file linked above uses an embedded etcd cluster, which is fine for development purposes. However, if you wish to use this in production, you will want an  external etcd  cluster.",
            "title": "etcd"
        },
        {
            "location": "/how_to/query/#aggregation",
            "text": "You will notice that in the setup linked above, M3DB has just one unaggregated namespace configured. If you want aggregated metrics, you will need to set up an aggregated namespace in M3DB  and  in the m3query configuration. It is important to note that all writes go to all namespaces so as long as you include all namespaces in your query config, you will be querying all namespaces. Aggregation is done strictly by the query service. For example if you have an aggregated namespace setup in M3DB named  metrics_10s_48h , you can add the following to the query config:  -   namespace :   metrics_10s_48h \n   type :   aggregated \n   retention :   48h \n   resolution :   10s",
            "title": "Aggregation"
        },
        {
            "location": "/how_to/query/#disabling-automatic-aggregation",
            "text": "If you run Statsite, m3agg, or some other aggregation tier, you will want to set the  all  flag under  downsample  to  false . Otherwise, you will be aggregating metrics that have already been aggregated.  -   namespace :   metrics_10s_48h \n   type :   aggregated \n   retention :   48h \n   resolution :   10s \n   downsample : \n     all :   false",
            "title": "Disabling automatic aggregation"
        },
        {
            "location": "/how_to/query/#id-generation",
            "text": "The default generation scheme for IDs,  legacy , is unfortunately prone to collisions, but remains the default for backwards compatibility reasons. It is suggested to set the ID generation scheme to one of either  quoted  or  prepend_meta .  quoted  generation scheme yields the most human-readable IDs, whereas  prepend_meta  is better for more compact IDs, or if tags are expected to contain non-ASCII characters. To set the ID generation scheme, add the following to your m3coordinator configuration yaml file:  tagOptions : \n   idScheme :   <name>   As an example of how these schemes generate IDs, consider a series with the following 4 tags, [{\"t1\":v1}, {t2:\"v2\"}, {t3:v3}, {t4:v4}] . The following is an example of how different schemes will generate IDs.  legacy: \"t1\"=v1,t2=\"v2\",t3=v3,t4=v4,\nprepend_meta: 4,2,2,4,2,2,2,2!\"t1\"v1t2\"v2\"t3v3t4v4\nquoted: {\\\"t1\\\"=\"v1\",t2=\"\\\"v2\\\"\",t3=\"v3\",t4=\"v4\"}  If there is a chance that your metric tags will contain \"control\" characters, specifically  ,  and  = , it is highly recommended that one of either the  quoted  or  prepend_meta  schemes are specified, as the  legacy  scheme may cause ID collisions. As a general guideline, we suggest  quoted , as it mirrors the more familiar Prometheus style IDs.  We technically have a fourth ID generation scheme that is used for Graphite IDs, but it is exclusive to the Graphite ingestion path and is not selectable as a general scheme.  WARNING:  Once a scheme is selected, be very careful about changing it. If changed, all incoming metrics will resolve to a new ID, effectively doubling the metric cardinality until all of the older-style metric IDs fall out of retention.",
            "title": "ID generation"
        },
        {
            "location": "/how_to/query/#migration",
            "text": "We recently updated our ID generation scheme in m3coordinator to avoid the collision issues discussed above. To ease migration, we're temporarily enforcing that an ID generation scheme be explicitly provided in the m3coordinator configuration files.  If you have been running m3query or m3coordinator already, you may want to counterintuitively select the collision-prone  legacy  scheme, as all the IDs for all of your current metrics would have already been generated with this scheme, and choosing another will effectively double your index size. If the twofold increase in cardinality is an acceptable increase (and unfortunately, this is likely to mean doubled cardinality until your longest retention cluster rotates out), it's suggested to choose a collision-resistant scheme instead.  An example of a configuration file for a standalone m3query instance with the ID generation scheme can be found  here . If you're running m3query or m3coordinator embedded, these configuration options should be nested under the  coordinator:  heading, as seen  here .  If none of these options work for you, or you would like further clarification, please stop by our  Slack  and we'll be happy to help you.",
            "title": "Migration"
        },
        {
            "location": "/how_to/query/#grafana",
            "text": "You can also set up m3query as a  datasource in Grafana . To do this, add a new datasource with a type of  Prometheus . The URL should point to the host/port running m3query. By default, m3query runs on port  7201 .",
            "title": "Grafana"
        },
        {
            "location": "/how_to/aggregator/",
            "text": "Setting up M3 Aggregator\n\n\nIntroduction\n\n\nm3aggregator\n is used to cluster stateful downsampling and rollup of metrics before they are store in M3DB. The M3 Coordinator also performs this role but is not cluster aware. This means metrics will not get aggregated properly if you send metrics in round robin fashion to multiple M3 Coordinators for the same metrics ingestion source (e.g. Prometheus server).\n\n\nSimilar to M3DB, \nm3aggregator\n supports clustering and replication by default. This means that metrics are correctly routed to the instance(s) responsible for aggregating each metric and multiple \nm3aggregator\n replicas can be configured such that there are no single points of failure for aggregation.\n\n\nConfiguration\n\n\nBefore setting up m3aggregator, make sure that you have at least \none M3DB node running\n and a dedicated m3coordinator setup.\n\n\nWe highly recommend running with at least a replication factor 2 for a \nm3aggregator\n deployment. If you run with replication factor 1 then when you restart an aggregator it will temporarily interrupt good the stream of aggregated metrics and there will be some data loss.\n\n\nTopology\n\n\nInitializing aggregator topology\n\n\nYou can setup a m3aggregator topology by issuing a request to your coordinator (be sure to use your own hostnames, number of shards and replication factor):\n\ncurl -vvvsSf -H \n\"Cluster-Environment-Name: namespace/m3db-cluster-name\"\n -X POST http://m3dbnode-with-embedded-coordinator:7201/api/v1/services/m3aggregator/placement/init -d \n'{\n\n\n    \"num_shards\": 64,\n\n\n    \"replication_factor\": 2,\n\n\n    \"instances\": [\n\n\n        {\n\n\n            \"id\": \"m3aggregator01:6000\",\n\n\n            \"isolation_group\": \"availability-zone-a\",\n\n\n            \"zone\": \"embedded\",\n\n\n            \"weight\": 100,\n\n\n            \"endpoint\": \"m3aggregator01:6000\",\n\n\n            \"hostname\": \"m3aggregator01\",\n\n\n            \"port\": 6000\n\n\n        },\n\n\n        {\n\n\n            \"id\": \"m3aggregator02:6000\",\n\n\n            \"isolation_group\": \"availability-zone-b\",\n\n\n            \"zone\": \"embedded\",\n\n\n            \"weight\": 100,\n\n\n            \"endpoint\": \"m3aggregator02:6000\",\n\n\n            \"hostname\": \"m3aggregator02\",\n\n\n            \"port\": 6000\n\n\n        }\n\n\n    ]\n\n\n}'\n\n\n\n\nInitializing m3msg topic for m3aggregator to receive from m3coordinators to aggregate metrics\n\n\nNow we must setup a topic for the \nm3aggregator\n to receive unaggregated metrics from \nm3coordinator\n instances:\n\n\ncurl -vvvsSf -H \n\"Cluster-Environment-Name: namespace/m3db-cluster-name\"\n -H \n\"Topic-Name: aggregator_ingest\"\n -X POST http://m3dbnode-with-embedded-coordinator:7201/api/v1/topic/init -d \n'{\n\n\n    \"numberOfShards\": 64\n\n\n}'\n\n\n\n\n\nAdd m3aggregagtor consumer group to ingest topic\n\n\nAdd the \nm3aggregator\n placement to receive traffic from the topic (make sure to set message TTL to match your desired maximum in memory retry message buffer):\n\ncurl -vvvsSf -H \n\"Cluster-Environment-Name: namespace/m3db-cluster-name\"\n -H \n\"Topic-Name: aggregator_ingest\"\n -X POST http://m3dbnode-with-embedded-coordinator:7201/api/v1/topic -d \n'{\n\n\n  \"consumerService\": {\n\n\n    \"serviceId\": {\n\n\n      \"name\": \"m3aggregator\",\n\n\n      \"environment\": \"namespace/m3db-cluster-name\",\n\n\n      \"zone\": \"embedded\"\n\n\n    },\n\n\n    \"consumptionType\": \"REPLICATED\",\n\n\n    \"messageTtlNanos\": \"300000000000\"\n\n\n  }\n\n\n}'\n\n\n\n\nNote:\n 300000000000 nanoseconds is a TTL of 5 minutes for messages to rebuffer for retry.\n\n\nInitializing m3msg topic for m3coordinator to receive from m3aggregator to write to M3DB\n\n\nNow we must setup a topic for the \nm3coordinator\n to receive aggregated metrics from \nm3aggregator\n instances to write to M3DB:\n\ncurl -vvvsSf -H \n\"Cluster-Environment-Name: namespace/m3db-cluster-name\"\n -H \n\"Topic-Name: aggregated_metrics\"\n -X POST http://m3dbnode-with-embedded-coordinator:7201/api/v1/topic/init -d \n'{\n\n\n    \"numberOfShards\": 64\n\n\n}'\n\n\n\n\nInitializing m3coordinator topology\n\n\nThen \nm3coordinator\n instances need to be configured to receive traffic for this topic (note ingest at port 7507 must match the configured port for your \nm3coordinator\n ingest server, see config at bottom of this guide):\n\ncurl -vvvsSf -H \n\"Cluster-Environment-Name: namespace/m3db-cluster-name\"\n -X POST http://m3dbnode-with-embedded-coordinator:7201/api/v1/services/m3coordinator/placement/init -d \n'{\n\n\n    \"instances\": [\n\n\n        {\n\n\n            \"id\": \"m3coordinator01\",\n\n\n            \"zone\": \"embedded\",\n\n\n            \"endpoint\": \"m3coordinator01:7507\",\n\n\n            \"hostname\": \"m3coordinator01\",\n\n\n            \"port\": 7507\n\n\n        }\n\n\n    ]\n\n\n}'\n\n\n\n\nNote:\n When you add or remove \nm3coordinator\n instances they must be added to this placement.\n\n\nAdd m3coordinator consumer group to outbound topic\n\n\nAdd the \nm3coordinator\n placement to receive traffic from the topic (make sure to set message TTL to match your desired maximum in memory retry message buffer):\n\ncurl -vvvsSf -H \n\"Cluster-Environment-Name: namespace/m3db-cluster-name\"\n -H \n\"Topic-Name: aggregated_metrics\"\n -X POST http://m3dbnode-with-embedded-coordinator:7201/api/v1/topic -d \n'{\n\n\n  \"consumerService\": {\n\n\n    \"serviceId\": {\n\n\n      \"name\": \"m3coordinator\",\n\n\n      \"environment\": \"namespace/m3db-cluster-name\",\n\n\n      \"zone\": \"embedded\"\n\n\n    },\n\n\n    \"consumptionType\": \"SHARED\",\n\n\n    \"messageTtlNanos\": \"300000000000\"\n\n\n  }\n\n\n}'\n\n\n\n\nNote:\n 300000000000 nanoseconds is a TTL of 5 minutes for messages to rebuffer for retry.\n\n\nRunning\n\n\nDedicated Coordinator\n\n\nMetrics will still arrive at the \nm3coordinator\n, they simply need to be forwarded to an \nm3aggregator\n. The \nm3coordinator\n then also needs to receive metrics that have been aggregated from the \nm3aggregator\n and store them in M3DB, so running an ingestion server should be configured.\n\n\nHere is the config you should add to your \nm3coordinator\n:\n\n# This is for sending metrics to the remote m3aggregators\n\n\ndownsample\n:\n\n  \nremoteAggregator\n:\n\n    \nclient\n:\n\n      \ntype\n:\n \nm3msg\n\n      \nm3msg\n:\n\n        \nproducer\n:\n\n          \nwriter\n:\n\n            \ntopicName\n:\n \naggregator_ingest\n\n            \ntopicServiceOverride\n:\n\n              \nzone\n:\n \nembedded\n\n              \nenvironment\n:\n \nnamespace/m3db-cluster-name\n\n            \nplacement\n:\n\n              \nisStaged\n:\n \ntrue\n\n            \nplacementServiceOverride\n:\n\n              \nnamespaces\n:\n\n                \nplacement\n:\n \n/placement\n\n            \nconnection\n:\n\n              \nnumConnections\n:\n \n4\n\n            \nmessagePool\n:\n\n              \nsize\n:\n \n16384\n\n              \nwatermark\n:\n\n                \nlow\n:\n \n0.2\n\n                \nhigh\n:\n \n0.5\n\n\n\n# This is for configuring the ingestion server that will receive metrics from the m3aggregators on port 7507\n\n\ningest\n:\n\n  \ningester\n:\n\n    \nworkerPoolSize\n:\n \n10000\n\n    \nopPool\n:\n\n      \nsize\n:\n \n10000\n\n    \nretry\n:\n\n      \nmaxRetries\n:\n \n3\n\n      \njitter\n:\n \ntrue\n\n    \nlogSampleRate\n:\n \n0.01\n\n  \nm3msg\n:\n\n    \nserver\n:\n\n      \nlistenAddress\n:\n \n\"0.0.0.0:7507\"\n\n      \nretry\n:\n\n        \nmaxBackoff\n:\n \n10s\n\n        \njitter\n:\n \ntrue\n\n\n\n\nM3 Aggregator\n\n\nYou can run \nm3aggregator\n by either building and running the binary yourself:\n\n\nmake m3aggregator\n./bin/m3aggregator -f ./src/aggregator/config/m3aggregator.yml\n\n\n\n\nOr you can run it with Docker using the Docker file located at \ndocker/m3aggregator/Dockerfile\n or the publicly provided image \nquay.io/m3db/m3aggregator:latest\n.\n\n\nYou can use a config like so, making note of the topics used such as \naggregator_ingest\n and \naggregated_metrics\n and the corresponding environment \nnamespace/m3db-cluster-name\n:\n\n\nlogging\n:\n\n  \nlevel\n:\n \ninfo\n\n\n\nmetrics\n:\n\n  \nscope\n:\n\n    \nprefix\n:\n \nm3aggregator\n\n  \nprometheus\n:\n\n    \nonError\n:\n \nnone\n\n    \nhandlerPath\n:\n \n/metrics\n\n    \nlistenAddress\n:\n \n0.0.0.0:6002\n\n    \ntimerType\n:\n \nhistogram\n\n  \nsanitization\n:\n \nprometheus\n\n  \nsamplingRate\n:\n \n1.0\n\n  \nextended\n:\n \nnone\n\n\n\nm3msg\n:\n\n  \nserver\n:\n\n    \nlistenAddress\n:\n \n0.0.0.0:6000\n\n    \nretry\n:\n\n      \nmaxBackoff\n:\n \n10s\n\n      \njitter\n:\n \ntrue\n\n  \nconsumer\n:\n\n    \nmessagePool\n:\n\n      \nsize\n:\n \n16384\n\n      \nwatermark\n:\n\n        \nlow\n:\n \n0.2\n\n        \nhigh\n:\n \n0.5\n\n\n\nhttp\n:\n\n  \nlistenAddress\n:\n \n0.0.0.0:6001\n\n  \nreadTimeout\n:\n \n60s\n\n  \nwriteTimeout\n:\n \n60s\n\n\n\nkvClient\n:\n\n  \netcd\n:\n\n    \nenv\n:\n \nnamespace/m3db-cluster-name\n\n    \nzone\n:\n \nembedded\n\n    \nservice\n:\n \nm3aggregator\n\n    \ncacheDir\n:\n \n/var/lib/m3kv\n\n    \netcdClusters\n:\n\n      \n-\n \nzone\n:\n \nembedded\n\n        \nendpoints\n:\n\n          \n-\n \ndbnode01:2379\n\n\n\nruntimeOptions\n:\n\n  \nkvConfig\n:\n\n    \nenvironment\n:\n \nnamespace/m3db-cluster-name\n\n    \nzone\n:\n \nembedded\n\n  \nwriteValuesPerMetricLimitPerSecondKey\n:\n \nwrite-values-per-metric-limit-per-second\n\n  \nwriteValuesPerMetricLimitPerSecond\n:\n \n0\n\n  \nwriteNewMetricLimitClusterPerSecondKey\n:\n \nwrite-new-metric-limit-cluster-per-second\n\n  \nwriteNewMetricLimitClusterPerSecond\n:\n \n0\n\n  \nwriteNewMetricNoLimitWarmupDuration\n:\n \n0\n\n\n\naggregator\n:\n\n  \nhostID\n:\n\n    \nresolver\n:\n \nenvironment\n\n    \nenvVarName\n:\n \nM3AGGREGATOR_HOST_ID\n\n  \ninstanceID\n:\n\n    \ntype\n:\n \nhost_id\n\n  \nverboseErrors\n:\n \ntrue\n\n  \nmetricPrefix\n:\n \n\"\"\n\n  \ncounterPrefix\n:\n \n\"\"\n\n  \ntimerPrefix\n:\n \n\"\"\n\n  \ngaugePrefix\n:\n \n\"\"\n\n  \naggregationTypes\n:\n\n    \ncounterTransformFnType\n:\n \nempty\n\n    \ntimerTransformFnType\n:\n \nsuffix\n\n    \ngaugeTransformFnType\n:\n \nempty\n\n    \naggregationTypesPool\n:\n\n      \nsize\n:\n \n1024\n\n    \nquantilesPool\n:\n\n      \nbuckets\n:\n\n        \n-\n \ncount\n:\n \n256\n\n          \ncapacity\n:\n \n4\n\n        \n-\n \ncount\n:\n \n128\n\n          \ncapacity\n:\n \n8\n\n  \nstream\n:\n\n    \neps\n:\n \n0.001\n\n    \ncapacity\n:\n \n32\n\n    \nstreamPool\n:\n\n      \nsize\n:\n \n4096\n\n    \nsamplePool\n:\n\n      \nsize\n:\n \n4096\n\n    \nfloatsPool\n:\n\n      \nbuckets\n:\n\n        \n-\n \ncount\n:\n \n4096\n\n          \ncapacity\n:\n \n16\n\n        \n-\n \ncount\n:\n \n2048\n\n          \ncapacity\n:\n \n32\n\n        \n-\n \ncount\n:\n \n1024\n\n          \ncapacity\n:\n \n64\n\n  \nclient\n:\n\n    \ntype\n:\n \nm3msg\n\n    \nm3msg\n:\n\n      \nproducer\n:\n\n        \nwriter\n:\n\n          \ntopicName\n:\n \naggregator_ingest\n\n          \ntopicServiceOverride\n:\n\n            \nzone\n:\n \nembedded\n\n            \nenvironment\n:\n \nnamespace/m3db-cluster-name\n\n          \nplacement\n:\n\n            \nisStaged\n:\n \ntrue\n\n          \nplacementServiceOverride\n:\n\n            \nnamespaces\n:\n\n              \nplacement\n:\n \n/placement\n\n          \nmessagePool\n:\n\n            \nsize\n:\n \n16384\n\n            \nwatermark\n:\n\n              \nlow\n:\n \n0.2\n\n              \nhigh\n:\n \n0.5\n\n  \nplacementManager\n:\n\n    \nkvConfig\n:\n\n      \nnamespace\n:\n \n/placement\n\n      \nenvironment\n:\n \nnamespace/m3db-cluster-name\n\n      \nzone\n:\n \nembedded\n\n    \nplacementWatcher\n:\n\n      \nkey\n:\n \nm3aggregator\n\n      \ninitWatchTimeout\n:\n \n10s\n\n  \nhashType\n:\n \nmurmur32\n\n  \nbufferDurationBeforeShardCutover\n:\n \n10m\n\n  \nbufferDurationAfterShardCutoff\n:\n \n10m\n\n  \nbufferDurationForFutureTimedMetric\n:\n \n10m\n \n# Allow test to write into future.\n\n  \nresignTimeout\n:\n \n1m\n\n  \nflushTimesManager\n:\n\n    \nkvConfig\n:\n\n      \nenvironment\n:\n \nnamespace/m3db-cluster-name\n\n      \nzone\n:\n \nembedded\n\n    \nflushTimesKeyFmt\n:\n \nshardset/%d/flush\n\n    \nflushTimesPersistRetrier\n:\n\n      \ninitialBackoff\n:\n \n100ms\n\n      \nbackoffFactor\n:\n \n2.0\n\n      \nmaxBackoff\n:\n \n2s\n\n      \nmaxRetries\n:\n \n3\n\n  \nelectionManager\n:\n\n    \nelection\n:\n\n      \nleaderTimeout\n:\n \n10s\n\n      \nresignTimeout\n:\n \n10s\n\n      \nttlSeconds\n:\n \n10\n\n    \nserviceID\n:\n\n      \nname\n:\n \nm3aggregator\n\n      \nenvironment\n:\n \nnamespace/m3db-cluster-name\n\n      \nzone\n:\n \nembedded\n\n    \nelectionKeyFmt\n:\n \nshardset/%d/lock\n\n    \ncampaignRetrier\n:\n\n      \ninitialBackoff\n:\n \n100ms\n\n      \nbackoffFactor\n:\n \n2.0\n\n      \nmaxBackoff\n:\n \n2s\n\n      \nforever\n:\n \ntrue\n\n      \njitter\n:\n \ntrue\n\n    \nchangeRetrier\n:\n\n      \ninitialBackoff\n:\n \n100ms\n\n      \nbackoffFactor\n:\n \n2.0\n\n      \nmaxBackoff\n:\n \n5s\n\n      \nforever\n:\n \ntrue\n\n      \njitter\n:\n \ntrue\n\n    \nresignRetrier\n:\n\n      \ninitialBackoff\n:\n \n100ms\n\n      \nbackoffFactor\n:\n \n2.0\n\n      \nmaxBackoff\n:\n \n5s\n\n      \nforever\n:\n \ntrue\n\n      \njitter\n:\n \ntrue\n\n    \ncampaignStateCheckInterval\n:\n \n1s\n\n    \nshardCutoffCheckOffset\n:\n \n30s\n\n  \nflushManager\n:\n\n    \ncheckEvery\n:\n \n1s\n\n    \njitterEnabled\n:\n \ntrue\n\n    \nmaxJitters\n:\n\n      \n-\n \nflushInterval\n:\n \n5s\n\n        \nmaxJitterPercent\n:\n \n1.0\n\n      \n-\n \nflushInterval\n:\n \n10s\n\n        \nmaxJitterPercent\n:\n \n0.5\n\n      \n-\n \nflushInterval\n:\n \n1m\n\n        \nmaxJitterPercent\n:\n \n0.5\n\n      \n-\n \nflushInterval\n:\n \n10m\n\n        \nmaxJitterPercent\n:\n \n0.5\n\n      \n-\n \nflushInterval\n:\n \n1h\n\n        \nmaxJitterPercent\n:\n \n0.25\n\n    \nnumWorkersPerCPU\n:\n \n0.5\n\n    \nflushTimesPersistEvery\n:\n \n10s\n\n    \nmaxBufferSize\n:\n \n5m\n\n    \nforcedFlushWindowSize\n:\n \n10s\n\n  \nflush\n:\n\n    \nhandlers\n:\n\n      \n-\n \ndynamicBackend\n:\n\n          \nname\n:\n \nm3msg\n\n          \nhashType\n:\n \nmurmur32\n\n          \nproducer\n:\n\n            \nwriter\n:\n\n              \ntopicName\n:\n \naggregated_metrics\n\n              \ntopicServiceOverride\n:\n\n                \nzone\n:\n \nembedded\n\n                \nenvironment\n:\n \nnamespace/m3db-cluster-name\n\n              \nmessagePool\n:\n\n                \nsize\n:\n \n16384\n\n                \nwatermark\n:\n\n                  \nlow\n:\n \n0.2\n\n                  \nhigh\n:\n \n0.5\n\n  \npassthrough\n:\n\n    \nenabled\n:\n \ntrue\n\n  \nforwarding\n:\n\n    \nmaxConstDelay\n:\n \n5m\n \n# Need to add some buffer window, since timed metrics by default are delayed by 1min.\n\n  \nentryTTL\n:\n \n1h\n\n  \nentryCheckInterval\n:\n \n10m\n\n  \nmaxTimerBatchSizePerWrite\n:\n \n140\n\n  \ndefaultStoragePolicies\n:\n \n[]\n\n  \nmaxNumCachedSourceSets\n:\n \n2\n\n  \ndiscardNaNAggregatedValues\n:\n \ntrue\n\n  \nentryPool\n:\n\n    \nsize\n:\n \n4096\n\n  \ncounterElemPool\n:\n\n    \nsize\n:\n \n4096\n\n  \ntimerElemPool\n:\n\n    \nsize\n:\n \n4096\n\n  \ngaugeElemPool\n:\n\n    \nsize\n:\n \n4096\n\n\n\n\n\nUsage\n\n\nSend metrics as usual to your \nm3coordinator\n instances in round robin fashion (or any other load balancing strategy), the metrics will be forwarded to the \nm3aggregator\n instances, then once aggregated they will be returned to the \nm3coordinator\n instances to write to M3DB.",
            "title": "M3Aggregator"
        },
        {
            "location": "/how_to/aggregator/#setting-up-m3-aggregator",
            "text": "",
            "title": "Setting up M3 Aggregator"
        },
        {
            "location": "/how_to/aggregator/#introduction",
            "text": "m3aggregator  is used to cluster stateful downsampling and rollup of metrics before they are store in M3DB. The M3 Coordinator also performs this role but is not cluster aware. This means metrics will not get aggregated properly if you send metrics in round robin fashion to multiple M3 Coordinators for the same metrics ingestion source (e.g. Prometheus server).  Similar to M3DB,  m3aggregator  supports clustering and replication by default. This means that metrics are correctly routed to the instance(s) responsible for aggregating each metric and multiple  m3aggregator  replicas can be configured such that there are no single points of failure for aggregation.",
            "title": "Introduction"
        },
        {
            "location": "/how_to/aggregator/#configuration",
            "text": "Before setting up m3aggregator, make sure that you have at least  one M3DB node running  and a dedicated m3coordinator setup.  We highly recommend running with at least a replication factor 2 for a  m3aggregator  deployment. If you run with replication factor 1 then when you restart an aggregator it will temporarily interrupt good the stream of aggregated metrics and there will be some data loss.",
            "title": "Configuration"
        },
        {
            "location": "/how_to/aggregator/#topology",
            "text": "",
            "title": "Topology"
        },
        {
            "location": "/how_to/aggregator/#initializing-aggregator-topology",
            "text": "You can setup a m3aggregator topology by issuing a request to your coordinator (be sure to use your own hostnames, number of shards and replication factor): curl -vvvsSf -H  \"Cluster-Environment-Name: namespace/m3db-cluster-name\"  -X POST http://m3dbnode-with-embedded-coordinator:7201/api/v1/services/m3aggregator/placement/init -d  '{      \"num_shards\": 64,      \"replication_factor\": 2,      \"instances\": [          {              \"id\": \"m3aggregator01:6000\",              \"isolation_group\": \"availability-zone-a\",              \"zone\": \"embedded\",              \"weight\": 100,              \"endpoint\": \"m3aggregator01:6000\",              \"hostname\": \"m3aggregator01\",              \"port\": 6000          },          {              \"id\": \"m3aggregator02:6000\",              \"isolation_group\": \"availability-zone-b\",              \"zone\": \"embedded\",              \"weight\": 100,              \"endpoint\": \"m3aggregator02:6000\",              \"hostname\": \"m3aggregator02\",              \"port\": 6000          }      ]  }'",
            "title": "Initializing aggregator topology"
        },
        {
            "location": "/how_to/aggregator/#initializing-m3msg-topic-for-m3aggregator-to-receive-from-m3coordinators-to-aggregate-metrics",
            "text": "Now we must setup a topic for the  m3aggregator  to receive unaggregated metrics from  m3coordinator  instances:  curl -vvvsSf -H  \"Cluster-Environment-Name: namespace/m3db-cluster-name\"  -H  \"Topic-Name: aggregator_ingest\"  -X POST http://m3dbnode-with-embedded-coordinator:7201/api/v1/topic/init -d  '{      \"numberOfShards\": 64  }'",
            "title": "Initializing m3msg topic for m3aggregator to receive from m3coordinators to aggregate metrics"
        },
        {
            "location": "/how_to/aggregator/#add-m3aggregagtor-consumer-group-to-ingest-topic",
            "text": "Add the  m3aggregator  placement to receive traffic from the topic (make sure to set message TTL to match your desired maximum in memory retry message buffer): curl -vvvsSf -H  \"Cluster-Environment-Name: namespace/m3db-cluster-name\"  -H  \"Topic-Name: aggregator_ingest\"  -X POST http://m3dbnode-with-embedded-coordinator:7201/api/v1/topic -d  '{    \"consumerService\": {      \"serviceId\": {        \"name\": \"m3aggregator\",        \"environment\": \"namespace/m3db-cluster-name\",        \"zone\": \"embedded\"      },      \"consumptionType\": \"REPLICATED\",      \"messageTtlNanos\": \"300000000000\"    }  }'   Note:  300000000000 nanoseconds is a TTL of 5 minutes for messages to rebuffer for retry.",
            "title": "Add m3aggregagtor consumer group to ingest topic"
        },
        {
            "location": "/how_to/aggregator/#initializing-m3msg-topic-for-m3coordinator-to-receive-from-m3aggregator-to-write-to-m3db",
            "text": "Now we must setup a topic for the  m3coordinator  to receive aggregated metrics from  m3aggregator  instances to write to M3DB: curl -vvvsSf -H  \"Cluster-Environment-Name: namespace/m3db-cluster-name\"  -H  \"Topic-Name: aggregated_metrics\"  -X POST http://m3dbnode-with-embedded-coordinator:7201/api/v1/topic/init -d  '{      \"numberOfShards\": 64  }'",
            "title": "Initializing m3msg topic for m3coordinator to receive from m3aggregator to write to M3DB"
        },
        {
            "location": "/how_to/aggregator/#initializing-m3coordinator-topology",
            "text": "Then  m3coordinator  instances need to be configured to receive traffic for this topic (note ingest at port 7507 must match the configured port for your  m3coordinator  ingest server, see config at bottom of this guide): curl -vvvsSf -H  \"Cluster-Environment-Name: namespace/m3db-cluster-name\"  -X POST http://m3dbnode-with-embedded-coordinator:7201/api/v1/services/m3coordinator/placement/init -d  '{      \"instances\": [          {              \"id\": \"m3coordinator01\",              \"zone\": \"embedded\",              \"endpoint\": \"m3coordinator01:7507\",              \"hostname\": \"m3coordinator01\",              \"port\": 7507          }      ]  }'   Note:  When you add or remove  m3coordinator  instances they must be added to this placement.",
            "title": "Initializing m3coordinator topology"
        },
        {
            "location": "/how_to/aggregator/#add-m3coordinator-consumer-group-to-outbound-topic",
            "text": "Add the  m3coordinator  placement to receive traffic from the topic (make sure to set message TTL to match your desired maximum in memory retry message buffer): curl -vvvsSf -H  \"Cluster-Environment-Name: namespace/m3db-cluster-name\"  -H  \"Topic-Name: aggregated_metrics\"  -X POST http://m3dbnode-with-embedded-coordinator:7201/api/v1/topic -d  '{    \"consumerService\": {      \"serviceId\": {        \"name\": \"m3coordinator\",        \"environment\": \"namespace/m3db-cluster-name\",        \"zone\": \"embedded\"      },      \"consumptionType\": \"SHARED\",      \"messageTtlNanos\": \"300000000000\"    }  }'   Note:  300000000000 nanoseconds is a TTL of 5 minutes for messages to rebuffer for retry.",
            "title": "Add m3coordinator consumer group to outbound topic"
        },
        {
            "location": "/how_to/aggregator/#running",
            "text": "",
            "title": "Running"
        },
        {
            "location": "/how_to/aggregator/#dedicated-coordinator",
            "text": "Metrics will still arrive at the  m3coordinator , they simply need to be forwarded to an  m3aggregator . The  m3coordinator  then also needs to receive metrics that have been aggregated from the  m3aggregator  and store them in M3DB, so running an ingestion server should be configured.  Here is the config you should add to your  m3coordinator : # This is for sending metrics to the remote m3aggregators  downsample : \n   remoteAggregator : \n     client : \n       type :   m3msg \n       m3msg : \n         producer : \n           writer : \n             topicName :   aggregator_ingest \n             topicServiceOverride : \n               zone :   embedded \n               environment :   namespace/m3db-cluster-name \n             placement : \n               isStaged :   true \n             placementServiceOverride : \n               namespaces : \n                 placement :   /placement \n             connection : \n               numConnections :   4 \n             messagePool : \n               size :   16384 \n               watermark : \n                 low :   0.2 \n                 high :   0.5  # This is for configuring the ingestion server that will receive metrics from the m3aggregators on port 7507  ingest : \n   ingester : \n     workerPoolSize :   10000 \n     opPool : \n       size :   10000 \n     retry : \n       maxRetries :   3 \n       jitter :   true \n     logSampleRate :   0.01 \n   m3msg : \n     server : \n       listenAddress :   \"0.0.0.0:7507\" \n       retry : \n         maxBackoff :   10s \n         jitter :   true",
            "title": "Dedicated Coordinator"
        },
        {
            "location": "/how_to/aggregator/#m3-aggregator",
            "text": "You can run  m3aggregator  by either building and running the binary yourself:  make m3aggregator\n./bin/m3aggregator -f ./src/aggregator/config/m3aggregator.yml  Or you can run it with Docker using the Docker file located at  docker/m3aggregator/Dockerfile  or the publicly provided image  quay.io/m3db/m3aggregator:latest .  You can use a config like so, making note of the topics used such as  aggregator_ingest  and  aggregated_metrics  and the corresponding environment  namespace/m3db-cluster-name :  logging : \n   level :   info  metrics : \n   scope : \n     prefix :   m3aggregator \n   prometheus : \n     onError :   none \n     handlerPath :   /metrics \n     listenAddress :   0.0.0.0:6002 \n     timerType :   histogram \n   sanitization :   prometheus \n   samplingRate :   1.0 \n   extended :   none  m3msg : \n   server : \n     listenAddress :   0.0.0.0:6000 \n     retry : \n       maxBackoff :   10s \n       jitter :   true \n   consumer : \n     messagePool : \n       size :   16384 \n       watermark : \n         low :   0.2 \n         high :   0.5  http : \n   listenAddress :   0.0.0.0:6001 \n   readTimeout :   60s \n   writeTimeout :   60s  kvClient : \n   etcd : \n     env :   namespace/m3db-cluster-name \n     zone :   embedded \n     service :   m3aggregator \n     cacheDir :   /var/lib/m3kv \n     etcdClusters : \n       -   zone :   embedded \n         endpoints : \n           -   dbnode01:2379  runtimeOptions : \n   kvConfig : \n     environment :   namespace/m3db-cluster-name \n     zone :   embedded \n   writeValuesPerMetricLimitPerSecondKey :   write-values-per-metric-limit-per-second \n   writeValuesPerMetricLimitPerSecond :   0 \n   writeNewMetricLimitClusterPerSecondKey :   write-new-metric-limit-cluster-per-second \n   writeNewMetricLimitClusterPerSecond :   0 \n   writeNewMetricNoLimitWarmupDuration :   0  aggregator : \n   hostID : \n     resolver :   environment \n     envVarName :   M3AGGREGATOR_HOST_ID \n   instanceID : \n     type :   host_id \n   verboseErrors :   true \n   metricPrefix :   \"\" \n   counterPrefix :   \"\" \n   timerPrefix :   \"\" \n   gaugePrefix :   \"\" \n   aggregationTypes : \n     counterTransformFnType :   empty \n     timerTransformFnType :   suffix \n     gaugeTransformFnType :   empty \n     aggregationTypesPool : \n       size :   1024 \n     quantilesPool : \n       buckets : \n         -   count :   256 \n           capacity :   4 \n         -   count :   128 \n           capacity :   8 \n   stream : \n     eps :   0.001 \n     capacity :   32 \n     streamPool : \n       size :   4096 \n     samplePool : \n       size :   4096 \n     floatsPool : \n       buckets : \n         -   count :   4096 \n           capacity :   16 \n         -   count :   2048 \n           capacity :   32 \n         -   count :   1024 \n           capacity :   64 \n   client : \n     type :   m3msg \n     m3msg : \n       producer : \n         writer : \n           topicName :   aggregator_ingest \n           topicServiceOverride : \n             zone :   embedded \n             environment :   namespace/m3db-cluster-name \n           placement : \n             isStaged :   true \n           placementServiceOverride : \n             namespaces : \n               placement :   /placement \n           messagePool : \n             size :   16384 \n             watermark : \n               low :   0.2 \n               high :   0.5 \n   placementManager : \n     kvConfig : \n       namespace :   /placement \n       environment :   namespace/m3db-cluster-name \n       zone :   embedded \n     placementWatcher : \n       key :   m3aggregator \n       initWatchTimeout :   10s \n   hashType :   murmur32 \n   bufferDurationBeforeShardCutover :   10m \n   bufferDurationAfterShardCutoff :   10m \n   bufferDurationForFutureTimedMetric :   10m   # Allow test to write into future. \n   resignTimeout :   1m \n   flushTimesManager : \n     kvConfig : \n       environment :   namespace/m3db-cluster-name \n       zone :   embedded \n     flushTimesKeyFmt :   shardset/%d/flush \n     flushTimesPersistRetrier : \n       initialBackoff :   100ms \n       backoffFactor :   2.0 \n       maxBackoff :   2s \n       maxRetries :   3 \n   electionManager : \n     election : \n       leaderTimeout :   10s \n       resignTimeout :   10s \n       ttlSeconds :   10 \n     serviceID : \n       name :   m3aggregator \n       environment :   namespace/m3db-cluster-name \n       zone :   embedded \n     electionKeyFmt :   shardset/%d/lock \n     campaignRetrier : \n       initialBackoff :   100ms \n       backoffFactor :   2.0 \n       maxBackoff :   2s \n       forever :   true \n       jitter :   true \n     changeRetrier : \n       initialBackoff :   100ms \n       backoffFactor :   2.0 \n       maxBackoff :   5s \n       forever :   true \n       jitter :   true \n     resignRetrier : \n       initialBackoff :   100ms \n       backoffFactor :   2.0 \n       maxBackoff :   5s \n       forever :   true \n       jitter :   true \n     campaignStateCheckInterval :   1s \n     shardCutoffCheckOffset :   30s \n   flushManager : \n     checkEvery :   1s \n     jitterEnabled :   true \n     maxJitters : \n       -   flushInterval :   5s \n         maxJitterPercent :   1.0 \n       -   flushInterval :   10s \n         maxJitterPercent :   0.5 \n       -   flushInterval :   1m \n         maxJitterPercent :   0.5 \n       -   flushInterval :   10m \n         maxJitterPercent :   0.5 \n       -   flushInterval :   1h \n         maxJitterPercent :   0.25 \n     numWorkersPerCPU :   0.5 \n     flushTimesPersistEvery :   10s \n     maxBufferSize :   5m \n     forcedFlushWindowSize :   10s \n   flush : \n     handlers : \n       -   dynamicBackend : \n           name :   m3msg \n           hashType :   murmur32 \n           producer : \n             writer : \n               topicName :   aggregated_metrics \n               topicServiceOverride : \n                 zone :   embedded \n                 environment :   namespace/m3db-cluster-name \n               messagePool : \n                 size :   16384 \n                 watermark : \n                   low :   0.2 \n                   high :   0.5 \n   passthrough : \n     enabled :   true \n   forwarding : \n     maxConstDelay :   5m   # Need to add some buffer window, since timed metrics by default are delayed by 1min. \n   entryTTL :   1h \n   entryCheckInterval :   10m \n   maxTimerBatchSizePerWrite :   140 \n   defaultStoragePolicies :   [] \n   maxNumCachedSourceSets :   2 \n   discardNaNAggregatedValues :   true \n   entryPool : \n     size :   4096 \n   counterElemPool : \n     size :   4096 \n   timerElemPool : \n     size :   4096 \n   gaugeElemPool : \n     size :   4096",
            "title": "M3 Aggregator"
        },
        {
            "location": "/how_to/aggregator/#usage",
            "text": "Send metrics as usual to your  m3coordinator  instances in round robin fashion (or any other load balancing strategy), the metrics will be forwarded to the  m3aggregator  instances, then once aggregated they will be returned to the  m3coordinator  instances to write to M3DB.",
            "title": "Usage"
        },
        {
            "location": "/how_to/use_as_tsdb/",
            "text": "Using M3DB as a general purpose time series database\n\n\nOverview\n\n\nM3 has native integrations that make it particularly easy to use it as a metrics storage for \nPrometheus\n and \nGraphite\n. M3DB can also be used as a general purpose distributed time series database by itself.\n\n\nData Model\n\n\nIDs and Tags\n\n\nM3DB's data model allows multiple namespaces, each of which can be \nconfigured and tuned independently\n.\n\n\nEach namespace can also be configured with its own schema (see \"Schema Modeling\" section below).\n\n\nWithin a namespace, each time series is uniquely identified by an ID which can be any valid string / byte array. In addition, tags can be attached to any series which makes the series queryable using the inverted index.\n\n\nM3DB's inverted index supports term (exact match) and regular expression queries over all tag values, and individual tag queries can be arbitrarily combined using \nAND\n, \nOR\n, and \nNOT\n operators.\n\n\nFor example, imagine an application that tracks a fleet of vehicles. One potential structure for the time series could be as follows:\n\n\n\n\n\n\n\n\n\n\nTimeseries 1\n\n\nTimeseries 2\n\n\nTimeseries 3\n\n\nTimeseries 4\n\n\n\n\n\n\n\n\n\n\nTimeseries ID\n\n\nvehicle_id_1\n\n\nvehicle_id_2\n\n\nvehicle_id_3\n\n\nvehicle_id_4\n\n\n\n\n\n\n\"type\" tag value\n\n\nsedan\n\n\nbike\n\n\nscooter\n\n\nscooter\n\n\n\n\n\n\n\"city\" tag value\n\n\nsan_francisco\n\n\nsan_francisco\n\n\nnew_york\n\n\nchicago\n\n\n\n\n\n\n\"version\" tag value\n\n\n0_1_0\n\n\n0_1_0\n\n\n0_1_1\n\n\n0_1_2\n\n\n\n\n\n\n\n\nThis would allow users to issue queries that answer questions like:\n\n\n\n\n\"What time series IDs exist for any vehicle type operating in San Francisco?\"\n\n\n\"What time series IDs exist for scooters that are NOT operating in Chicago?\"\n\n\n\"What time series IDs exist where the \"version\" tag matches the regular expression: \n0_1_[12]\n\"\n\n\n\n\nTODO(rartoul): Discuss the ability to perform limited amounts of aggregation queries here as well.\n\n\nTODO(rartoul): Discuss ID / tags mutability.\n\n\nData Points\n\n\nEach time series in M3DB stores data as a stream of data points in the form of \n<timestamp, value>\n tuples. Timestamp resolution can be as granular as individual nanoseconds.\n\n\nThe \nvalue\n portion of the tuple is a Protobuf message that matches the configured namespace schema, which requires that all values in the current time series must also match this schema. This limitation may be lifted in the future.\n\n\nSchema Modeling\n\n\nEvery M3DB namespace can be configured with a Protobuf-defined schema that every value in the time series must conform to\n\n\nFor example, continuing with the vehicle fleet tracking example introduced earlier, a schema might look as follows:\n\n\nsyntax\n \n=\n \n\"proto3\"\n;\n\n\n\nmessage\n \nVehicleLocation\n \n{\n\n  \ndouble\n \nlatitude\n \n=\n \n1\n;\n\n  \ndouble\n \nlongitude\n \n=\n \n2\n;\n\n  \ndouble\n \nfuel_percent\n \n=\n \n3\n;\n\n  \nstring\n \nstatus\n \n=\n \n4\n;\n\n\n}\n\n\n\n\n\nWhile M3DB strives to support the entire \nproto3 language spec\n, only \nthe following features are currently supported\n:\n\n\n\n\nScalar values\n\n\nNested messages\n\n\nRepeated fields\n\n\nMap fields\n\n\nReserved fields\n\n\n\n\nThe following features are currently not supported:\n\n\n\n\nAny\n fields\n\n\nOneof\n fields\n\n\nOptions of any type\n\n\nCustom field types\n\n\n\n\nCompression\n\n\nWhile M3DB supports schemas that contain nested messages, repeated fields, and map fields, currently it can only effectively compress top level scalar fields. For example, M3DB can compress every field in the following schema:\n\n\nsyntax\n \n=\n \n\"proto3\"\n;\n\n\n\nmessage\n \nVehicleLocation\n \n{\n\n  \ndouble\n \nlatitude\n \n=\n \n1\n;\n\n  \ndouble\n \nlongitude\n \n=\n \n2\n;\n\n  \ndouble\n \nfuel_percent\n \n=\n \n3\n;\n\n  \nstring\n \nstatus\n \n=\n \n4\n;\n\n\n}\n\n\n\n\n\nhowever, it will not apply any form of compression to the \nattributes\n field in this schema:\n\n\nsyntax\n \n=\n \n\"proto3\"\n;\n\n\n\nmessage\n \nVehicleLocation\n \n{\n\n  \ndouble\n \nlatitude\n \n=\n \n1\n;\n\n  \ndouble\n \nlongitude\n \n=\n \n2\n;\n\n  \ndouble\n \nfuel_percent\n \n=\n \n3\n;\n\n  \nstring\n \nstatus\n \n=\n \n4\n;\n\n  \nmap\n<\nstring\n,\n \nstring\n>\n \nattributes\n \n=\n \n5\n;\n\n\n}\n\n\n\n\n\nWhile the latter schema is valid, the attributes field will not be compressed; users should weigh the tradeoffs between more expressive schema and better compression for each use case.\n\n\nFor more details on the compression scheme and its limitations, review \nthe documentation for M3DB's compressed Protobuf encoding\n.\n\n\nGetting Started\n\n\nM3DB setup\n\n\nFor more advanced setups, it's best to follow the guides on how to configure an M3DB cluster \nmanually\n or \nusing Kubernetes\n. However, this tutorial will walk you through configuring a single node setup locally for development.\n\n\nFirst, run the following command to pull the latest M3DB image:\n\n\ndocker pull quay.io/m3db/m3dbnode:latest\n\n\n\n\nNext, run the following command to start the M3DB container:\n\n\ndocker run -p 7201:7201 -p 7203:7203 -p 9000:9000 -p 9001:9001 -p 9002:9002 -p 9003:9003 -p 9004:9004 -p 2379:2379 --name m3db -v $(pwd)/m3db_data:/var/lib/m3db -v $(pwd)/src/dbnode/config/m3dbnode-local-etcd-proto.yml:/etc/m3dbnode/m3dbnode.yml -v <PATH_TO_SCHEMA_Protobuf_FILE>:/etc/m3dbnode/default_schema.proto quay.io/m3db/m3dbnode:latest\n\n\n\n\nBreaking that down:\n\n\n\n\nAll the \n-p\n flags expose the necessary ports.\n\n\nThe \n-v $(pwd)/m3db_data:/var/lib/m3db\n section creates a bind mount that enables M3DB to persist data between container restarts.\n\n\nThe \n-v <PATH_TO_YAML_CONFIG_FILE>:/etc/m3dbnode/m3dbnode.yml\n section mounts the specified configuration file in the container which allows configuration changes by restarting the container (rather than rebuilding it). \nThis example file\n can be used as a good starting point. It configures the database to have the Protobuf feature enabled and expects one namespace with the name \ndefault\n and a Protobuf message name of \nVehicleLocation\n for the schema. You'll need to update that portion of the config if you intend to use a different schema than the example one used throughout this document. Note that hard-coding paths to the schema should only be done for local development and testing. For production use-cases, M3DB supports storing the current schema in \netcd\n so that it can be update dynamically. TODO(rartoul): Document how to do that as well as what kind of schema changes are safe / backwards compatible.\n\n\nThe \n-v <PATH_TO_SCHEMA_Protobuf_FILE>:/etc/m3dbnode/default_schema.proto\n section mounts the Protobuf file containing the schema in the container, similar to the configuration file this allows the schema to be changed by restarting the container instead of rebuilding it. You can use \nthis example schema\n as a starting point. Is is also the same example schema that is used by the sample Go program discussed below in the \"Clients\" section. Also see the bullet point above about not hard coding schema files in production.\n\n\n\n\nOnce the M3DB container has started, issue the following CURL statement to create the \ndefault\n namespace:\n\n\ncurl -X POST http://localhost:7201/api/v1/database/create -d '{\n  \"type\": \"local\",\n  \"namespaceName\": \"default\",\n  \"retentionTime\": \"4h\"\n}'\n\n\n\n\nNote that the \nretentionTime\n is set artificially low to conserve resources.\n\n\nAfter a few moments, the M3DB container should finish bootstrapping. At this point it should be ready to serve write and read queries.\n\n\nClients\n\n\nNote: M3DB only has a Go client; this is unlikely to change in the future due to the fact that the client is \"fat\" and contains a substantial amount of logic that would be difficult to port to other languages.\n\n\nUsers interested in interacting with M3DB directly from Go applications can reference \nthis runnable example\n to get an understanding of how to interact with M3DB in Go. Note that the example above uses the same \ndefault\n namespace and \nVehicleLocation\n schema used throughout this document so it can be run directly against an M3DB docker container setup using the \"M3DB setup\" instructions above.\n\n\nM3DB will eventually support other languages by exposing an \nM3Coordinator\n endpoint which will allow users to write/read from M3DB directly using GRPC/JSON.",
            "title": "Use M3DB as a general purpose time series database"
        },
        {
            "location": "/how_to/use_as_tsdb/#using-m3db-as-a-general-purpose-time-series-database",
            "text": "",
            "title": "Using M3DB as a general purpose time series database"
        },
        {
            "location": "/how_to/use_as_tsdb/#overview",
            "text": "M3 has native integrations that make it particularly easy to use it as a metrics storage for  Prometheus  and  Graphite . M3DB can also be used as a general purpose distributed time series database by itself.",
            "title": "Overview"
        },
        {
            "location": "/how_to/use_as_tsdb/#data-model",
            "text": "",
            "title": "Data Model"
        },
        {
            "location": "/how_to/use_as_tsdb/#ids-and-tags",
            "text": "M3DB's data model allows multiple namespaces, each of which can be  configured and tuned independently .  Each namespace can also be configured with its own schema (see \"Schema Modeling\" section below).  Within a namespace, each time series is uniquely identified by an ID which can be any valid string / byte array. In addition, tags can be attached to any series which makes the series queryable using the inverted index.  M3DB's inverted index supports term (exact match) and regular expression queries over all tag values, and individual tag queries can be arbitrarily combined using  AND ,  OR , and  NOT  operators.  For example, imagine an application that tracks a fleet of vehicles. One potential structure for the time series could be as follows:      Timeseries 1  Timeseries 2  Timeseries 3  Timeseries 4      Timeseries ID  vehicle_id_1  vehicle_id_2  vehicle_id_3  vehicle_id_4    \"type\" tag value  sedan  bike  scooter  scooter    \"city\" tag value  san_francisco  san_francisco  new_york  chicago    \"version\" tag value  0_1_0  0_1_0  0_1_1  0_1_2     This would allow users to issue queries that answer questions like:   \"What time series IDs exist for any vehicle type operating in San Francisco?\"  \"What time series IDs exist for scooters that are NOT operating in Chicago?\"  \"What time series IDs exist where the \"version\" tag matches the regular expression:  0_1_[12] \"   TODO(rartoul): Discuss the ability to perform limited amounts of aggregation queries here as well.  TODO(rartoul): Discuss ID / tags mutability.",
            "title": "IDs and Tags"
        },
        {
            "location": "/how_to/use_as_tsdb/#data-points",
            "text": "Each time series in M3DB stores data as a stream of data points in the form of  <timestamp, value>  tuples. Timestamp resolution can be as granular as individual nanoseconds.  The  value  portion of the tuple is a Protobuf message that matches the configured namespace schema, which requires that all values in the current time series must also match this schema. This limitation may be lifted in the future.",
            "title": "Data Points"
        },
        {
            "location": "/how_to/use_as_tsdb/#schema-modeling",
            "text": "Every M3DB namespace can be configured with a Protobuf-defined schema that every value in the time series must conform to  For example, continuing with the vehicle fleet tracking example introduced earlier, a schema might look as follows:  syntax   =   \"proto3\" ;  message   VehicleLocation   { \n   double   latitude   =   1 ; \n   double   longitude   =   2 ; \n   double   fuel_percent   =   3 ; \n   string   status   =   4 ;  }   While M3DB strives to support the entire  proto3 language spec , only  the following features are currently supported :   Scalar values  Nested messages  Repeated fields  Map fields  Reserved fields   The following features are currently not supported:   Any  fields  Oneof  fields  Options of any type  Custom field types",
            "title": "Schema Modeling"
        },
        {
            "location": "/how_to/use_as_tsdb/#compression",
            "text": "While M3DB supports schemas that contain nested messages, repeated fields, and map fields, currently it can only effectively compress top level scalar fields. For example, M3DB can compress every field in the following schema:  syntax   =   \"proto3\" ;  message   VehicleLocation   { \n   double   latitude   =   1 ; \n   double   longitude   =   2 ; \n   double   fuel_percent   =   3 ; \n   string   status   =   4 ;  }   however, it will not apply any form of compression to the  attributes  field in this schema:  syntax   =   \"proto3\" ;  message   VehicleLocation   { \n   double   latitude   =   1 ; \n   double   longitude   =   2 ; \n   double   fuel_percent   =   3 ; \n   string   status   =   4 ; \n   map < string ,   string >   attributes   =   5 ;  }   While the latter schema is valid, the attributes field will not be compressed; users should weigh the tradeoffs between more expressive schema and better compression for each use case.  For more details on the compression scheme and its limitations, review  the documentation for M3DB's compressed Protobuf encoding .",
            "title": "Compression"
        },
        {
            "location": "/how_to/use_as_tsdb/#getting-started",
            "text": "",
            "title": "Getting Started"
        },
        {
            "location": "/how_to/use_as_tsdb/#m3db-setup",
            "text": "For more advanced setups, it's best to follow the guides on how to configure an M3DB cluster  manually  or  using Kubernetes . However, this tutorial will walk you through configuring a single node setup locally for development.  First, run the following command to pull the latest M3DB image:  docker pull quay.io/m3db/m3dbnode:latest  Next, run the following command to start the M3DB container:  docker run -p 7201:7201 -p 7203:7203 -p 9000:9000 -p 9001:9001 -p 9002:9002 -p 9003:9003 -p 9004:9004 -p 2379:2379 --name m3db -v $(pwd)/m3db_data:/var/lib/m3db -v $(pwd)/src/dbnode/config/m3dbnode-local-etcd-proto.yml:/etc/m3dbnode/m3dbnode.yml -v <PATH_TO_SCHEMA_Protobuf_FILE>:/etc/m3dbnode/default_schema.proto quay.io/m3db/m3dbnode:latest  Breaking that down:   All the  -p  flags expose the necessary ports.  The  -v $(pwd)/m3db_data:/var/lib/m3db  section creates a bind mount that enables M3DB to persist data between container restarts.  The  -v <PATH_TO_YAML_CONFIG_FILE>:/etc/m3dbnode/m3dbnode.yml  section mounts the specified configuration file in the container which allows configuration changes by restarting the container (rather than rebuilding it).  This example file  can be used as a good starting point. It configures the database to have the Protobuf feature enabled and expects one namespace with the name  default  and a Protobuf message name of  VehicleLocation  for the schema. You'll need to update that portion of the config if you intend to use a different schema than the example one used throughout this document. Note that hard-coding paths to the schema should only be done for local development and testing. For production use-cases, M3DB supports storing the current schema in  etcd  so that it can be update dynamically. TODO(rartoul): Document how to do that as well as what kind of schema changes are safe / backwards compatible.  The  -v <PATH_TO_SCHEMA_Protobuf_FILE>:/etc/m3dbnode/default_schema.proto  section mounts the Protobuf file containing the schema in the container, similar to the configuration file this allows the schema to be changed by restarting the container instead of rebuilding it. You can use  this example schema  as a starting point. Is is also the same example schema that is used by the sample Go program discussed below in the \"Clients\" section. Also see the bullet point above about not hard coding schema files in production.   Once the M3DB container has started, issue the following CURL statement to create the  default  namespace:  curl -X POST http://localhost:7201/api/v1/database/create -d '{\n  \"type\": \"local\",\n  \"namespaceName\": \"default\",\n  \"retentionTime\": \"4h\"\n}'  Note that the  retentionTime  is set artificially low to conserve resources.  After a few moments, the M3DB container should finish bootstrapping. At this point it should be ready to serve write and read queries.",
            "title": "M3DB setup"
        },
        {
            "location": "/how_to/use_as_tsdb/#clients",
            "text": "Note: M3DB only has a Go client; this is unlikely to change in the future due to the fact that the client is \"fat\" and contains a substantial amount of logic that would be difficult to port to other languages.  Users interested in interacting with M3DB directly from Go applications can reference  this runnable example  to get an understanding of how to interact with M3DB in Go. Note that the example above uses the same  default  namespace and  VehicleLocation  schema used throughout this document so it can be run directly against an M3DB docker container setup using the \"M3DB setup\" instructions above.  M3DB will eventually support other languages by exposing an  M3Coordinator  endpoint which will allow users to write/read from M3DB directly using GRPC/JSON.",
            "title": "Clients"
        },
        {
            "location": "/operational_guide/",
            "text": "Operational Guides\n\n\nThis list of operational guides provide documentation for operating M3.",
            "title": "Overview"
        },
        {
            "location": "/operational_guide/#operational-guides",
            "text": "This list of operational guides provide documentation for operating M3.",
            "title": "Operational Guides"
        },
        {
            "location": "/operational_guide/replication_and_deployment_in_zones/",
            "text": "Replication and Deployment in Zones\n\n\nOverview\n\n\nM3DB supports both deploying across multiple zones in a region or deploying to a single zone with rack-level isolation. It can also be deployed across multiple regions for a global view of data, though both latency and bandwidth costs may increase as a result.\n\n\nIn addition, M3DB has support for automatically replicating data between isolated M3DB clusters (potentially running in different zones / regions). More details can be found in the \nReplication between clusters\n operational guide.\n\n\nReplication\n\n\nA replication factor of at least 3 is highly recommended for any M3DB deployment, due to the consistency levels (for both reads and writes) that require quorum in order to complete an operation. For more information on consistency levels, see the documentation concerning \ntuning availability, consistency and durability\n.\n\n\nM3DB will do its best to distribute shards evenly among the availability zones while still taking each individual node's weight into account, but if some of the availability zones have less available hosts than others then each host in that zone will be responsible for more shards than hosts in the other zones and will thus be subjected to heavier load.\n\n\nReplication Factor Recommendations\n\n\nRunning with \nRF=1\n or \nRF=2\n is not recommended for any multi-node use cases (testing or production). In \nthe\nfuture\n such topologies may be rejected by M3DB entirely. It is also recommended to only run with an odd number of\nreplicas.\n\n\nRF=1\n is not recommended as it is impossible to perform a safe upgrade or tolerate any node failures: as soon as one\nnode is down, all writes destined for the shards it owned will fail. If the node's storage is lost (e.g. the disk\nfails), the data is gone forever.\n\n\nRF=2\n, despite having an extra replica, entails many of the same problems \nRF=1\n does. When M3DB is configured to\nperform quorum writes and reads (the default), as soon as a single node is down (for planned maintenance or an unplanned\ndisruption) clients will be unable to read or write (as the quorum of 2 nodes is 2). Even if clients relax their\nconsistency guarantees and read from the remaining serving node, users may experience flapping results depending on\nwhether one node had data for a time window that the other did not.\n\n\nFinally, it is only recommended to run with an odd number of replicas. Because the quorum size of an even-RF \nN\n is\n\n(N/2)+1\n, any cluster with an even replica factor N has the same failure tolerance as a cluster with \nRF=N-1\n. \"Failure\ntolerance\" is defined as the number of \nisolation groups\n you can concurrently lose nodes across. The\nfollowing table demonstrates the quorum size and failure tolerance of various RF's, inspired by etcd's \nfailure\ntolerance\n documentation.\n\n\n\n\n\n\n\n\nReplica Factor\n\n\nQuorum Size\n\n\nFailure Tolerance\n\n\n\n\n\n\n\n\n\n\n1\n\n\n1\n\n\n0\n\n\n\n\n\n\n2\n\n\n2\n\n\n0\n\n\n\n\n\n\n3\n\n\n2\n\n\n1\n\n\n\n\n\n\n4\n\n\n3\n\n\n1\n\n\n\n\n\n\n5\n\n\n3\n\n\n2\n\n\n\n\n\n\n6\n\n\n4\n\n\n2\n\n\n\n\n\n\n7\n\n\n4\n\n\n3\n\n\n\n\n\n\n\n\nUpgrading hosts in a deployment\n\n\nWhen an M3DB node is restarted it has to perform a bootstrap process before it can serve reads. During this time the node will continue to accept writes, but will not be available for reads.\n\n\nObviously, there is also a small window of time during between when the process is stopped and then started again where it will also be unavailable for writes.\n\n\nDeployment across multiple availability zones in a region\n\n\nFor deployment in a region, it is recommended to set the \nisolationGroup\n host attribute to the name of the availability zone a host is in.\n\n\nIn this configuration, shards are distributed among hosts such that each will not be placed more than once in the same availability zone. This allows an entire availability zone to be lost at any given time, as it is guaranteed to only affect one replica of data.\n\n\nFor example, in a multi-zone deployment with four shards spread over three availability zones:\n\n\n\n\nTypically, deployments have many more than four shards - this is a simple example that illustrates how M3DB maintains availability while losing an availability zone, as two of three replicas are still intact.\n\n\nDeployment in a single zone\n\n\nFor deployment in a single zone, it is recommended to set the \nisolationGroup\n host attribute to the name of the rack a host is in or another logical unit that separates groups of hosts in your zone.\n\n\nIn this configuration, shards are distributed among hosts such that each will not be placed more than once in the same defined rack or logical unit. This allows an entire unit to be lost at any given time, as it is guaranteed to only affect one replica of data.\n\n\nFor example, in a single-zone deployment with three shards spread over four racks:\n\n\n\n\nTypically, deployments have many more than three shards - this is a simple example that illustrates how M3DB maintains availability while losing a single rack, as two of three replicas are still intact.\n\n\nDeployment across multiple regions\n\n\nFor deployment across regions, it is recommended to set the \nisolationGroup\n host attribute to the name of the region a host is in.\n\n\nAs mentioned previously, latency and bandwidth costs may increase when using clusters that span regions.\n\n\nIn this configuration, shards are distributed among hosts such that each will not be placed more than once in the same region. This allows an entire region to be lost at any given time, as it is guaranteed to only affect one replica of data.\n\n\nFor example, in a multi-region deployment with four shards spread over five regions:\n\n\n\n\nTypically, deployments have many more than four shards - this is a simple example that illustrates how M3DB maintains availability while losing up to two regions, as three of five replicas are still intact.",
            "title": "Replication and Deployment in Zones"
        },
        {
            "location": "/operational_guide/replication_and_deployment_in_zones/#replication-and-deployment-in-zones",
            "text": "",
            "title": "Replication and Deployment in Zones"
        },
        {
            "location": "/operational_guide/replication_and_deployment_in_zones/#overview",
            "text": "M3DB supports both deploying across multiple zones in a region or deploying to a single zone with rack-level isolation. It can also be deployed across multiple regions for a global view of data, though both latency and bandwidth costs may increase as a result.  In addition, M3DB has support for automatically replicating data between isolated M3DB clusters (potentially running in different zones / regions). More details can be found in the  Replication between clusters  operational guide.",
            "title": "Overview"
        },
        {
            "location": "/operational_guide/replication_and_deployment_in_zones/#replication",
            "text": "A replication factor of at least 3 is highly recommended for any M3DB deployment, due to the consistency levels (for both reads and writes) that require quorum in order to complete an operation. For more information on consistency levels, see the documentation concerning  tuning availability, consistency and durability .  M3DB will do its best to distribute shards evenly among the availability zones while still taking each individual node's weight into account, but if some of the availability zones have less available hosts than others then each host in that zone will be responsible for more shards than hosts in the other zones and will thus be subjected to heavier load.",
            "title": "Replication"
        },
        {
            "location": "/operational_guide/replication_and_deployment_in_zones/#replication-factor-recommendations",
            "text": "Running with  RF=1  or  RF=2  is not recommended for any multi-node use cases (testing or production). In  the\nfuture  such topologies may be rejected by M3DB entirely. It is also recommended to only run with an odd number of\nreplicas.  RF=1  is not recommended as it is impossible to perform a safe upgrade or tolerate any node failures: as soon as one\nnode is down, all writes destined for the shards it owned will fail. If the node's storage is lost (e.g. the disk\nfails), the data is gone forever.  RF=2 , despite having an extra replica, entails many of the same problems  RF=1  does. When M3DB is configured to\nperform quorum writes and reads (the default), as soon as a single node is down (for planned maintenance or an unplanned\ndisruption) clients will be unable to read or write (as the quorum of 2 nodes is 2). Even if clients relax their\nconsistency guarantees and read from the remaining serving node, users may experience flapping results depending on\nwhether one node had data for a time window that the other did not.  Finally, it is only recommended to run with an odd number of replicas. Because the quorum size of an even-RF  N  is (N/2)+1 , any cluster with an even replica factor N has the same failure tolerance as a cluster with  RF=N-1 . \"Failure\ntolerance\" is defined as the number of  isolation groups  you can concurrently lose nodes across. The\nfollowing table demonstrates the quorum size and failure tolerance of various RF's, inspired by etcd's  failure\ntolerance  documentation.     Replica Factor  Quorum Size  Failure Tolerance      1  1  0    2  2  0    3  2  1    4  3  1    5  3  2    6  4  2    7  4  3",
            "title": "Replication Factor Recommendations"
        },
        {
            "location": "/operational_guide/replication_and_deployment_in_zones/#upgrading-hosts-in-a-deployment",
            "text": "When an M3DB node is restarted it has to perform a bootstrap process before it can serve reads. During this time the node will continue to accept writes, but will not be available for reads.  Obviously, there is also a small window of time during between when the process is stopped and then started again where it will also be unavailable for writes.",
            "title": "Upgrading hosts in a deployment"
        },
        {
            "location": "/operational_guide/replication_and_deployment_in_zones/#deployment-across-multiple-availability-zones-in-a-region",
            "text": "For deployment in a region, it is recommended to set the  isolationGroup  host attribute to the name of the availability zone a host is in.  In this configuration, shards are distributed among hosts such that each will not be placed more than once in the same availability zone. This allows an entire availability zone to be lost at any given time, as it is guaranteed to only affect one replica of data.  For example, in a multi-zone deployment with four shards spread over three availability zones:   Typically, deployments have many more than four shards - this is a simple example that illustrates how M3DB maintains availability while losing an availability zone, as two of three replicas are still intact.",
            "title": "Deployment across multiple availability zones in a region"
        },
        {
            "location": "/operational_guide/replication_and_deployment_in_zones/#deployment-in-a-single-zone",
            "text": "For deployment in a single zone, it is recommended to set the  isolationGroup  host attribute to the name of the rack a host is in or another logical unit that separates groups of hosts in your zone.  In this configuration, shards are distributed among hosts such that each will not be placed more than once in the same defined rack or logical unit. This allows an entire unit to be lost at any given time, as it is guaranteed to only affect one replica of data.  For example, in a single-zone deployment with three shards spread over four racks:   Typically, deployments have many more than three shards - this is a simple example that illustrates how M3DB maintains availability while losing a single rack, as two of three replicas are still intact.",
            "title": "Deployment in a single zone"
        },
        {
            "location": "/operational_guide/replication_and_deployment_in_zones/#deployment-across-multiple-regions",
            "text": "For deployment across regions, it is recommended to set the  isolationGroup  host attribute to the name of the region a host is in.  As mentioned previously, latency and bandwidth costs may increase when using clusters that span regions.  In this configuration, shards are distributed among hosts such that each will not be placed more than once in the same region. This allows an entire region to be lost at any given time, as it is guaranteed to only affect one replica of data.  For example, in a multi-region deployment with four shards spread over five regions:   Typically, deployments have many more than four shards - this is a simple example that illustrates how M3DB maintains availability while losing up to two regions, as three of five replicas are still intact.",
            "title": "Deployment across multiple regions"
        },
        {
            "location": "/operational_guide/monitoring/",
            "text": "Metrics\n\n\nIt is best to use Prometheus to monitor M3DB, M3 Coordinator and M3 Query using the \nGrafana dashboards\n.\n\n\nLogs\n\n\nLogs are printed to process output in JSON by default for semi-structured log processing.\n\n\nTracing\n\n\nM3DB is integrated with \nopentracing\n to provide\ninsight into query performance and errors.\n\n\nJaeger\n\n\nTo enable Jaeger as the tracing backend, set tracing.backend to \"jaeger\" (see also our \nsample local\nconfig\n:\n\n\ntracing:\n    backend: jaeger  # enables jaeger with default configs\n    jaeger:\n        # optional configuration for jaeger -- see\n        # https://github.com/jaegertracing/jaeger-client-go/blob/master/config/config.go#L37\n        # for options\n        ...\n\n\n\n\nJaeger can be run locally with docker as described in\n\nhttps://www.jaegertracing.io/docs/1.9/getting-started/\n.\n\n\nThe default configuration will report traces via udp to localhost:6831;\nusing the all-in-one jaeger container, they will be accessible at\n\n\nhttp://localhost:16686\n\n\nN.B.: for production workloads, you will almost certainly want to use\nsampler.type=remote with\n\nadaptive sampling\n\nfor Jaeger, as write volumes are likely orders of magnitude higher than\nread volumes in most timeseries systems.\n\n\nLightStep\n\n\nTo use LightStep as the tracing backend, set \ntracing.backend\n to \n\"lightstep\"\n and configure necessary information for\nyour client under \nlightstep\n. Any \noptions exposed\n in \nlightstep-tracer-go\n can be set in config.\nAny environment variables may be interpolated. For example:\n\n\ntracing\n:\n\n  \nserviceName\n:\n \nm3coordinator\n\n  \nbackend\n:\n \nlightstep\n\n  \nlightstep\n:\n\n    \naccess_token\n:\n \n${LIGHTSTEP_ACCESS_TOKEN:\"\"}\n\n    \ncollector\n:\n\n      \nscheme\n:\n \nhttps\n\n      \nhost\n:\n \nmy-satellite-address.domain\n\n      \nport\n:\n \n8181\n\n\n\n\n\nAlternative backends\n\n\nIf you'd like additional backends, we'd love to support them!\n\n\nFile an issue against M3 and we can work with you on how best to add\nthe backend. The first time's going to be a little rough--opentracing\nunfortunately doesn't support Go plugins (yet--see\n\nhttps://github.com/opentracing/opentracing-go/issues/133\n), and Go's dependency\nmodel means that adding dependencies directly will update\n\neverything\n, which isn't ideal for an isolated dependency change.\nThese problems are all solvable though,\nand we'll work with you to make it happen!\n\n\nUse cases\n\n\nNote: all URLs assume a local jaeger setup as described in Jaeger's\n\ndocs\n.\n\n\nFinding slow queries\n\n\nTo find prom queries longer than \n, filter for \nminDuration >= <threshold>\n on\n\noperation=\"GET /api/v1/query_range\"\n.\n\n\nSample query:\n\nhttp://localhost:16686/search?end=1548876672544000&limit=20&lookback=1h&maxDuration&minDuration=1ms&operation=GET%20%2Fapi%2Fv1%2Fquery_range&service=m3query&start=1548873072544000\n\n\nFinding queries with errors\n\n\nSearch for \nerror=true\n on \noperation=\"GET /api/v1/query_range\"\n\n\nhttp://localhost:16686/search?operation=GET%20%2Fapi%2Fv1%2Fquery_range&service=m3query&tags=%7B%22error%22%3A%22true%22%7D\n\n\nFinding 500 (Internal Server Error) responses\n\n\nSearch for \nhttp.status_code=500\n.\n\n\nhttp://localhost:16686/search?limit=20&lookback=24h&maxDuration&minDuration&operation=GET%20%2Fapi%2Fv1%2Fquery_range&service=m3query&start=1548802430108000&tags=%7B\"http.status_code\"%3A\"500\"%7D",
            "title": "Monitoring"
        },
        {
            "location": "/operational_guide/monitoring/#metrics",
            "text": "It is best to use Prometheus to monitor M3DB, M3 Coordinator and M3 Query using the  Grafana dashboards .",
            "title": "Metrics"
        },
        {
            "location": "/operational_guide/monitoring/#logs",
            "text": "Logs are printed to process output in JSON by default for semi-structured log processing.",
            "title": "Logs"
        },
        {
            "location": "/operational_guide/monitoring/#tracing",
            "text": "M3DB is integrated with  opentracing  to provide\ninsight into query performance and errors.",
            "title": "Tracing"
        },
        {
            "location": "/operational_guide/monitoring/#jaeger",
            "text": "To enable Jaeger as the tracing backend, set tracing.backend to \"jaeger\" (see also our  sample local\nconfig :  tracing:\n    backend: jaeger  # enables jaeger with default configs\n    jaeger:\n        # optional configuration for jaeger -- see\n        # https://github.com/jaegertracing/jaeger-client-go/blob/master/config/config.go#L37\n        # for options\n        ...  Jaeger can be run locally with docker as described in https://www.jaegertracing.io/docs/1.9/getting-started/ .  The default configuration will report traces via udp to localhost:6831;\nusing the all-in-one jaeger container, they will be accessible at  http://localhost:16686  N.B.: for production workloads, you will almost certainly want to use\nsampler.type=remote with adaptive sampling \nfor Jaeger, as write volumes are likely orders of magnitude higher than\nread volumes in most timeseries systems.",
            "title": "Jaeger"
        },
        {
            "location": "/operational_guide/monitoring/#lightstep",
            "text": "To use LightStep as the tracing backend, set  tracing.backend  to  \"lightstep\"  and configure necessary information for\nyour client under  lightstep . Any  options exposed  in  lightstep-tracer-go  can be set in config.\nAny environment variables may be interpolated. For example:  tracing : \n   serviceName :   m3coordinator \n   backend :   lightstep \n   lightstep : \n     access_token :   ${LIGHTSTEP_ACCESS_TOKEN:\"\"} \n     collector : \n       scheme :   https \n       host :   my-satellite-address.domain \n       port :   8181",
            "title": "LightStep"
        },
        {
            "location": "/operational_guide/monitoring/#alternative-backends",
            "text": "If you'd like additional backends, we'd love to support them!  File an issue against M3 and we can work with you on how best to add\nthe backend. The first time's going to be a little rough--opentracing\nunfortunately doesn't support Go plugins (yet--see https://github.com/opentracing/opentracing-go/issues/133 ), and Go's dependency\nmodel means that adding dependencies directly will update everything , which isn't ideal for an isolated dependency change.\nThese problems are all solvable though,\nand we'll work with you to make it happen!",
            "title": "Alternative backends"
        },
        {
            "location": "/operational_guide/monitoring/#use-cases",
            "text": "Note: all URLs assume a local jaeger setup as described in Jaeger's docs .",
            "title": "Use cases"
        },
        {
            "location": "/operational_guide/monitoring/#finding-slow-queries",
            "text": "To find prom queries longer than  , filter for  minDuration >= <threshold>  on operation=\"GET /api/v1/query_range\" .  Sample query: http://localhost:16686/search?end=1548876672544000&limit=20&lookback=1h&maxDuration&minDuration=1ms&operation=GET%20%2Fapi%2Fv1%2Fquery_range&service=m3query&start=1548873072544000",
            "title": "Finding slow queries"
        },
        {
            "location": "/operational_guide/monitoring/#finding-queries-with-errors",
            "text": "Search for  error=true  on  operation=\"GET /api/v1/query_range\"  http://localhost:16686/search?operation=GET%20%2Fapi%2Fv1%2Fquery_range&service=m3query&tags=%7B%22error%22%3A%22true%22%7D",
            "title": "Finding queries with errors"
        },
        {
            "location": "/operational_guide/monitoring/#finding-500-internal-server-error-responses",
            "text": "Search for  http.status_code=500 .  http://localhost:16686/search?limit=20&lookback=24h&maxDuration&minDuration&operation=GET%20%2Fapi%2Fv1%2Fquery_range&service=m3query&start=1548802430108000&tags=%7B\"http.status_code\"%3A\"500\"%7D",
            "title": "Finding 500 (Internal Server Error) responses"
        },
        {
            "location": "/operational_guide/upgrading_m3/",
            "text": "Upgrading M3\n\n\nOverview\n\n\nThis guide explains how to upgrade M3 from one version to another (e.g. from 0.14.0 to 0.15.0).\nThis includes upgrading:\n\n\n\n\nm3dbnode\n\n\nm3coordinator\n\n\nm3query\n\n\nm3aggregator\n\n\n\n\nm3dbnode\n\n\nGraphs to monitor\n\n\nWhile upgrading M3DB nodes, it's important to monitor the status of bootstrapping the individual nodes. This can be monitored using the \nM3DB Node Details\n dashboard.\nTypically, the \nBootstrapped\n graph under \nBackground Tasks\n and the graphs within the \nCPU and Memory Utilization\n give a good understanding of how well bootstrapping is going.\n\n\nKubernetes\n\n\nIf running \nM3DB\n on Kubernetes, upgrade by completing the following steps. \n\n\n\n\n\n\nIdentify the version of m3dbnode to upgrade to \non Quay\n.\n\n\n\n\n\n\nReplace the Docker image in the \nStatefulSet\n manifest (or \nm3db-operator\n manifest) to be the new version of m3dbnode.\n\n\n\n\n\n\nspec\n:\n\n  \nimage\n:\n \nquay.io/m3db/m3dbnode:$VERSION\n\n\n\n\n\n\n\nOnce updated, apply the updated manifest and a rolling restart will be performed. You must wait until the \nStatefulSet\n is entirely upgraded and bootstrapped (as per the M3DB Node Details dashboard) before proceeding to the next \nStatefulSet\n otherwise multiple replicas will be unavailable at once.\n\n\n\n\nkubectl apply -f <m3dbnode_manifest>\n\n\n\n\nDowngrading\n\n\nThe \nupgrading\n steps above can also be used to downgrade M3DB. However, it is important to refer to the release notes to make sure that versions are\nbackwards compatible.\n\n\nm3coordinator\n\n\nm3coordinator\n can be upgraded using similar steps as \nm3dbnode\n, however, the images can be \nfound here\n instead.\n\n\nm3query\n\n\nm3query\n can be upgraded using similar steps as \nm3dbnode\n, however, the images can be \nfound here\n instead.\n\n\nm3aggregator\n\n\nm3aggregator\n can be upgraded using similar steps as \nm3dbnode\n, however, the images can be \nfound here\n instead.\n\n\nNon-Kubernetes\n\n\nIt is very important that for each replica set, only one node gets upgraded at a time. However, multiple nodes can be upgraded across replica sets. \n\n\n1) Download new binary (linux example below).\n\n\nwget \n\"https://github.com/m3db/m3/releases/download/v\n$VERSION\n/m3_\n$VERSION_linux_amd64\n.tar.gz\"\n \n&&\n tar xvzf m3_\n$VERSION_linux_amd64\n.tar.gz \n&&\n rm m3_\n$VERSION_linux_amd64\n.tar.gz\n\n\n\n\n2) Stop and upgrade one M3DB node at a time per replica set using the \nsystemd unit\n.\n\n\n# stop m3dbnode\n\nsudo systemctl stop m3dbnode\n\n\n# start m3dbnode with the new binary (which should be placed in the path specified in the systemd unit)\n\nsudo systemctl start m3dbnode\n\n\n\n\nNote:\n If unable to stop \nm3dbnode\n using \nsystemctl\n, use \npkill\n instead.\n\n\n# stop m3dbnode\n\npkill m3dbnode\n\n\n# start m3dbnode with new binary\n\n./m3_\n$VERSION_linux_amd64\n/m3dbnode -f <config-name.yml>\n\n\n\n\n3) Confirm m3dbnode has finished bootstrapping.\n\n\n20:10:12.911218[I] updating database namespaces [{adds [default]} {updates []} {removals []}]\n20:10:13.462798[I] node tchannelthrift: listening on 0.0.0.0:9000\n20:10:13.463107[I] cluster tchannelthrift: listening on 0.0.0.0:9001\n20:10:13.747173[I] node httpjson: listening on 0.0.0.0:9002\n20:10:13.747506[I] cluster httpjson: listening on 0.0.0.0:9003\n20:10:13.747763[I] bootstrapping shards for range starting ...\n...\n20:10:13.757834[I] bootstrap finished [{namespace metrics} {duration 10.1261ms}]\n20:10:13.758001[I] bootstrapped\n20:10:14.764771[I] successfully updated topology to 3 hosts\n\n\n\n\n4) Repeat steps 2 and 3 until all nodes have been upgraded.",
            "title": "Upgrading M3"
        },
        {
            "location": "/operational_guide/upgrading_m3/#upgrading-m3",
            "text": "",
            "title": "Upgrading M3"
        },
        {
            "location": "/operational_guide/upgrading_m3/#overview",
            "text": "This guide explains how to upgrade M3 from one version to another (e.g. from 0.14.0 to 0.15.0).\nThis includes upgrading:   m3dbnode  m3coordinator  m3query  m3aggregator",
            "title": "Overview"
        },
        {
            "location": "/operational_guide/upgrading_m3/#m3dbnode",
            "text": "",
            "title": "m3dbnode"
        },
        {
            "location": "/operational_guide/upgrading_m3/#graphs-to-monitor",
            "text": "While upgrading M3DB nodes, it's important to monitor the status of bootstrapping the individual nodes. This can be monitored using the  M3DB Node Details  dashboard.\nTypically, the  Bootstrapped  graph under  Background Tasks  and the graphs within the  CPU and Memory Utilization  give a good understanding of how well bootstrapping is going.",
            "title": "Graphs to monitor"
        },
        {
            "location": "/operational_guide/upgrading_m3/#kubernetes",
            "text": "If running  M3DB  on Kubernetes, upgrade by completing the following steps.     Identify the version of m3dbnode to upgrade to  on Quay .    Replace the Docker image in the  StatefulSet  manifest (or  m3db-operator  manifest) to be the new version of m3dbnode.    spec : \n   image :   quay.io/m3db/m3dbnode:$VERSION    Once updated, apply the updated manifest and a rolling restart will be performed. You must wait until the  StatefulSet  is entirely upgraded and bootstrapped (as per the M3DB Node Details dashboard) before proceeding to the next  StatefulSet  otherwise multiple replicas will be unavailable at once.   kubectl apply -f <m3dbnode_manifest>",
            "title": "Kubernetes"
        },
        {
            "location": "/operational_guide/upgrading_m3/#downgrading",
            "text": "The  upgrading  steps above can also be used to downgrade M3DB. However, it is important to refer to the release notes to make sure that versions are\nbackwards compatible.",
            "title": "Downgrading"
        },
        {
            "location": "/operational_guide/upgrading_m3/#m3coordinator",
            "text": "m3coordinator  can be upgraded using similar steps as  m3dbnode , however, the images can be  found here  instead.",
            "title": "m3coordinator"
        },
        {
            "location": "/operational_guide/upgrading_m3/#m3query",
            "text": "m3query  can be upgraded using similar steps as  m3dbnode , however, the images can be  found here  instead.",
            "title": "m3query"
        },
        {
            "location": "/operational_guide/upgrading_m3/#m3aggregator",
            "text": "m3aggregator  can be upgraded using similar steps as  m3dbnode , however, the images can be  found here  instead.",
            "title": "m3aggregator"
        },
        {
            "location": "/operational_guide/upgrading_m3/#non-kubernetes",
            "text": "It is very important that for each replica set, only one node gets upgraded at a time. However, multiple nodes can be upgraded across replica sets.   1) Download new binary (linux example below).  wget  \"https://github.com/m3db/m3/releases/download/v $VERSION /m3_ $VERSION_linux_amd64 .tar.gz\"   &&  tar xvzf m3_ $VERSION_linux_amd64 .tar.gz  &&  rm m3_ $VERSION_linux_amd64 .tar.gz  2) Stop and upgrade one M3DB node at a time per replica set using the  systemd unit .  # stop m3dbnode \nsudo systemctl stop m3dbnode # start m3dbnode with the new binary (which should be placed in the path specified in the systemd unit) \nsudo systemctl start m3dbnode  Note:  If unable to stop  m3dbnode  using  systemctl , use  pkill  instead.  # stop m3dbnode \npkill m3dbnode # start m3dbnode with new binary \n./m3_ $VERSION_linux_amd64 /m3dbnode -f <config-name.yml>  3) Confirm m3dbnode has finished bootstrapping.  20:10:12.911218[I] updating database namespaces [{adds [default]} {updates []} {removals []}]\n20:10:13.462798[I] node tchannelthrift: listening on 0.0.0.0:9000\n20:10:13.463107[I] cluster tchannelthrift: listening on 0.0.0.0:9001\n20:10:13.747173[I] node httpjson: listening on 0.0.0.0:9002\n20:10:13.747506[I] cluster httpjson: listening on 0.0.0.0:9003\n20:10:13.747763[I] bootstrapping shards for range starting ...\n...\n20:10:13.757834[I] bootstrap finished [{namespace metrics} {duration 10.1261ms}]\n20:10:13.758001[I] bootstrapped\n20:10:14.764771[I] successfully updated topology to 3 hosts  4) Repeat steps 2 and 3 until all nodes have been upgraded.",
            "title": "Non-Kubernetes"
        },
        {
            "location": "/operational_guide/resource_limits/",
            "text": "Resource Limits and Preventing Abusive Reads/Writes\n\n\nThis operational guide provides an overview of how to set resource limits on \nM3 components to prevent abusive reads/writes impacting availability or \nperformance of M3 in a production environment.\n\n\nM3DB\n\n\nConfiguring limits\n\n\nThe best way to get started protecting M3DB nodes is to set a few limits on the\ntop level \nlimits\n config stanza for M3DB.\n\n\nWhen using M3DB for metrics workloads, queries arrive as a set of matchers \nthat select time series based on certain dimensions. The primary mechanism to \nprotect against these matchers matching huge amounts of data in an unbounded \nway is to set a maximum limit for the amount of time series blocks allowed to\nbe matched and consequently read in a given time window. This can be done using \n\nmaxRecentlyQueriedSeriesBlocks\n to set a maximum value and lookback time window \nto determine the duration over which the max limit is enforced.\n\n\nYou can use the Prometheus query \nrate(query_stats_total_docs_per_block[1m])\n to \ndetermine how many time series blocks are queried per second by your cluster \ntoday to determine what is a sane value to set this to. Make sure to multiply \nthat number by the \nlookback\n period to get your desired max value. For \ninstance, if the query shows that you frequently query 10,000 time series blocks \nper second safely with your deployment and you want to use the default lookback \nof \n5s\n then you would multiply 10,000 by 5 to get 50,000 as a max value with \na 5s lookback.\n\n\nAnnotated configuration\n\n\nlimits:\n  # If set, will enforce a maximum cap on time series blocks matched for\n  # queries searching time series by dimensions.\n  maxRecentlyQueriedSeriesBlocks:\n    # Value sets the maximum time series blocks matched, use your block \n    # settings to understand how many datapoints that may actually translate \n    # to (e.g. 2 hour blocks for unaggregated data with 30s scrape interval\n    # will translate to 240 datapoints per single time series block matched).\n    value: 0\n    # Lookback sets the time window that this limit is enforced over, every \n    # lookback period the global count is reset to zero and when the limit \n    # is reached it will reject any further time series blocks being matched \n    # and read until the lookback period resets.\n    lookback: 5s\n\n  # If set then will limit the number of parallel write batch requests to the \n  # database and return errors if hit.\n  maxOutstandingWriteRequests: 0\n\n  # If set then will limit the number of parallel read requests to the \n  # database and return errors if hit. \n  # Note since reads can be so variable in terms of how expensive they are\n  # it is not always very useful to use this config to prevent resource \n  # exhaustion from reads.\n  maxOutstandingReadRequests: 0\n\n\n\n\nM3 Query and M3 Coordinator\n\n\nDeployment\n\n\nProtecting queries impacting your ingestion of metrics for metrics workloads \ncan first and foremost be done by deploying M3 Query and M3 Coordinator \nindependently. That is, for writes to M3 use a dedicated deployment of \nM3 Coordinator instances, and then for queries to M3 use a dedicated deployment \nof M3 Query instances.\n\n\nThis ensures when M3 Query instances become busy and are starved of resources \nserving an unexpected query load, they will not interrupt the flow of metrics\nbeing ingested to M3.\n\n\nConfiguring limits\n\n\nTo protect against individual queries using too many resources, you can specify some\nsane limits in the M3 Query (and consequently M3 Coordinator) configuration \nfile under the top level \nlimits\n config stanza.\n\n\nThere are two types of limits:\n\n\n\n\nPer query time series limit\n\n\nPer query time series * blocks limit (docs limit)\n\n\n\n\nWhen either of these limits are hit, you can define the behavior you would like, \neither to return an error when this limit is hit, or to return a partial result \nwith the response header \nM3-Results-Limited\n detailing the limit that was hit \nand a warning included in the response body.\n\n\nAnnotated configuration\n\n\nlimits:\n  # If set will override default limits set per query.\n  perQuery:\n    # If set limits the number of time series returned for any given \n    # individual storage node per query, before returning result to query \n    # service.\n    maxFetchedSeries: 0\n\n    # If set limits the number of index documents matched for any given \n    # individual storage node per query, before returning result to query \n    # service.\n    # This equates to the number of time series * number of blocks, so for \n    # 100 time series matching 4 hours of data for a namespace using a 2 hour \n    # block size, that would result in matching 200 index documents.\n    maxFetchedDocs: 0\n\n    # If true this results in causing a query error if the query exceeds \n    # the series or blocks limit for any given individual storage node per query.\n    requireExhaustive: false\n\n    # If set this limits the max number of datapoints allowed to be used by a\n    # given query. This is applied at the query service after the result has \n    # been returned by a storage node.\n    maxFetchedDatapoints: 0\n\n  # If set will override default limits set globally.\n  global:\n    # If set this limits the max number of datapoints allowed to be used by all\n    # queries at any point in time, this is applied at the query service after \n    # the result has been returned by a storage node.\n    maxFetchedDatapoints: 0\n\n\n\n\nHeaders\n\n\nThe following headers can also be used to override configured limits on a per request basis (to allow for different limits dependent on caller):\n\n\n\n\nM3-Limit-Max-Series\n:\n\n If this header is set it will override any configured per query time series limit. If the limit is hit, it will either return a partial result or an error based on the require exhaustive configuration set.\n\n\nM3-Limit-Max-Docs\n:\n\n If this header is set it will override any configured per query time series * blocks limit (docs limit). If the limit is hit, it will either return a partial result or an error based on the require exhaustive configuration set.\n\n\nM3-Limit-Require-Exhaustive\n:\n\n If this header is set it will override any configured require exhaustive setting. If \"true\" it will return an error if query hits a configured limit (such as series or docs limit) instead of a partial result. Otherwise if \"false\" it will return a partial result of the time series already matched with the response header \nM3-Results-Limited\n detailing the limit that was hit and a warning included in the response body.",
            "title": "Resource Limits and Preventing Abusive Reads/Writes"
        },
        {
            "location": "/operational_guide/resource_limits/#resource-limits-and-preventing-abusive-readswrites",
            "text": "This operational guide provides an overview of how to set resource limits on \nM3 components to prevent abusive reads/writes impacting availability or \nperformance of M3 in a production environment.",
            "title": "Resource Limits and Preventing Abusive Reads/Writes"
        },
        {
            "location": "/operational_guide/resource_limits/#m3db",
            "text": "",
            "title": "M3DB"
        },
        {
            "location": "/operational_guide/resource_limits/#configuring-limits",
            "text": "The best way to get started protecting M3DB nodes is to set a few limits on the\ntop level  limits  config stanza for M3DB.  When using M3DB for metrics workloads, queries arrive as a set of matchers \nthat select time series based on certain dimensions. The primary mechanism to \nprotect against these matchers matching huge amounts of data in an unbounded \nway is to set a maximum limit for the amount of time series blocks allowed to\nbe matched and consequently read in a given time window. This can be done using  maxRecentlyQueriedSeriesBlocks  to set a maximum value and lookback time window \nto determine the duration over which the max limit is enforced.  You can use the Prometheus query  rate(query_stats_total_docs_per_block[1m])  to \ndetermine how many time series blocks are queried per second by your cluster \ntoday to determine what is a sane value to set this to. Make sure to multiply \nthat number by the  lookback  period to get your desired max value. For \ninstance, if the query shows that you frequently query 10,000 time series blocks \nper second safely with your deployment and you want to use the default lookback \nof  5s  then you would multiply 10,000 by 5 to get 50,000 as a max value with \na 5s lookback.",
            "title": "Configuring limits"
        },
        {
            "location": "/operational_guide/resource_limits/#annotated-configuration",
            "text": "limits:\n  # If set, will enforce a maximum cap on time series blocks matched for\n  # queries searching time series by dimensions.\n  maxRecentlyQueriedSeriesBlocks:\n    # Value sets the maximum time series blocks matched, use your block \n    # settings to understand how many datapoints that may actually translate \n    # to (e.g. 2 hour blocks for unaggregated data with 30s scrape interval\n    # will translate to 240 datapoints per single time series block matched).\n    value: 0\n    # Lookback sets the time window that this limit is enforced over, every \n    # lookback period the global count is reset to zero and when the limit \n    # is reached it will reject any further time series blocks being matched \n    # and read until the lookback period resets.\n    lookback: 5s\n\n  # If set then will limit the number of parallel write batch requests to the \n  # database and return errors if hit.\n  maxOutstandingWriteRequests: 0\n\n  # If set then will limit the number of parallel read requests to the \n  # database and return errors if hit. \n  # Note since reads can be so variable in terms of how expensive they are\n  # it is not always very useful to use this config to prevent resource \n  # exhaustion from reads.\n  maxOutstandingReadRequests: 0",
            "title": "Annotated configuration"
        },
        {
            "location": "/operational_guide/resource_limits/#m3-query-and-m3-coordinator",
            "text": "",
            "title": "M3 Query and M3 Coordinator"
        },
        {
            "location": "/operational_guide/resource_limits/#deployment",
            "text": "Protecting queries impacting your ingestion of metrics for metrics workloads \ncan first and foremost be done by deploying M3 Query and M3 Coordinator \nindependently. That is, for writes to M3 use a dedicated deployment of \nM3 Coordinator instances, and then for queries to M3 use a dedicated deployment \nof M3 Query instances.  This ensures when M3 Query instances become busy and are starved of resources \nserving an unexpected query load, they will not interrupt the flow of metrics\nbeing ingested to M3.",
            "title": "Deployment"
        },
        {
            "location": "/operational_guide/resource_limits/#configuring-limits_1",
            "text": "To protect against individual queries using too many resources, you can specify some\nsane limits in the M3 Query (and consequently M3 Coordinator) configuration \nfile under the top level  limits  config stanza.  There are two types of limits:   Per query time series limit  Per query time series * blocks limit (docs limit)   When either of these limits are hit, you can define the behavior you would like, \neither to return an error when this limit is hit, or to return a partial result \nwith the response header  M3-Results-Limited  detailing the limit that was hit \nand a warning included in the response body.",
            "title": "Configuring limits"
        },
        {
            "location": "/operational_guide/resource_limits/#annotated-configuration_1",
            "text": "limits:\n  # If set will override default limits set per query.\n  perQuery:\n    # If set limits the number of time series returned for any given \n    # individual storage node per query, before returning result to query \n    # service.\n    maxFetchedSeries: 0\n\n    # If set limits the number of index documents matched for any given \n    # individual storage node per query, before returning result to query \n    # service.\n    # This equates to the number of time series * number of blocks, so for \n    # 100 time series matching 4 hours of data for a namespace using a 2 hour \n    # block size, that would result in matching 200 index documents.\n    maxFetchedDocs: 0\n\n    # If true this results in causing a query error if the query exceeds \n    # the series or blocks limit for any given individual storage node per query.\n    requireExhaustive: false\n\n    # If set this limits the max number of datapoints allowed to be used by a\n    # given query. This is applied at the query service after the result has \n    # been returned by a storage node.\n    maxFetchedDatapoints: 0\n\n  # If set will override default limits set globally.\n  global:\n    # If set this limits the max number of datapoints allowed to be used by all\n    # queries at any point in time, this is applied at the query service after \n    # the result has been returned by a storage node.\n    maxFetchedDatapoints: 0",
            "title": "Annotated configuration"
        },
        {
            "location": "/operational_guide/resource_limits/#headers",
            "text": "The following headers can also be used to override configured limits on a per request basis (to allow for different limits dependent on caller):   M3-Limit-Max-Series : \n If this header is set it will override any configured per query time series limit. If the limit is hit, it will either return a partial result or an error based on the require exhaustive configuration set.  M3-Limit-Max-Docs : \n If this header is set it will override any configured per query time series * blocks limit (docs limit). If the limit is hit, it will either return a partial result or an error based on the require exhaustive configuration set.  M3-Limit-Require-Exhaustive : \n If this header is set it will override any configured require exhaustive setting. If \"true\" it will return an error if query hits a configured limit (such as series or docs limit) instead of a partial result. Otherwise if \"false\" it will return a partial result of the time series already matched with the response header  M3-Results-Limited  detailing the limit that was hit and a warning included in the response body.",
            "title": "Headers"
        },
        {
            "location": "/operational_guide/availability_consistency_durability/",
            "text": "Tuning Availability, Consistency, and Durability\n\n\nOverview\n\n\nM3DB is designed as a High Availability \nHA\n system because it doesn't use a consensus protocol like Raft or Paxos to enforce strong consensus and consistency guarantees.\nHowever, even within the category of HA systems, there is a broad spectrum of consistency and durability guarantees that a database can provide.\nTo address as many use cases as possible, M3DB can be tuned to achieve the desired balance between performance, availability, durability, and consistency.\n\n\nGenerally speaking, \nthe default and example configuration for M3DB\n favors performance and availability, as that is well-suited for M3DB's most common metrics and Observability use cases. To instead favor consistency and durability, consider tuning values as described in the \"Tuning for Consistency and Durability\" section.\nDatabase operators who are using M3DB for workloads that require stricter consistency and durability guarantees should consider tuning the default configuration to better suit their use case.\n\n\nThe rest of this document describes the various configuration options that are available to M3DB operators to make such tradeoffs.\nWhile reading it, we recommend referring to \nthe default configuration file\n (which has every possible configuration value set) to see how the described values fit into M3DB's configuration as a whole.\n\n\nTuning for Performance and Availability\n\n\nClient Write and Read consistency\n\n\nWe recommend running the client with \nwriteConsistencyLevel\n set to \nmajority\n and \nreadConsistencyLevel\n set to \nunstrict_majority\n.\nThis means that all write must be acknowledged by a quorums of nodes in order to be considered succesful, and that reads will attempt to achieve quorum, but will return the data from a single node if they are unable to achieve quorum. This ensures that reads will normally ensure consistency, but degraded conditions will cause reads to fail outright as long as at least a single node can satisfy the request.\n\n\nYou can read about the consistency levels in more detail in \nthe Consistency Levels section\n\n\nCommitlog Configuration\n\n\nWe recommend running M3DB with an asynchronous commitlog.\nThis means that writes will be reported as successful by the client, though the data may not have been flushed to disk yet.\n\n\nFor example, consider the default configuration:\n\n\ncommitlog:\n  flushMaxBytes: 524288\n  flushEvery: 1s\n  queue:\n    calculationType: fixed\n    size: 2097152\n\n\n\n\nThis configuration states that the commitlog should be flushed whenever either of the following is true:\n\n\n\n\n524288 or more bytes have been written since the last time M3DB flushed the commitlog.\n\n\nOne or more seconds has elapsed since the last time M3DB flushed the commitlog.\n\n\n\n\nIn addition, the configuration also states that M3DB should allow up to \n2097152\n writes to be buffered in the commitlog queue before the database node will begin rejecting incoming writes so it can attempt to drain the queue and catch up. Increasing the size of this queue can often increase the write throughput of an M3DB node at the cost of potentially losing more data if the node experiences a sudden failure like a hard crash or power loss.\n\n\nWriting New Series Asynchronously\n\n\nThe default M3DB YAML configuration will contain the following as a top-level key under the \ndb\n section:\n\n\nwriteNewSeriesAsync: true\n\n\n\n\nThis instructs M3DB to handle writes for new timeseries (for a given time block) asynchronously. Creating a new timeseries in memory is much more expensive than simply appending a new write to an existing series, so the default configuration of creating them asynchronously improves M3DBs write throughput significantly when many new series are being created all at once.\n\n\nHowever, since new time series are created asynchronously, it's possible that there may be a brief delay inbetween when a write is acknowledged by the client and when that series becomes available for subsequent reads.\n\n\nM3DB also allows operators to rate limit the number of new series that can be created per second via the following configuration:\n\n\nwriteNewSeriesLimitPerSecond: 1048576\n\n\n\n\nThis value can be set much lower than the default value for workloads in which a significant increase in cardinality usually indicates a misbehaving caller.\n\n\nIgnoring Corrupt Commitlogs on Bootstrap\n\n\nIf M3DB is shut down gracefully (i.e via SIGTERM), it will ensure that all pending writes are flushed to the commitlog on disk before the process exists.\nHowever, in situations where the process crashed/exited unexpectedly or the node itself experienced a sudden failure, the tail end of the commitlog may be corrupt.\nIn such situations, M3DB will read as much of the commitlog as possible in an attempt to recover the maximum amount of data. However, it then needs to make a decision: it can either \n(a)\n come up successfully and tolerate an ostensibly minor amount of data or loss, or \n(b)\n attempt to stream the missing data from its peers.\nThis behavior is controlled by the following default configuration:\n\n\nbootstrap:\n  commitlog:\n    returnUnfulfilledForCorruptCommitLogFiles: false\n\n\n\n\nIn the situation where only a single node fails, the optimal outcome is for the node to attempt to repair itself from one of its peers.\nHowever, if a quorum of nodes fail and encounter corrupt commitlog files, they will deadlock while attempting to stream data from each other, as no nodes will be able to make progress due to a lack of quorum.\nThis issue requires an operator with significant M3DB operational experience to manually bootstrap the cluster; thus the official recommendation is to set \nreturnUnfulfilledForCorruptCommitLogFiles: false\n to avoid this issue altogether. In most cases, a small amount of data loss is preferable to a quorum of nodes that crash and fail to start back up automatically.\n\n\nTuning for Consistency and Durability\n\n\nClient Write and Read consistency\n\n\nThe most important thing to understand is that \nif you want to guarantee that you will be able to read the result of every successful write, then both writes and reads must be done with \nmajority\n consistency.\n\nThis means that both writes \nand\n reads will fail if a quorum of nodes are unavailable for a given shard.\nYou can read about the consistency levels in more detail in \nthe Consistency Levels section\n\n\nCommitlog Configuration\n\n\nM3DB supports running the commitlog synchronously such that every write is flushed to disk and fsync'd before the client receives a successful acknowledgement, but this is not currently exposed to users in the YAML configuration and generally leads to a massive performance degradation.\nWe only recommend operating M3DB this way for workloads where data consistency and durability is strictly required, and even then there may be better alternatives such as running M3DB with the bootstrapping configuration: \nfilesystem,peers,uninitialized_topology\n as described in our \nbootstrapping operational guide\n.\n\n\nWriting New Series Asynchronously\n\n\nIf you want to guarantee that M3DB will immediately allow you to read data for writes that have been acknowledged by the client, including the situation where the previous write was for a brand new timeseries, then you  will need to change the default M3DB configuration to set \nwriteNewSeriesAsync: false\n as a top-level key under the \ndb\n section:\n\n\nwriteNewSeriesAsync: false\n\n\n\n\nThis instructs M3DB to handle writes for new timeseries (for a given time block) synchronously. Creating a new timeseries in memory is much more expensive than simply appending a new write to an existing series, so this configuration could have an adverse effect on performance when many new timeseries are being inserted into M3DB concurrently.\n\n\nSince this operation is so expensive, M3DB allows operator to rate limit the number of new series that can be created per second via the following configuration (also a top-level key under the \ndb\n section):\n\n\nwriteNewSeriesLimitPerSecond: 1048576\n\n\n\n\nIgnoring Corrupt Commitlogs on Bootstrap\n\n\nAs described in the \"Tuning for Performance and Availability\" section, we recommend configuring M3DB to ignore corrupt commitlog files on bootstrap. However, if you want to avoid any amount of inconsistency or data loss, no matter how minor, then you should configure M3DB to return unfulfilled when the commitlog bootstrapper encounters corrupt commitlog files. You can do so by modifying your configuration to look like this:\n\n\nbootstrap:\n  commitlog:\n    returnUnfulfilledForCorruptCommitLogFiles: true\n\n\n\n\nThis will force your M3DB nodes to attempt to repair corrupted commitlog files on bootstrap by streaming the data from their peers.\nIn most situations this will be transparent to the operator and the M3DB node will finish bootstrapping without trouble.\nHowever, in the scenario where a quorum of nodes for a given shard failed in unison, the nodes will deadlock while attempting to stream data from each other, as no nodes will be able to make progress due to a lack of quorum.\nThis issue requires an operator with significant M3DB operational experience to manually bootstrap the cluster; thus the official recommendation is to avoid configuring M3DB in this way unless data consistency and durability are of utmost importance.",
            "title": "Tuning Availability, Consistency, and Durability"
        },
        {
            "location": "/operational_guide/availability_consistency_durability/#tuning-availability-consistency-and-durability",
            "text": "",
            "title": "Tuning Availability, Consistency, and Durability"
        },
        {
            "location": "/operational_guide/availability_consistency_durability/#overview",
            "text": "M3DB is designed as a High Availability  HA  system because it doesn't use a consensus protocol like Raft or Paxos to enforce strong consensus and consistency guarantees.\nHowever, even within the category of HA systems, there is a broad spectrum of consistency and durability guarantees that a database can provide.\nTo address as many use cases as possible, M3DB can be tuned to achieve the desired balance between performance, availability, durability, and consistency.  Generally speaking,  the default and example configuration for M3DB  favors performance and availability, as that is well-suited for M3DB's most common metrics and Observability use cases. To instead favor consistency and durability, consider tuning values as described in the \"Tuning for Consistency and Durability\" section.\nDatabase operators who are using M3DB for workloads that require stricter consistency and durability guarantees should consider tuning the default configuration to better suit their use case.  The rest of this document describes the various configuration options that are available to M3DB operators to make such tradeoffs.\nWhile reading it, we recommend referring to  the default configuration file  (which has every possible configuration value set) to see how the described values fit into M3DB's configuration as a whole.",
            "title": "Overview"
        },
        {
            "location": "/operational_guide/availability_consistency_durability/#tuning-for-performance-and-availability",
            "text": "",
            "title": "Tuning for Performance and Availability"
        },
        {
            "location": "/operational_guide/availability_consistency_durability/#client-write-and-read-consistency",
            "text": "We recommend running the client with  writeConsistencyLevel  set to  majority  and  readConsistencyLevel  set to  unstrict_majority .\nThis means that all write must be acknowledged by a quorums of nodes in order to be considered succesful, and that reads will attempt to achieve quorum, but will return the data from a single node if they are unable to achieve quorum. This ensures that reads will normally ensure consistency, but degraded conditions will cause reads to fail outright as long as at least a single node can satisfy the request.  You can read about the consistency levels in more detail in  the Consistency Levels section",
            "title": "Client Write and Read consistency"
        },
        {
            "location": "/operational_guide/availability_consistency_durability/#commitlog-configuration",
            "text": "We recommend running M3DB with an asynchronous commitlog.\nThis means that writes will be reported as successful by the client, though the data may not have been flushed to disk yet.  For example, consider the default configuration:  commitlog:\n  flushMaxBytes: 524288\n  flushEvery: 1s\n  queue:\n    calculationType: fixed\n    size: 2097152  This configuration states that the commitlog should be flushed whenever either of the following is true:   524288 or more bytes have been written since the last time M3DB flushed the commitlog.  One or more seconds has elapsed since the last time M3DB flushed the commitlog.   In addition, the configuration also states that M3DB should allow up to  2097152  writes to be buffered in the commitlog queue before the database node will begin rejecting incoming writes so it can attempt to drain the queue and catch up. Increasing the size of this queue can often increase the write throughput of an M3DB node at the cost of potentially losing more data if the node experiences a sudden failure like a hard crash or power loss.",
            "title": "Commitlog Configuration"
        },
        {
            "location": "/operational_guide/availability_consistency_durability/#writing-new-series-asynchronously",
            "text": "The default M3DB YAML configuration will contain the following as a top-level key under the  db  section:  writeNewSeriesAsync: true  This instructs M3DB to handle writes for new timeseries (for a given time block) asynchronously. Creating a new timeseries in memory is much more expensive than simply appending a new write to an existing series, so the default configuration of creating them asynchronously improves M3DBs write throughput significantly when many new series are being created all at once.  However, since new time series are created asynchronously, it's possible that there may be a brief delay inbetween when a write is acknowledged by the client and when that series becomes available for subsequent reads.  M3DB also allows operators to rate limit the number of new series that can be created per second via the following configuration:  writeNewSeriesLimitPerSecond: 1048576  This value can be set much lower than the default value for workloads in which a significant increase in cardinality usually indicates a misbehaving caller.",
            "title": "Writing New Series Asynchronously"
        },
        {
            "location": "/operational_guide/availability_consistency_durability/#ignoring-corrupt-commitlogs-on-bootstrap",
            "text": "If M3DB is shut down gracefully (i.e via SIGTERM), it will ensure that all pending writes are flushed to the commitlog on disk before the process exists.\nHowever, in situations where the process crashed/exited unexpectedly or the node itself experienced a sudden failure, the tail end of the commitlog may be corrupt.\nIn such situations, M3DB will read as much of the commitlog as possible in an attempt to recover the maximum amount of data. However, it then needs to make a decision: it can either  (a)  come up successfully and tolerate an ostensibly minor amount of data or loss, or  (b)  attempt to stream the missing data from its peers.\nThis behavior is controlled by the following default configuration:  bootstrap:\n  commitlog:\n    returnUnfulfilledForCorruptCommitLogFiles: false  In the situation where only a single node fails, the optimal outcome is for the node to attempt to repair itself from one of its peers.\nHowever, if a quorum of nodes fail and encounter corrupt commitlog files, they will deadlock while attempting to stream data from each other, as no nodes will be able to make progress due to a lack of quorum.\nThis issue requires an operator with significant M3DB operational experience to manually bootstrap the cluster; thus the official recommendation is to set  returnUnfulfilledForCorruptCommitLogFiles: false  to avoid this issue altogether. In most cases, a small amount of data loss is preferable to a quorum of nodes that crash and fail to start back up automatically.",
            "title": "Ignoring Corrupt Commitlogs on Bootstrap"
        },
        {
            "location": "/operational_guide/availability_consistency_durability/#tuning-for-consistency-and-durability",
            "text": "",
            "title": "Tuning for Consistency and Durability"
        },
        {
            "location": "/operational_guide/availability_consistency_durability/#client-write-and-read-consistency_1",
            "text": "The most important thing to understand is that  if you want to guarantee that you will be able to read the result of every successful write, then both writes and reads must be done with  majority  consistency. \nThis means that both writes  and  reads will fail if a quorum of nodes are unavailable for a given shard.\nYou can read about the consistency levels in more detail in  the Consistency Levels section",
            "title": "Client Write and Read consistency"
        },
        {
            "location": "/operational_guide/availability_consistency_durability/#commitlog-configuration_1",
            "text": "M3DB supports running the commitlog synchronously such that every write is flushed to disk and fsync'd before the client receives a successful acknowledgement, but this is not currently exposed to users in the YAML configuration and generally leads to a massive performance degradation.\nWe only recommend operating M3DB this way for workloads where data consistency and durability is strictly required, and even then there may be better alternatives such as running M3DB with the bootstrapping configuration:  filesystem,peers,uninitialized_topology  as described in our  bootstrapping operational guide .",
            "title": "Commitlog Configuration"
        },
        {
            "location": "/operational_guide/availability_consistency_durability/#writing-new-series-asynchronously_1",
            "text": "If you want to guarantee that M3DB will immediately allow you to read data for writes that have been acknowledged by the client, including the situation where the previous write was for a brand new timeseries, then you  will need to change the default M3DB configuration to set  writeNewSeriesAsync: false  as a top-level key under the  db  section:  writeNewSeriesAsync: false  This instructs M3DB to handle writes for new timeseries (for a given time block) synchronously. Creating a new timeseries in memory is much more expensive than simply appending a new write to an existing series, so this configuration could have an adverse effect on performance when many new timeseries are being inserted into M3DB concurrently.  Since this operation is so expensive, M3DB allows operator to rate limit the number of new series that can be created per second via the following configuration (also a top-level key under the  db  section):  writeNewSeriesLimitPerSecond: 1048576",
            "title": "Writing New Series Asynchronously"
        },
        {
            "location": "/operational_guide/availability_consistency_durability/#ignoring-corrupt-commitlogs-on-bootstrap_1",
            "text": "As described in the \"Tuning for Performance and Availability\" section, we recommend configuring M3DB to ignore corrupt commitlog files on bootstrap. However, if you want to avoid any amount of inconsistency or data loss, no matter how minor, then you should configure M3DB to return unfulfilled when the commitlog bootstrapper encounters corrupt commitlog files. You can do so by modifying your configuration to look like this:  bootstrap:\n  commitlog:\n    returnUnfulfilledForCorruptCommitLogFiles: true  This will force your M3DB nodes to attempt to repair corrupted commitlog files on bootstrap by streaming the data from their peers.\nIn most situations this will be transparent to the operator and the M3DB node will finish bootstrapping without trouble.\nHowever, in the scenario where a quorum of nodes for a given shard failed in unison, the nodes will deadlock while attempting to stream data from each other, as no nodes will be able to make progress due to a lack of quorum.\nThis issue requires an operator with significant M3DB operational experience to manually bootstrap the cluster; thus the official recommendation is to avoid configuring M3DB in this way unless data consistency and durability are of utmost importance.",
            "title": "Ignoring Corrupt Commitlogs on Bootstrap"
        },
        {
            "location": "/operational_guide/placement/",
            "text": "Placement\n\n\nOverview\n\n\nNote\n: The words \nplacement\n and \ntopology\n are used interchangeably throughout the M3DB documentation and codebase.\n\n\nA M3DB cluster has exactly one Placement. That placement maps the cluster's shard replicas to nodes. A cluster also has 0 or more namespaces (analogous to tables in other databases), and each node serves every namespace for the shards it owns. In other words, if the cluster topology states that node A owns shards 1, 2, and 3 then node A will own shards 1, 2, 3 for all configured namespaces in the cluster.\n\n\nM3DB stores its placement (mapping of which NODES are responsible for which shards) in \netcd\n. There are three possible states that each node/shard pair can be in:\n\n\n\n\nInitializing\n\n\nAvailable\n\n\nLeaving\n\n\n\n\nNote that these states are not a reflection of the current status of an M3DB node, but an indication of whether a given node has ever successfully bootstrapped and taken ownership of a given shard (achieved goal state). For example, in a new cluster all the nodes will begin with all of their shards in the \nInitializing\n state. Once all the nodes finish bootstrapping, they will mark all of their shards as \nAvailable\n. If all the M3DB nodes are stopped at the same time, the cluster placement will still show all of the shards for all of the nodes as \nAvailable\n.\n\n\nInitializing State\n\n\nThe \nInitializing\n state is the state in which all new node/shard combinations begin. For example, upon creating a new placement all the node/shard pairs will begin in the \nInitializing\n state and only once they have successfully bootstrapped will they transition to the \nAvailable\n state.\n\n\nThe \nInitializing\n state is not limited to new placement, however, as it can also occur during placement changes. For example, during a node add/replace the new node will begin with all of its shards in the \nInitializing\n state until it can stream the data it is missing from its peers. During a node removal, all of the nodes who receive new shards (as a result of taking over the responsibilities of the node that is leaving) will begin with those shards marked as \nInitializing\n until they can stream in the data from the node leaving the cluster, or one of its peers.\n\n\nAvailable State\n\n\nOnce a node with a shard in the \nInitializing\n state successfully bootstraps all of the data for that shard, it will mark that shard as \nAvailable\n (for the single node) in the cluster placement.\n\n\nLeaving State\n\n\nThe \nLeaving\n state indicates that a node has been marked for removal from the cluster. The purpose of this state is to allow the node to remain in the cluster long enough for the nodes that are taking over its responsibilities to stream data from it.\n\n\nSample Cluster State Transitions - Node Add\n\n\nNode adds are performed by adding the new node to the placement. Some portion of the existing shards will be assigned to the new node based on its weight, and they will begin in the \nInitializing\n state. Similarly, the shards will be marked as \nLeaving\n on the node that are destined to lose ownership of them. Once the new node finishes bootstrapping the shards, it will update the placement to indicate that the shards it owns are \nAvailable\n and that the \nLeaving\n node should no longer own that shard in the placement.\n\n\nReplication factor: 3\n\n                                 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                 \u2502     Node A      \u2502          \u2502     Node B      \u2502        \u2502     Node C      \u2502       \u2502     Node D      \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2510\n\u2502                          \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502                         \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502                         \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502                         \u2502\n\u2502                          \u2502 \u2502   Shard 1: Available    \u2502 \u2502 \u2502  Shard 1: Available   \u2502 \u2502 \u2502  Shard 1: Available  \u2502\u2502                         \u2502\n\u2502  1) Initial Placement    \u2502 \u2502   Shard 2: Available    \u2502 \u2502 \u2502  Shard 2: Available   \u2502 \u2502 \u2502  Shard 2: Available  \u2502\u2502                         \u2502\n\u2502                          \u2502 \u2502   Shard 3: Available    \u2502 \u2502 \u2502  Shard 3: Available   \u2502 \u2502 \u2502  Shard 3: Available  \u2502\u2502                         \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502                         \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502                         \u2502\n\u2502                          \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502                         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                          \u2502                             \u2502                           \u2502                         \u2502                         \u2502\n\u2502                          \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2502    Shard 1: Leaving     \u2502 \u2502 \u2502   Shard 1: Available  \u2502 \u2502 \u2502  Shard 1: Available  \u2502\u2502\u2502Shard 1: Initializing \u2502 \u2502\n\u2502   2) Begin Node Add      \u2502 \u2502    Shard 2: Available   \u2502 \u2502 \u2502   Shard 2: Leaving    \u2502 \u2502 \u2502  Shard 2: Available  \u2502\u2502\u2502Shard 2: Initializing \u2502 \u2502\n\u2502                          \u2502 \u2502    Shard 3: Available   \u2502 \u2502 \u2502   Shard 3: Available  \u2502 \u2502 \u2502  Shard 3: Leaving    \u2502\u2502\u2502Shard 3: Initializing \u2502 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                          \u2502                             \u2502                           \u2502                         \u2502                         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                          \u2502                             \u2502                           \u2502                         \u2502                         \u2502\n\u2502                          \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2502   Shard 2: Available    \u2502 \u2502 \u2502  Shard 1: Available   \u2502 \u2502 \u2502  Shard 1: Available  \u2502\u2502\u2502  Shard 1: Available  \u2502 \u2502\n\u2502  3) Complete Node Add    \u2502 \u2502   Shard 3: Available    \u2502 \u2502 \u2502  Shard 3: Available   \u2502 \u2502 \u2502  Shard 2: Available  \u2502\u2502\u2502  Shard 2: Available  \u2502 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502\u2502  Shard 3: Available  \u2502 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                          \u2502                             \u2502                           \u2502                         \u2502                         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\n\n\nSample Cluster State Transitions - Node Remove\n\n\nNode removes are performed by updating the placement such that all the shards on the node that will be removed from the cluster are marked as \nLeaving\n and those shards are distributed to the remaining nodes (based on their weight) and assigned a state of \nInitializing\n. Once the existing nodes that are taking ownership of the leaving nodes shards finish bootstrapping, they will update the placement to indicate that the shards that they just acquired are \nAvailable\n and that the leaving node should no longer own those shards in the placement.\n\n\nReplication factor: 3\n\n                                 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                 \u2502     Node A      \u2502          \u2502     Node B      \u2502        \u2502     Node C      \u2502       \u2502     Node D      \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2510\n\u2502                          \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2502   Shard 2: Available    \u2502 \u2502 \u2502  Shard 1: Available   \u2502 \u2502 \u2502  Shard 1: Available  \u2502\u2502\u2502  Shard 1: Available  \u2502 \u2502\n\u2502  1) Initial Placement    \u2502 \u2502   Shard 3: Available    \u2502 \u2502 \u2502  Shard 3: Available   \u2502 \u2502 \u2502  Shard 2: Available  \u2502\u2502\u2502  Shard 2: Available  \u2502 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502\u2502  Shard 3: Available  \u2502 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                          \u2502                             \u2502                           \u2502                         \u2502                         \u2502\n\u2502                          \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502\u2502                       \u2502\u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502\u2502                       \u2502\u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2502   Shard 1: Initializing \u2502 \u2502 \u2502  Shard 1: Available   \u2502 \u2502\u2502  Shard 1: Available   \u2502\u2502\u2502   Shard 1: Leaving   \u2502 \u2502\n\u2502  2) Begin Node Remove    \u2502 \u2502   Shard 2: Available    \u2502 \u2502 \u2502  Shard 2: Initializing\u2502 \u2502\u2502  Shard 2: Available   \u2502\u2502\u2502   Shard 2: Leaving   \u2502 \u2502\n\u2502                          \u2502 \u2502   Shard 3: Available    \u2502 \u2502 \u2502  Shard 3: Available   \u2502 \u2502\u2502  Shard 3: Initializing\u2502\u2502\u2502   Shard 3: Leaving   \u2502 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502\u2502                       \u2502\u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502\u2502                       \u2502\u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                          \u2502                             \u2502                           \u2502                         \u2502                         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                          \u2502                             \u2502                           \u2502                         \u2502                         \u2502\n\u2502                          \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502                         \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502                         \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502                         \u2502\n\u2502                          \u2502 \u2502    Shard 1: Avaiable    \u2502 \u2502 \u2502  Shard 1: Available   \u2502 \u2502 \u2502  Shard 1: Available  \u2502\u2502                         \u2502\n\u2502  3) Complete Node Remove \u2502 \u2502   Shard 2: Available    \u2502 \u2502 \u2502  Shard 2: Available   \u2502 \u2502 \u2502  Shard 2: Available  \u2502\u2502                         \u2502\n\u2502                          \u2502 \u2502   Shard 3: Available    \u2502 \u2502 \u2502  Shard 3: Available   \u2502 \u2502 \u2502  Shard 3: Available  \u2502\u2502                         \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502                         \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502                         \u2502\n\u2502                          \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502                         \u2502\n\u2502                          \u2502                             \u2502                           \u2502                         \u2502                         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\n\n\nSample Cluster State Transitions - Node Replace\n\n\nNode replaces are performed by updating the placement such that all the shards on the node that will be removed from the cluster are marked as \nLeaving\n and those shards are all added to the node that is being added and assigned a state of \nInitializing\n. Once the replacement node finishes bootstrapping, it will update the placement to indicate that the shards that it acquired are \nAvailable\n and that the leaving node should no longer own those shards in the placement.\n\n\nReplication factor: 3\n\n                                 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                 \u2502     Node A      \u2502          \u2502     Node B      \u2502        \u2502     Node C      \u2502       \u2502     Node D      \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2510\n\u2502                          \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502                         \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502                         \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502                         \u2502\n\u2502                          \u2502 \u2502   Shard 1: Available    \u2502 \u2502 \u2502  Shard 1: Available   \u2502 \u2502 \u2502  Shard 1: Available  \u2502\u2502                         \u2502\n\u2502  1) Initial Placement    \u2502 \u2502   Shard 2: Available    \u2502 \u2502 \u2502  Shard 2: Available   \u2502 \u2502 \u2502  Shard 2: Available  \u2502\u2502                         \u2502\n\u2502                          \u2502 \u2502   Shard 3: Available    \u2502 \u2502 \u2502  Shard 3: Available   \u2502 \u2502 \u2502  Shard 3: Available  \u2502\u2502                         \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502                         \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502                         \u2502\n\u2502                          \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502                         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                          \u2502                             \u2502                           \u2502                         \u2502                         \u2502\n\u2502                          \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502\u2502                       \u2502\u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502\u2502                       \u2502\u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2502   Shard 1: Available    \u2502 \u2502 \u2502  Shard 1: Available   \u2502 \u2502\u2502   Shard 1: Leaving    \u2502\u2502\u2502Shard 1: Initializing \u2502 \u2502\n\u2502  2) Begin Node Replace   \u2502 \u2502   Shard 2: Available    \u2502 \u2502 \u2502  Shard 2: Available   \u2502 \u2502\u2502   Shard 2: Leaving    \u2502\u2502\u2502Shard 2: Initializing \u2502 \u2502\n\u2502                          \u2502 \u2502   Shard 3: Available    \u2502 \u2502 \u2502  Shard 3: Available   \u2502 \u2502\u2502   Shard 3: Leaving    \u2502\u2502\u2502Shard 3: Initializing \u2502 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502\u2502                       \u2502\u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502\u2502                       \u2502\u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                          \u2502                             \u2502                           \u2502                         \u2502                         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                          \u2502                             \u2502                           \u2502                         \u2502                         \u2502\n\u2502                          \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502                         \u2502\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502                         \u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502                         \u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2502   Shard 1: Avaiable     \u2502 \u2502 \u2502  Shard 1: Available   \u2502 \u2502                         \u2502\u2502  Shard 1: Available  \u2502 \u2502\n\u2502  3) Complete Node Replace\u2502 \u2502   Shard 2: Available    \u2502 \u2502 \u2502  Shard 2: Available   \u2502 \u2502                         \u2502\u2502  Shard 2: Available  \u2502 \u2502\n\u2502                          \u2502 \u2502   Shard 3: Available    \u2502 \u2502 \u2502  Shard 3: Available   \u2502 \u2502                         \u2502\u2502  Shard 3: Available  \u2502 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502                         \u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502                         \u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502                         \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                          \u2502                             \u2502                           \u2502                         \u2502                         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\n\n\nCluster State Transitions - Placement Updates Initiation\n\n\nThe diagram below depicts the sequence of events that happen during a node replace and illustrates which entity is performing the placement update (in etcd) at each step.\n\n\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502             Node A             \u2502\n \u2502                                \u2502\n \u2502       Shard 1: Available       \u2502\n \u2502       Shard 2: Available       \u2502     Operator performs node replace by\n \u2502       Shard 3: Available       \u2502      updating placement in etcd such\n \u2502                                \u2502     that shards on node A are marked\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524     Leaving and shards on node B are\n                                  \u2502            marked Initializing\n                                  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                                                    \u2502\n                                                                    \u2502\n                                                                    \u2502\n                                                                    \u2502\n                                                                    \u2502\n                                                                    \u25bc\n                                                   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                                   \u2502             Node A             \u2502\n                                                   \u2502                                \u2502\n                                                   \u2502        Shard 1: Leaving        \u2502\n                                                   \u2502        Shard 2: Leaving        \u2502\n                                                   \u2502        Shard 3: Leaving        \u2502\n                                                   \u2502                                \u2502\n                                                   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n                                                   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                                   \u2502             Node B             \u2502\n                                                   \u2502                                \u2502\n                                                   \u2502     Shard 1: Initializing      \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                 \u2502     Shard 2: Initializing      \u2502\n\u2502                                \u2502                 \u2502     Shard 3: Initializing      \u2502\n\u2502                                \u2502                 \u2502                                \u2502\n\u2502             Node A             \u2502                 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2502                                \u2502                                  \u2502\n\u2502                                \u2502                                  \u2502\n\u2502                                \u2502                                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                  \u2502\n                                                                    \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                  \u2502\n\u2502             Node B             \u2502                                  \u2502\n\u2502                                \u2502                                  \u2502\n\u2502       Shard 1: Available       \u2502   Node B completes bootstrapping and\n\u2502       Shard 2: Available       \u2502\u25c0\u2500\u2500\u2500\u2500updates placement (via etcd) to\n\u2502       Shard 3: Available       \u2502    indicate shard state is Available and\n\u2502                                \u2502    that Node A should no longer own any shards\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518",
            "title": "Placement/Topology"
        },
        {
            "location": "/operational_guide/placement/#placement",
            "text": "",
            "title": "Placement"
        },
        {
            "location": "/operational_guide/placement/#overview",
            "text": "Note : The words  placement  and  topology  are used interchangeably throughout the M3DB documentation and codebase.  A M3DB cluster has exactly one Placement. That placement maps the cluster's shard replicas to nodes. A cluster also has 0 or more namespaces (analogous to tables in other databases), and each node serves every namespace for the shards it owns. In other words, if the cluster topology states that node A owns shards 1, 2, and 3 then node A will own shards 1, 2, 3 for all configured namespaces in the cluster.  M3DB stores its placement (mapping of which NODES are responsible for which shards) in  etcd . There are three possible states that each node/shard pair can be in:   Initializing  Available  Leaving   Note that these states are not a reflection of the current status of an M3DB node, but an indication of whether a given node has ever successfully bootstrapped and taken ownership of a given shard (achieved goal state). For example, in a new cluster all the nodes will begin with all of their shards in the  Initializing  state. Once all the nodes finish bootstrapping, they will mark all of their shards as  Available . If all the M3DB nodes are stopped at the same time, the cluster placement will still show all of the shards for all of the nodes as  Available .",
            "title": "Overview"
        },
        {
            "location": "/operational_guide/placement/#initializing-state",
            "text": "The  Initializing  state is the state in which all new node/shard combinations begin. For example, upon creating a new placement all the node/shard pairs will begin in the  Initializing  state and only once they have successfully bootstrapped will they transition to the  Available  state.  The  Initializing  state is not limited to new placement, however, as it can also occur during placement changes. For example, during a node add/replace the new node will begin with all of its shards in the  Initializing  state until it can stream the data it is missing from its peers. During a node removal, all of the nodes who receive new shards (as a result of taking over the responsibilities of the node that is leaving) will begin with those shards marked as  Initializing  until they can stream in the data from the node leaving the cluster, or one of its peers.",
            "title": "Initializing State"
        },
        {
            "location": "/operational_guide/placement/#available-state",
            "text": "Once a node with a shard in the  Initializing  state successfully bootstraps all of the data for that shard, it will mark that shard as  Available  (for the single node) in the cluster placement.",
            "title": "Available State"
        },
        {
            "location": "/operational_guide/placement/#leaving-state",
            "text": "The  Leaving  state indicates that a node has been marked for removal from the cluster. The purpose of this state is to allow the node to remain in the cluster long enough for the nodes that are taking over its responsibilities to stream data from it.",
            "title": "Leaving State"
        },
        {
            "location": "/operational_guide/placement/#sample-cluster-state-transitions-node-add",
            "text": "Node adds are performed by adding the new node to the placement. Some portion of the existing shards will be assigned to the new node based on its weight, and they will begin in the  Initializing  state. Similarly, the shards will be marked as  Leaving  on the node that are destined to lose ownership of them. Once the new node finishes bootstrapping the shards, it will update the placement to indicate that the shards it owns are  Available  and that the  Leaving  node should no longer own that shard in the placement.  Replication factor: 3\n\n                                 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                 \u2502     Node A      \u2502          \u2502     Node B      \u2502        \u2502     Node C      \u2502       \u2502     Node D      \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2510\n\u2502                          \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502                         \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502                         \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502                         \u2502\n\u2502                          \u2502 \u2502   Shard 1: Available    \u2502 \u2502 \u2502  Shard 1: Available   \u2502 \u2502 \u2502  Shard 1: Available  \u2502\u2502                         \u2502\n\u2502  1) Initial Placement    \u2502 \u2502   Shard 2: Available    \u2502 \u2502 \u2502  Shard 2: Available   \u2502 \u2502 \u2502  Shard 2: Available  \u2502\u2502                         \u2502\n\u2502                          \u2502 \u2502   Shard 3: Available    \u2502 \u2502 \u2502  Shard 3: Available   \u2502 \u2502 \u2502  Shard 3: Available  \u2502\u2502                         \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502                         \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502                         \u2502\n\u2502                          \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502                         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                          \u2502                             \u2502                           \u2502                         \u2502                         \u2502\n\u2502                          \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2502    Shard 1: Leaving     \u2502 \u2502 \u2502   Shard 1: Available  \u2502 \u2502 \u2502  Shard 1: Available  \u2502\u2502\u2502Shard 1: Initializing \u2502 \u2502\n\u2502   2) Begin Node Add      \u2502 \u2502    Shard 2: Available   \u2502 \u2502 \u2502   Shard 2: Leaving    \u2502 \u2502 \u2502  Shard 2: Available  \u2502\u2502\u2502Shard 2: Initializing \u2502 \u2502\n\u2502                          \u2502 \u2502    Shard 3: Available   \u2502 \u2502 \u2502   Shard 3: Available  \u2502 \u2502 \u2502  Shard 3: Leaving    \u2502\u2502\u2502Shard 3: Initializing \u2502 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                          \u2502                             \u2502                           \u2502                         \u2502                         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                          \u2502                             \u2502                           \u2502                         \u2502                         \u2502\n\u2502                          \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2502   Shard 2: Available    \u2502 \u2502 \u2502  Shard 1: Available   \u2502 \u2502 \u2502  Shard 1: Available  \u2502\u2502\u2502  Shard 1: Available  \u2502 \u2502\n\u2502  3) Complete Node Add    \u2502 \u2502   Shard 3: Available    \u2502 \u2502 \u2502  Shard 3: Available   \u2502 \u2502 \u2502  Shard 2: Available  \u2502\u2502\u2502  Shard 2: Available  \u2502 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502\u2502  Shard 3: Available  \u2502 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                          \u2502                             \u2502                           \u2502                         \u2502                         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518",
            "title": "Sample Cluster State Transitions - Node Add"
        },
        {
            "location": "/operational_guide/placement/#sample-cluster-state-transitions-node-remove",
            "text": "Node removes are performed by updating the placement such that all the shards on the node that will be removed from the cluster are marked as  Leaving  and those shards are distributed to the remaining nodes (based on their weight) and assigned a state of  Initializing . Once the existing nodes that are taking ownership of the leaving nodes shards finish bootstrapping, they will update the placement to indicate that the shards that they just acquired are  Available  and that the leaving node should no longer own those shards in the placement.  Replication factor: 3\n\n                                 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                 \u2502     Node A      \u2502          \u2502     Node B      \u2502        \u2502     Node C      \u2502       \u2502     Node D      \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2510\n\u2502                          \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2502   Shard 2: Available    \u2502 \u2502 \u2502  Shard 1: Available   \u2502 \u2502 \u2502  Shard 1: Available  \u2502\u2502\u2502  Shard 1: Available  \u2502 \u2502\n\u2502  1) Initial Placement    \u2502 \u2502   Shard 3: Available    \u2502 \u2502 \u2502  Shard 3: Available   \u2502 \u2502 \u2502  Shard 2: Available  \u2502\u2502\u2502  Shard 2: Available  \u2502 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502\u2502  Shard 3: Available  \u2502 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                          \u2502                             \u2502                           \u2502                         \u2502                         \u2502\n\u2502                          \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502\u2502                       \u2502\u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502\u2502                       \u2502\u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2502   Shard 1: Initializing \u2502 \u2502 \u2502  Shard 1: Available   \u2502 \u2502\u2502  Shard 1: Available   \u2502\u2502\u2502   Shard 1: Leaving   \u2502 \u2502\n\u2502  2) Begin Node Remove    \u2502 \u2502   Shard 2: Available    \u2502 \u2502 \u2502  Shard 2: Initializing\u2502 \u2502\u2502  Shard 2: Available   \u2502\u2502\u2502   Shard 2: Leaving   \u2502 \u2502\n\u2502                          \u2502 \u2502   Shard 3: Available    \u2502 \u2502 \u2502  Shard 3: Available   \u2502 \u2502\u2502  Shard 3: Initializing\u2502\u2502\u2502   Shard 3: Leaving   \u2502 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502\u2502                       \u2502\u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502\u2502                       \u2502\u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                          \u2502                             \u2502                           \u2502                         \u2502                         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                          \u2502                             \u2502                           \u2502                         \u2502                         \u2502\n\u2502                          \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502                         \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502                         \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502                         \u2502\n\u2502                          \u2502 \u2502    Shard 1: Avaiable    \u2502 \u2502 \u2502  Shard 1: Available   \u2502 \u2502 \u2502  Shard 1: Available  \u2502\u2502                         \u2502\n\u2502  3) Complete Node Remove \u2502 \u2502   Shard 2: Available    \u2502 \u2502 \u2502  Shard 2: Available   \u2502 \u2502 \u2502  Shard 2: Available  \u2502\u2502                         \u2502\n\u2502                          \u2502 \u2502   Shard 3: Available    \u2502 \u2502 \u2502  Shard 3: Available   \u2502 \u2502 \u2502  Shard 3: Available  \u2502\u2502                         \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502                         \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502                         \u2502\n\u2502                          \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502                         \u2502\n\u2502                          \u2502                             \u2502                           \u2502                         \u2502                         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518",
            "title": "Sample Cluster State Transitions - Node Remove"
        },
        {
            "location": "/operational_guide/placement/#sample-cluster-state-transitions-node-replace",
            "text": "Node replaces are performed by updating the placement such that all the shards on the node that will be removed from the cluster are marked as  Leaving  and those shards are all added to the node that is being added and assigned a state of  Initializing . Once the replacement node finishes bootstrapping, it will update the placement to indicate that the shards that it acquired are  Available  and that the leaving node should no longer own those shards in the placement.  Replication factor: 3\n\n                                 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                 \u2502     Node A      \u2502          \u2502     Node B      \u2502        \u2502     Node C      \u2502       \u2502     Node D      \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2510\n\u2502                          \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502                         \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502                         \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502                         \u2502\n\u2502                          \u2502 \u2502   Shard 1: Available    \u2502 \u2502 \u2502  Shard 1: Available   \u2502 \u2502 \u2502  Shard 1: Available  \u2502\u2502                         \u2502\n\u2502  1) Initial Placement    \u2502 \u2502   Shard 2: Available    \u2502 \u2502 \u2502  Shard 2: Available   \u2502 \u2502 \u2502  Shard 2: Available  \u2502\u2502                         \u2502\n\u2502                          \u2502 \u2502   Shard 3: Available    \u2502 \u2502 \u2502  Shard 3: Available   \u2502 \u2502 \u2502  Shard 3: Available  \u2502\u2502                         \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502                         \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502 \u2502                      \u2502\u2502                         \u2502\n\u2502                          \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502                         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                          \u2502                             \u2502                           \u2502                         \u2502                         \u2502\n\u2502                          \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502\u2502                       \u2502\u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502\u2502                       \u2502\u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2502   Shard 1: Available    \u2502 \u2502 \u2502  Shard 1: Available   \u2502 \u2502\u2502   Shard 1: Leaving    \u2502\u2502\u2502Shard 1: Initializing \u2502 \u2502\n\u2502  2) Begin Node Replace   \u2502 \u2502   Shard 2: Available    \u2502 \u2502 \u2502  Shard 2: Available   \u2502 \u2502\u2502   Shard 2: Leaving    \u2502\u2502\u2502Shard 2: Initializing \u2502 \u2502\n\u2502                          \u2502 \u2502   Shard 3: Available    \u2502 \u2502 \u2502  Shard 3: Available   \u2502 \u2502\u2502   Shard 3: Leaving    \u2502\u2502\u2502Shard 3: Initializing \u2502 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502\u2502                       \u2502\u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502\u2502                       \u2502\u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                          \u2502                             \u2502                           \u2502                         \u2502                         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                          \u2502                             \u2502                           \u2502                         \u2502                         \u2502\n\u2502                          \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502                         \u2502\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502                         \u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502                         \u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2502   Shard 1: Avaiable     \u2502 \u2502 \u2502  Shard 1: Available   \u2502 \u2502                         \u2502\u2502  Shard 1: Available  \u2502 \u2502\n\u2502  3) Complete Node Replace\u2502 \u2502   Shard 2: Available    \u2502 \u2502 \u2502  Shard 2: Available   \u2502 \u2502                         \u2502\u2502  Shard 2: Available  \u2502 \u2502\n\u2502                          \u2502 \u2502   Shard 3: Available    \u2502 \u2502 \u2502  Shard 3: Available   \u2502 \u2502                         \u2502\u2502  Shard 3: Available  \u2502 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502                         \u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2502                         \u2502 \u2502 \u2502                       \u2502 \u2502                         \u2502\u2502                      \u2502 \u2502\n\u2502                          \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502                         \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                          \u2502                             \u2502                           \u2502                         \u2502                         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518",
            "title": "Sample Cluster State Transitions - Node Replace"
        },
        {
            "location": "/operational_guide/placement/#cluster-state-transitions-placement-updates-initiation",
            "text": "The diagram below depicts the sequence of events that happen during a node replace and illustrates which entity is performing the placement update (in etcd) at each step.   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502             Node A             \u2502\n \u2502                                \u2502\n \u2502       Shard 1: Available       \u2502\n \u2502       Shard 2: Available       \u2502     Operator performs node replace by\n \u2502       Shard 3: Available       \u2502      updating placement in etcd such\n \u2502                                \u2502     that shards on node A are marked\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524     Leaving and shards on node B are\n                                  \u2502            marked Initializing\n                                  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                                                    \u2502\n                                                                    \u2502\n                                                                    \u2502\n                                                                    \u2502\n                                                                    \u2502\n                                                                    \u25bc\n                                                   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                                   \u2502             Node A             \u2502\n                                                   \u2502                                \u2502\n                                                   \u2502        Shard 1: Leaving        \u2502\n                                                   \u2502        Shard 2: Leaving        \u2502\n                                                   \u2502        Shard 3: Leaving        \u2502\n                                                   \u2502                                \u2502\n                                                   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n                                                   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                                   \u2502             Node B             \u2502\n                                                   \u2502                                \u2502\n                                                   \u2502     Shard 1: Initializing      \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                 \u2502     Shard 2: Initializing      \u2502\n\u2502                                \u2502                 \u2502     Shard 3: Initializing      \u2502\n\u2502                                \u2502                 \u2502                                \u2502\n\u2502             Node A             \u2502                 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2502                                \u2502                                  \u2502\n\u2502                                \u2502                                  \u2502\n\u2502                                \u2502                                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                  \u2502\n                                                                    \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                  \u2502\n\u2502             Node B             \u2502                                  \u2502\n\u2502                                \u2502                                  \u2502\n\u2502       Shard 1: Available       \u2502   Node B completes bootstrapping and\n\u2502       Shard 2: Available       \u2502\u25c0\u2500\u2500\u2500\u2500updates placement (via etcd) to\n\u2502       Shard 3: Available       \u2502    indicate shard state is Available and\n\u2502                                \u2502    that Node A should no longer own any shards\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518",
            "title": "Cluster State Transitions - Placement Updates Initiation"
        },
        {
            "location": "/operational_guide/placement_configuration/",
            "text": "Placement Configuration\n\n\nOverview\n\n\nM3DB was designed from the ground up to be a distributed (clustered) database that is availability zone or rack aware (by using isolation groups). Clusters will seamlessly scale with your data, and you can start with a small number of nodes and grow it to a size of several hundred nodes with no downtime or expensive migrations.\n\n\nBefore reading the rest of this document, we recommend familiarizing yourself with the \nM3DB placement documentation\n\n\nNote\n: The primary limiting factor for the maximum size of an M3DB cluster is the number of shards. Picking an appropriate number of shards is more of an art than a science, but our recommendation is as follows:\n\n\nThe number of shards that M3DB uses is configurable and there are a couple of key points to note when deciding the number to use. The\nmore nodes you have, the more shards you want because you want the shards to be evenly distributed amongst your nodes. However,\nbecause each shard requires more files to be created, you also don\u2019t want to have too many shards per node. This is due to the fact each\nbit of data needs to be repartitioned and moved around the cluster (i.e. every bit of data needs to be moved all at once). Below are\nsome guidelines depending on how many nodes you will have in your cluster eventually - you will need to decide the number of shards up front, you\ncannot change this once the cluster is created.\n\n\n\n\n\n\n\n\nNumber of Nodes\n\n\nNumber of Shards\n\n\n\n\n\n\n\n\n\n\n3\n\n\n64\n\n\n\n\n\n\n6\n\n\n128\n\n\n\n\n\n\n12\n\n\n256\n\n\n\n\n\n\n24\n\n\n512\n\n\n\n\n\n\n48\n\n\n1024\n\n\n\n\n\n\n128+\n\n\n4096\n\n\n\n\n\n\n\n\nAfter performing any of the instructions documented below a new placement will automatically be generated to distribute the shards among the M3DB nodes such that the isolation group and replication factor constraints are met.\n\n\nIf the constraints cannot be met, because there are not enough nodes to calculate a new placement such that each shard is replicated on the desired number of nodes with none of the nodes owning the same shard existing in the same isolation group, then the operation will fail.\n\n\nIn other words, all you have to do is issue the desired instruction and the M3 stack will take care of making sure that your data is distributed with appropriate replication and isolation.\n\n\nIn the case of the M3DB nodes, nodes that have received new shards will immediately begin receiving writes (but not serving reads) for the new shards that they are responsible for. They will also begin streaming in all the data for their newly acquired shards from the peers that already have data for those shards. Once the nodes have finished streaming in the data for the shards that they have acquired, they will mark their status for those shards as \nAvailable\n in the placement and begin accepting writes. Simultaneously, the nodes that are losing ownership of any shards will mark their status for those shards as \nLeaving\n. Once all the nodes accepting ownership of the new shards have finished streaming data from them, they will relinquish ownership of those shards and remove all the data associated with the shards they lost from memory and from disk.\n\n\nM3Coordinator nodes will also pickup the new placement from etcd and alter which M3DB nodes they issue writes and reads to appropriately.\n\n\nUnderstanding the Placement Configuration\n\n\nThe placement configuration contains a few core values that control how the placement behaves.\n\n\nID\n\n\nThis is the identifier for a node in the placement and can be any value that uniquely identifies an M3DB node.\n\n\nIsolation Group\n\n\nThis value controls how nodes that own the same M3DB shards are isolated from each other. For example, in a single datacenter configuration this value could be set to the rack that the M3DB node lives on. As a result, the placement will guarantee that nodes that exist on the same rack do not share any shards, allowing the cluster to survive the failure of an entire rack. Alternatively, if M3DB was deployed in an AWS region, the isolation group could be set to the region's availability zone and that would ensure that the cluster would survive the loss of an entire availability zone.\n\n\nZone\n\n\nThis value controls what etcd zone the M3DB node belongs to.\n\n\nWeight\n\n\nThis value should be an integer and controls how the cluster will weigh the number of shards that an individual node will own. If you're running the M3DB cluster on homogenous hardware, then you probably want to assign all M3DB nodes the same weight so that shards are distributed evenly. On the otherhand, if you're running the cluster on heterogenous hardware, then this value should be higher for nodes with higher resources for whatever the limiting factor is in your cluster setup. For example, if disk space (as opposed to memory or CPU) is the limiting factor in how many shards any given node in your cluster can tolerate, then you could assign a higher value to nodes in your cluster that have larger disks and the placement calculations would assign them a higher number of shards.\n\n\nEndpoint\n\n\nThis value should be in the form of \n<M3DB_HOST_NAME>:<M3DB_NODE_LISTEN_PORT>\n and identifies how network requests should be routed to this particular node in the placement.\n\n\nHostname\n\n\nThis value should be in the form of \n<M3DB_HOST_NAME>\n and identifies the address / host name of the M3DB node.\n\n\nPort\n\n\nThis value should be in the form of \n<M3DB_NODE_LISTEN_PORT>\n and identifies the port over which this M3DB node expects to receive traffic (defaults to 9000).\n\n\nPlacement Operations\n\n\nNOTE\n: If you find yourself performing operations on seed nodes, please refer to the seed node-specific sections\nbelow before making changes.\n\n\nThe instructions below all contain sample curl commands, but you can always review the API documentation by navigating to\n\n\nhttp://<M3_COORDINATOR_HOST_NAME>:<CONFIGURED_PORT(default 7201)>/api/v1/openapi\n or our \nonline API documentation\n.\n\n\nNote\n: The \npeers bootstrapper\n must be configured on all nodes in the M3DB cluster for placement changes to work. The \npeers\n bootstrapper is enabled by default, so you only need to worry about this if you modified the default bootstrapping configuration\n\n\nAdditionally, the following headers can be used in the placement operations: \n\n\n\n\nCluster-Environment-Name\n:\n\n This header is used to specify the cluster environment name. If not set, the default \ndefault_env\n is used. \n\n\n\nCluster-Zone-Name\n:\n\n This header is used to specify the cluster zone name. If not set, the default \nembedded\n is used. \n\n\n\n\nPlacement Initialization\n\n\nSend a POST request to the \n/api/v1/services/m3db/placement/init\n endpoint\n\n\ncurl -X POST localhost:7201/api/v1/services/m3db/placement/init -d \n'{\n\n\n    \"num_shards\": <DESIRED_NUMBER_OF_SHARDS>,\n\n\n    \"replication_factor\": <DESIRED_REPLICATION_FACTOR>(recommended 3),\n\n\n    \"instances\": [\n\n\n        {\n\n\n            \"id\": \"<NODE_1_ID>\",\n\n\n            \"isolation_group\": \"<NODE_1_ISOLATION_GROUP>\",\n\n\n            \"zone\": \"<ETCD_ZONE>\",\n\n\n            \"weight\": <NODE_WEIGHT>,\n\n\n            \"endpoint\": \"<NODE_1_HOST_NAME>:<NODE_1_PORT>\",\n\n\n            \"hostname\": \"<NODE_1_HOST_NAME>\",\n\n\n            \"port\": <NODE_1_PORT>\n\n\n        },\n\n\n        {\n\n\n            \"id\": \"<NODE_2_ID>\",\n\n\n            \"isolation_group\": \"<NODE_2_ISOLATION_GROUP>\",\n\n\n            \"zone\": \"<ETCD_ZONE>\",\n\n\n            \"weight\": <NODE_WEIGHT>,\n\n\n            \"endpoint\": \"<NODE_2_HOST_NAME>:<NODE_2_PORT>\",\n\n\n            \"hostname\": \"<NODE_2_HOST_NAME>\",\n\n\n            \"port\": <NODE_2_PORT>\n\n\n        },\n\n\n        {\n\n\n            \"id\": \"<NODE_3_ID>\",\n\n\n            \"isolation_group\": \"<NODE_3_ISOLATION_GROUP>\",\n\n\n            \"zone\": \"<ETCD_ZONE>\",\n\n\n            \"weight\": <NODE_WEIGHT>,\n\n\n            \"endpoint\": \"<NODE_3_HOST_NAME>:<NODE_3_PORT>\",\n\n\n            \"hostname\": \"<NODE_3_HOST_NAME>\",\n\n\n            \"port\": <NODE_3_PORT>\n\n\n        }\n\n\n    ]\n\n\n}'\n\n\n\n\n\nAdding a Node\n\n\nSend a POST request to the \n/api/v1/services/m3db/placement\n endpoint\n\n\ncurl -X POST <M3_COORDINATOR_HOST_NAME>:<M3_COORDINATOR_PORT\n(\ndefault \n7201\n)\n>/api/v1/services/m3db/placement -d \n'{\n\n\n  \"instances\": [\n\n\n    {\n\n\n      \"id\": \"<NEW_NODE_ID>\",\n\n\n      \"isolationGroup\": \"<NEW_NODE_ISOLATION_GROUP>\",\n\n\n      \"zone\": \"<ETCD_ZONE>\",\n\n\n      \"weight\": <NODE_WEIGHT>,\n\n\n      \"endpoint\": \"<NEW_NODE_HOST_NAME>:<NEW_NODE_PORT>(default 9000)\",\n\n\n      \"hostname\": \"<NEW_NODE_HOST_NAME>\",\n\n\n      \"port\": <NEW_NODE_PORT>\n\n\n    }\n\n\n  ]\n\n\n}'\n\n\n\n\n\nAfter sending the add command you will need to wait for the M3DB cluster to reach the new desired state. You'll know that this has been achieved when the placement shows that all shards for all hosts are in the \nAvailable\n state.\n\n\nRemoving a Node\n\n\nSend a DELETE request to the \n/api/v1/services/m3db/placement/<NODE_ID>\n endpoint.\n\n\ncurl -X DELETE <M3_COORDINATOR_HOST_NAME>:<M3_COORDINATOR_PORT\n(\ndefault \n7201\n)\n>/api/v1/services/m3db/placement/<NODE_ID>\n\n\n\n\nAfter sending the delete command you will need to wait for the M3DB cluster to reach the new desired state. You'll know that this has been achieved when the placement shows that all shards for all hosts are in the \nAvailable\n state.\n\n\nAdding / Removing Seed Nodes\n\n\nIf you find yourself adding or removing etcd seed nodes then we highly recommend setting up an \nexternal etcd\n cluster, as\nthe overhead of operating two stateful systems at once is non-trivial. As this is not a recommended production setup,\nthis section is intentionally brief.\n\n\nTo add or remove nodes to the etcd cluster, use \netcdctl member add\n and \netcdctl member remove\n as found in \nReplacing\na Seed Node\n below. A general rule to keep in mind is that any time the M3DB process starts on a seed node, the list of\ncluster members in \netcdctl member list\n must match \nexactly\n the list in config.\n\n\nReplacing a Node\n\n\nNOTE\n: If using embedded etcd and replacing a seed node, please read the section below.\n\n\nSend a POST request to the \n/api/v1/services/m3db/placement/replace\n endpoint containing hosts to replace and candidates to replace it with.\n\n\ncurl -X POST <M3_COORDINATOR_HOST_NAME>:<M3_COORDINATOR_PORT\n(\ndefault \n7201\n)\n>/api/v1/services/m3db/placement/replace -d \n'{\n\n\n    \"leavingInstanceIDs\": [\"<OLD_NODE_ID>\"],\n\n\n    \"candidates\": [\n\n\n        {\n\n\n          \"id\": \"<NEW_NODE_ID>\",\n\n\n          \"isolationGroup\": \"<NEW_NODE_ISOLATION_GROUP>\",\n\n\n          \"zone\": \"<ETCD_ZONE>\",\n\n\n          \"weight\": <NODE_WEIGHT>,\n\n\n          \"endpoint\": \"<NEW_NODE_HOST_NAME>:<NEW_NODE_PORT>(default 9000)\",\n\n\n          \"hostname\": \"<NEW_NODE_HOST_NAME>\",\n\n\n          \"port\": <NEW_NODE_PORT>\n\n\n        }\n\n\n    ]\n\n\n}'\n\n\n\n\n\nReplacing a Seed Node\n\n\nIf you are using the embedded etcd mode (which is only recommended for test purposes) and replacing a seed node then\nthere are a few more steps to be done, as you are essentially doing two replace operations at once (replacing an etcd\nnode \nand\n and M3DB node). To perform these steps you will need the \netcdctl\n binary (version 3.2 or later), which can\nbe downloaded from the \netcd releases\n page.\n\n\nMany of the instructions here are mentioned in the \netcd operational\nguide\n, which we recommend\nreading for more context.\n\n\nTo provide some context for the commands below, let's assume your cluster was created with the below configuration, and\nthat you'd like to replace \nhost3\n with a new host \nhost4\n, which has IP address \n1.1.1.4\n:\n\n\ninitialCluster:\n  - hostID: host1\n    endpoint: http://1.1.1.1:2380\n  - hostID: host2\n    endpoint: http://1.1.1.2:2380\n  - hostID: host3\n    endpoint: http://1.1.1.3:2380\n\n\n\n\n\n\nOn an existing node in the cluster that is \nnot\n the one you're removing, use \netcdctl\n to remove \nhost3\n from the\n   cluster:\n\n\n\n\n$ ETCDCTL_API=3 etcdctl member list\n9d29673cf1328d1, started, host1, http://1.1.1.1:2380, http://1.1.1.1:2379\nf14613b6c8a336b, started, host2, http://1.1.1.2:2380, http://1.1.1.2:2379\n2fd477713daf243, started, host3, http://1.1.1.3:2380, http://1.1.1.3:2379  # <<< INSTANCE WE WANT TO REMOVE\n\n$ ETCDCTL_API=3 etcdctl member remove 2fd477713daf243\nRemoved member 2fd477713daf243 from cluster\n\n\n\n\n\n\nFrom the same host, use \netcdctl\n to add \nhost4\n to the cluster:\n\n\n\n\n$ ETCDCTL_API=3 etcdctl member add host4 --peer-urls http://1.1.1.4:2380\n\n\n\n\n\n\nBefore\n starting M3DB on \nhost4\n, modify the initial cluster list to indicate \nhost4\n has a cluster state of\n   \nexisting\n. Note: if you had previously started M3DB on this host, you'll have to remove the \nmember\n subdirectory in\n   \n$M3DB_DATA_DIR/etcd/\n.\n\n\n\n\ninitialCluster:\n  - hostID: host1\n    endpoint: http://1.1.1.1:2380\n  - hostID: host2\n    endpoint: http://1.1.1.2:2380\n  - hostID: host4\n    clusterState: existing\n    endpoint: http://1.1.1.4:2380\n\n\n\n\n\n\n\n\nStart M3DB on \nhost4\n.\n\n\n\n\n\n\nOn all other seed nodes, update their \ninitialCluster\n list to be exactly equal to the list on \nhost4\n from step 3.\n   Rolling restart the hosts one at a time, waiting until they indicate they are bootstrapped (indicated in the\n   \n/health\n) endpoint before continuing to the next.\n\n\n\n\n\n\nFollow the steps from \nReplacing a Seed Node\n to replace \nhost3\n with \nhost4\n in the M3DB placement.\n\n\n\n\n\n\nSetting a new placement (Not Recommended)\n\n\nThis endpoint is unsafe since it creates a brand new placement and therefore should be used with extreme caution.\nSome use cases for using this endpoint include:\n\n\n\n\nChanging IP addresses of nodes\n\n\nRebalancing shards\n\n\n\n\nIf the placement for \nM3DB\n needs to be recreated, the \n/api/v1/services/m3db/placement/set\n can be used to do so.\nPlease note, a placement already needs to exist to use this endpoint. If no placement exists, use the \nPlacement Initialization\n\nendpoint described above. Also, as mentioned above, this endpoint creates an entirely new placement therefore \ncomplete placement information needs to be passed into the body of the request. The recommended way to this\nis to get the existing placement using \n/api/v1/placement\n and modify that (as the \nplacement\n field) along \nwith two additional fields -- \nversion\n and \nconfirm\n. Please see below for a full example:\n\n\ncurl -X POST localhost:7201/api/v1/services/m3db/placement/set -d \n'{\n\n\n  \"placement\": {\n\n\n    \"num_shards\": <DESIRED_NUMBER_OF_SHARDS>,\n\n\n    \"replication_factor\": <DESIRED_REPLICATION_FACTOR>(recommended 3),\n\n\n    \"instances\": [\n\n\n        {\n\n\n            \"id\": \"<NODE_1_ID>\",\n\n\n            \"isolation_group\": \"<NODE_1_ISOLATION_GROUP>\",\n\n\n            \"zone\": \"<ETCD_ZONE>\",\n\n\n            \"weight\": <NODE_WEIGHT>,\n\n\n            \"endpoint\": \"<NODE_1_HOST_NAME>:<NODE_1_PORT>\",\n\n\n            \"hostname\": \"<NODE_1_HOST_NAME>\",\n\n\n            \"port\": <NODE_1_PORT>\n\n\n        },\n\n\n        {\n\n\n            \"id\": \"<NODE_2_ID>\",\n\n\n            \"isolation_group\": \"<NODE_2_ISOLATION_GROUP>\",\n\n\n            \"zone\": \"<ETCD_ZONE>\",\n\n\n            \"weight\": <NODE_WEIGHT>,\n\n\n            \"endpoint\": \"<NODE_2_HOST_NAME>:<NODE_2_PORT>\",\n\n\n            \"hostname\": \"<NODE_2_HOST_NAME>\",\n\n\n            \"port\": <NODE_2_PORT>\n\n\n        },\n\n\n        {\n\n\n            \"id\": \"<NODE_3_ID>\",\n\n\n            \"isolation_group\": \"<NODE_3_ISOLATION_GROUP>\",\n\n\n            \"zone\": \"<ETCD_ZONE>\",\n\n\n            \"weight\": <NODE_WEIGHT>,\n\n\n            \"endpoint\": \"<NODE_3_HOST_NAME>:<NODE_3_PORT>\",\n\n\n            \"hostname\": \"<NODE_3_HOST_NAME>\",\n\n\n            \"port\": <NODE_3_PORT>\n\n\n        }\n\n\n      ]\n\n\n    },\n\n\n    \"version\": <version>,\n\n\n    \"confirm\": <true/false>\n\n\n}'\n\n\n\n\n\nNote:\n The \nset\n endpoint can also be used to set the placements in \nM3Aggregator\n and \nM3Coordinator\n using the following endpoints, respectively:\n\n/api/v1/m3aggregator/set\n/api/v1/m3coordinator/set",
            "title": "Placement/Topology Configuration"
        },
        {
            "location": "/operational_guide/placement_configuration/#placement-configuration",
            "text": "",
            "title": "Placement Configuration"
        },
        {
            "location": "/operational_guide/placement_configuration/#overview",
            "text": "M3DB was designed from the ground up to be a distributed (clustered) database that is availability zone or rack aware (by using isolation groups). Clusters will seamlessly scale with your data, and you can start with a small number of nodes and grow it to a size of several hundred nodes with no downtime or expensive migrations.  Before reading the rest of this document, we recommend familiarizing yourself with the  M3DB placement documentation  Note : The primary limiting factor for the maximum size of an M3DB cluster is the number of shards. Picking an appropriate number of shards is more of an art than a science, but our recommendation is as follows:  The number of shards that M3DB uses is configurable and there are a couple of key points to note when deciding the number to use. The\nmore nodes you have, the more shards you want because you want the shards to be evenly distributed amongst your nodes. However,\nbecause each shard requires more files to be created, you also don\u2019t want to have too many shards per node. This is due to the fact each\nbit of data needs to be repartitioned and moved around the cluster (i.e. every bit of data needs to be moved all at once). Below are\nsome guidelines depending on how many nodes you will have in your cluster eventually - you will need to decide the number of shards up front, you\ncannot change this once the cluster is created.     Number of Nodes  Number of Shards      3  64    6  128    12  256    24  512    48  1024    128+  4096     After performing any of the instructions documented below a new placement will automatically be generated to distribute the shards among the M3DB nodes such that the isolation group and replication factor constraints are met.  If the constraints cannot be met, because there are not enough nodes to calculate a new placement such that each shard is replicated on the desired number of nodes with none of the nodes owning the same shard existing in the same isolation group, then the operation will fail.  In other words, all you have to do is issue the desired instruction and the M3 stack will take care of making sure that your data is distributed with appropriate replication and isolation.  In the case of the M3DB nodes, nodes that have received new shards will immediately begin receiving writes (but not serving reads) for the new shards that they are responsible for. They will also begin streaming in all the data for their newly acquired shards from the peers that already have data for those shards. Once the nodes have finished streaming in the data for the shards that they have acquired, they will mark their status for those shards as  Available  in the placement and begin accepting writes. Simultaneously, the nodes that are losing ownership of any shards will mark their status for those shards as  Leaving . Once all the nodes accepting ownership of the new shards have finished streaming data from them, they will relinquish ownership of those shards and remove all the data associated with the shards they lost from memory and from disk.  M3Coordinator nodes will also pickup the new placement from etcd and alter which M3DB nodes they issue writes and reads to appropriately.",
            "title": "Overview"
        },
        {
            "location": "/operational_guide/placement_configuration/#understanding-the-placement-configuration",
            "text": "The placement configuration contains a few core values that control how the placement behaves.",
            "title": "Understanding the Placement Configuration"
        },
        {
            "location": "/operational_guide/placement_configuration/#id",
            "text": "This is the identifier for a node in the placement and can be any value that uniquely identifies an M3DB node.",
            "title": "ID"
        },
        {
            "location": "/operational_guide/placement_configuration/#isolation-group",
            "text": "This value controls how nodes that own the same M3DB shards are isolated from each other. For example, in a single datacenter configuration this value could be set to the rack that the M3DB node lives on. As a result, the placement will guarantee that nodes that exist on the same rack do not share any shards, allowing the cluster to survive the failure of an entire rack. Alternatively, if M3DB was deployed in an AWS region, the isolation group could be set to the region's availability zone and that would ensure that the cluster would survive the loss of an entire availability zone.",
            "title": "Isolation Group"
        },
        {
            "location": "/operational_guide/placement_configuration/#zone",
            "text": "This value controls what etcd zone the M3DB node belongs to.",
            "title": "Zone"
        },
        {
            "location": "/operational_guide/placement_configuration/#weight",
            "text": "This value should be an integer and controls how the cluster will weigh the number of shards that an individual node will own. If you're running the M3DB cluster on homogenous hardware, then you probably want to assign all M3DB nodes the same weight so that shards are distributed evenly. On the otherhand, if you're running the cluster on heterogenous hardware, then this value should be higher for nodes with higher resources for whatever the limiting factor is in your cluster setup. For example, if disk space (as opposed to memory or CPU) is the limiting factor in how many shards any given node in your cluster can tolerate, then you could assign a higher value to nodes in your cluster that have larger disks and the placement calculations would assign them a higher number of shards.",
            "title": "Weight"
        },
        {
            "location": "/operational_guide/placement_configuration/#endpoint",
            "text": "This value should be in the form of  <M3DB_HOST_NAME>:<M3DB_NODE_LISTEN_PORT>  and identifies how network requests should be routed to this particular node in the placement.",
            "title": "Endpoint"
        },
        {
            "location": "/operational_guide/placement_configuration/#hostname",
            "text": "This value should be in the form of  <M3DB_HOST_NAME>  and identifies the address / host name of the M3DB node.",
            "title": "Hostname"
        },
        {
            "location": "/operational_guide/placement_configuration/#port",
            "text": "This value should be in the form of  <M3DB_NODE_LISTEN_PORT>  and identifies the port over which this M3DB node expects to receive traffic (defaults to 9000).",
            "title": "Port"
        },
        {
            "location": "/operational_guide/placement_configuration/#placement-operations",
            "text": "NOTE : If you find yourself performing operations on seed nodes, please refer to the seed node-specific sections\nbelow before making changes.  The instructions below all contain sample curl commands, but you can always review the API documentation by navigating to  http://<M3_COORDINATOR_HOST_NAME>:<CONFIGURED_PORT(default 7201)>/api/v1/openapi  or our  online API documentation .  Note : The  peers bootstrapper  must be configured on all nodes in the M3DB cluster for placement changes to work. The  peers  bootstrapper is enabled by default, so you only need to worry about this if you modified the default bootstrapping configuration  Additionally, the following headers can be used in the placement operations:    Cluster-Environment-Name : \n This header is used to specify the cluster environment name. If not set, the default  default_env  is used.   Cluster-Zone-Name : \n This header is used to specify the cluster zone name. If not set, the default  embedded  is used.",
            "title": "Placement Operations"
        },
        {
            "location": "/operational_guide/placement_configuration/#placement-initialization",
            "text": "Send a POST request to the  /api/v1/services/m3db/placement/init  endpoint  curl -X POST localhost:7201/api/v1/services/m3db/placement/init -d  '{      \"num_shards\": <DESIRED_NUMBER_OF_SHARDS>,      \"replication_factor\": <DESIRED_REPLICATION_FACTOR>(recommended 3),      \"instances\": [          {              \"id\": \"<NODE_1_ID>\",              \"isolation_group\": \"<NODE_1_ISOLATION_GROUP>\",              \"zone\": \"<ETCD_ZONE>\",              \"weight\": <NODE_WEIGHT>,              \"endpoint\": \"<NODE_1_HOST_NAME>:<NODE_1_PORT>\",              \"hostname\": \"<NODE_1_HOST_NAME>\",              \"port\": <NODE_1_PORT>          },          {              \"id\": \"<NODE_2_ID>\",              \"isolation_group\": \"<NODE_2_ISOLATION_GROUP>\",              \"zone\": \"<ETCD_ZONE>\",              \"weight\": <NODE_WEIGHT>,              \"endpoint\": \"<NODE_2_HOST_NAME>:<NODE_2_PORT>\",              \"hostname\": \"<NODE_2_HOST_NAME>\",              \"port\": <NODE_2_PORT>          },          {              \"id\": \"<NODE_3_ID>\",              \"isolation_group\": \"<NODE_3_ISOLATION_GROUP>\",              \"zone\": \"<ETCD_ZONE>\",              \"weight\": <NODE_WEIGHT>,              \"endpoint\": \"<NODE_3_HOST_NAME>:<NODE_3_PORT>\",              \"hostname\": \"<NODE_3_HOST_NAME>\",              \"port\": <NODE_3_PORT>          }      ]  }'",
            "title": "Placement Initialization"
        },
        {
            "location": "/operational_guide/placement_configuration/#adding-a-node",
            "text": "Send a POST request to the  /api/v1/services/m3db/placement  endpoint  curl -X POST <M3_COORDINATOR_HOST_NAME>:<M3_COORDINATOR_PORT ( default  7201 ) >/api/v1/services/m3db/placement -d  '{    \"instances\": [      {        \"id\": \"<NEW_NODE_ID>\",        \"isolationGroup\": \"<NEW_NODE_ISOLATION_GROUP>\",        \"zone\": \"<ETCD_ZONE>\",        \"weight\": <NODE_WEIGHT>,        \"endpoint\": \"<NEW_NODE_HOST_NAME>:<NEW_NODE_PORT>(default 9000)\",        \"hostname\": \"<NEW_NODE_HOST_NAME>\",        \"port\": <NEW_NODE_PORT>      }    ]  }'   After sending the add command you will need to wait for the M3DB cluster to reach the new desired state. You'll know that this has been achieved when the placement shows that all shards for all hosts are in the  Available  state.",
            "title": "Adding a Node"
        },
        {
            "location": "/operational_guide/placement_configuration/#removing-a-node",
            "text": "Send a DELETE request to the  /api/v1/services/m3db/placement/<NODE_ID>  endpoint.  curl -X DELETE <M3_COORDINATOR_HOST_NAME>:<M3_COORDINATOR_PORT ( default  7201 ) >/api/v1/services/m3db/placement/<NODE_ID>  After sending the delete command you will need to wait for the M3DB cluster to reach the new desired state. You'll know that this has been achieved when the placement shows that all shards for all hosts are in the  Available  state.",
            "title": "Removing a Node"
        },
        {
            "location": "/operational_guide/placement_configuration/#adding-removing-seed-nodes",
            "text": "If you find yourself adding or removing etcd seed nodes then we highly recommend setting up an  external etcd  cluster, as\nthe overhead of operating two stateful systems at once is non-trivial. As this is not a recommended production setup,\nthis section is intentionally brief.  To add or remove nodes to the etcd cluster, use  etcdctl member add  and  etcdctl member remove  as found in  Replacing\na Seed Node  below. A general rule to keep in mind is that any time the M3DB process starts on a seed node, the list of\ncluster members in  etcdctl member list  must match  exactly  the list in config.",
            "title": "Adding / Removing Seed Nodes"
        },
        {
            "location": "/operational_guide/placement_configuration/#replacing-a-node",
            "text": "NOTE : If using embedded etcd and replacing a seed node, please read the section below.  Send a POST request to the  /api/v1/services/m3db/placement/replace  endpoint containing hosts to replace and candidates to replace it with.  curl -X POST <M3_COORDINATOR_HOST_NAME>:<M3_COORDINATOR_PORT ( default  7201 ) >/api/v1/services/m3db/placement/replace -d  '{      \"leavingInstanceIDs\": [\"<OLD_NODE_ID>\"],      \"candidates\": [          {            \"id\": \"<NEW_NODE_ID>\",            \"isolationGroup\": \"<NEW_NODE_ISOLATION_GROUP>\",            \"zone\": \"<ETCD_ZONE>\",            \"weight\": <NODE_WEIGHT>,            \"endpoint\": \"<NEW_NODE_HOST_NAME>:<NEW_NODE_PORT>(default 9000)\",            \"hostname\": \"<NEW_NODE_HOST_NAME>\",            \"port\": <NEW_NODE_PORT>          }      ]  }'",
            "title": "Replacing a Node"
        },
        {
            "location": "/operational_guide/placement_configuration/#replacing-a-seed-node",
            "text": "If you are using the embedded etcd mode (which is only recommended for test purposes) and replacing a seed node then\nthere are a few more steps to be done, as you are essentially doing two replace operations at once (replacing an etcd\nnode  and  and M3DB node). To perform these steps you will need the  etcdctl  binary (version 3.2 or later), which can\nbe downloaded from the  etcd releases  page.  Many of the instructions here are mentioned in the  etcd operational\nguide , which we recommend\nreading for more context.  To provide some context for the commands below, let's assume your cluster was created with the below configuration, and\nthat you'd like to replace  host3  with a new host  host4 , which has IP address  1.1.1.4 :  initialCluster:\n  - hostID: host1\n    endpoint: http://1.1.1.1:2380\n  - hostID: host2\n    endpoint: http://1.1.1.2:2380\n  - hostID: host3\n    endpoint: http://1.1.1.3:2380   On an existing node in the cluster that is  not  the one you're removing, use  etcdctl  to remove  host3  from the\n   cluster:   $ ETCDCTL_API=3 etcdctl member list\n9d29673cf1328d1, started, host1, http://1.1.1.1:2380, http://1.1.1.1:2379\nf14613b6c8a336b, started, host2, http://1.1.1.2:2380, http://1.1.1.2:2379\n2fd477713daf243, started, host3, http://1.1.1.3:2380, http://1.1.1.3:2379  # <<< INSTANCE WE WANT TO REMOVE\n\n$ ETCDCTL_API=3 etcdctl member remove 2fd477713daf243\nRemoved member 2fd477713daf243 from cluster   From the same host, use  etcdctl  to add  host4  to the cluster:   $ ETCDCTL_API=3 etcdctl member add host4 --peer-urls http://1.1.1.4:2380   Before  starting M3DB on  host4 , modify the initial cluster list to indicate  host4  has a cluster state of\n    existing . Note: if you had previously started M3DB on this host, you'll have to remove the  member  subdirectory in\n    $M3DB_DATA_DIR/etcd/ .   initialCluster:\n  - hostID: host1\n    endpoint: http://1.1.1.1:2380\n  - hostID: host2\n    endpoint: http://1.1.1.2:2380\n  - hostID: host4\n    clusterState: existing\n    endpoint: http://1.1.1.4:2380    Start M3DB on  host4 .    On all other seed nodes, update their  initialCluster  list to be exactly equal to the list on  host4  from step 3.\n   Rolling restart the hosts one at a time, waiting until they indicate they are bootstrapped (indicated in the\n    /health ) endpoint before continuing to the next.    Follow the steps from  Replacing a Seed Node  to replace  host3  with  host4  in the M3DB placement.",
            "title": "Replacing a Seed Node"
        },
        {
            "location": "/operational_guide/placement_configuration/#setting-a-new-placement-not-recommended",
            "text": "This endpoint is unsafe since it creates a brand new placement and therefore should be used with extreme caution.\nSome use cases for using this endpoint include:   Changing IP addresses of nodes  Rebalancing shards   If the placement for  M3DB  needs to be recreated, the  /api/v1/services/m3db/placement/set  can be used to do so.\nPlease note, a placement already needs to exist to use this endpoint. If no placement exists, use the  Placement Initialization \nendpoint described above. Also, as mentioned above, this endpoint creates an entirely new placement therefore \ncomplete placement information needs to be passed into the body of the request. The recommended way to this\nis to get the existing placement using  /api/v1/placement  and modify that (as the  placement  field) along \nwith two additional fields --  version  and  confirm . Please see below for a full example:  curl -X POST localhost:7201/api/v1/services/m3db/placement/set -d  '{    \"placement\": {      \"num_shards\": <DESIRED_NUMBER_OF_SHARDS>,      \"replication_factor\": <DESIRED_REPLICATION_FACTOR>(recommended 3),      \"instances\": [          {              \"id\": \"<NODE_1_ID>\",              \"isolation_group\": \"<NODE_1_ISOLATION_GROUP>\",              \"zone\": \"<ETCD_ZONE>\",              \"weight\": <NODE_WEIGHT>,              \"endpoint\": \"<NODE_1_HOST_NAME>:<NODE_1_PORT>\",              \"hostname\": \"<NODE_1_HOST_NAME>\",              \"port\": <NODE_1_PORT>          },          {              \"id\": \"<NODE_2_ID>\",              \"isolation_group\": \"<NODE_2_ISOLATION_GROUP>\",              \"zone\": \"<ETCD_ZONE>\",              \"weight\": <NODE_WEIGHT>,              \"endpoint\": \"<NODE_2_HOST_NAME>:<NODE_2_PORT>\",              \"hostname\": \"<NODE_2_HOST_NAME>\",              \"port\": <NODE_2_PORT>          },          {              \"id\": \"<NODE_3_ID>\",              \"isolation_group\": \"<NODE_3_ISOLATION_GROUP>\",              \"zone\": \"<ETCD_ZONE>\",              \"weight\": <NODE_WEIGHT>,              \"endpoint\": \"<NODE_3_HOST_NAME>:<NODE_3_PORT>\",              \"hostname\": \"<NODE_3_HOST_NAME>\",              \"port\": <NODE_3_PORT>          }        ]      },      \"version\": <version>,      \"confirm\": <true/false>  }'   Note:  The  set  endpoint can also be used to set the placements in  M3Aggregator  and  M3Coordinator  using the following endpoints, respectively: /api/v1/m3aggregator/set\n/api/v1/m3coordinator/set",
            "title": "Setting a new placement (Not Recommended)"
        },
        {
            "location": "/operational_guide/namespace_configuration/",
            "text": "Namespace Configuration\n\n\nIntroduction\n\n\nNamespaces in M3DB are analogous to tables in other databases. Each namespace has a unique name as well as distinct configuration with regards to data retention and blocksize. For more information about namespaces and the technical details of their implementation, read our \nstorage engine documentation\n.\n\n\nNamespace Operations\n\n\nThe operations below include sample cURLs, but you can always review the API documentation by navigating to\n\n\nhttp://<M3_COORDINATOR_HOST_NAME>:<CONFIGURED_PORT(default 7201)>/api/v1/openapi\n or our \nonline API documentation\n.\n\n\nAdditionally, the following headers can be used in the namespace operations: \n\n\n\n\nCluster-Environment-Name\n:\n\n This header is used to specify the cluster environment name. If not set, the default \ndefault_env\n is used. \n\n\n\nCluster-Zone-Name\n:\n\n This header is used to specify the cluster zone name. If not set, the default \nembedded\n is used. \n\n\n\n\nAdding a Namespace\n\n\nRecommended (Easy way)\n\n\nThe recommended way to add a namespace to M3DB is to use our \napi/v1/database/namespace/create\n endpoint. This API abstracts over a lot of the complexity of configuring a namespace and requires only two pieces of configuration to be provided: the name of the namespace, as well as its retention.\n\n\nFor example, the following cURL:\n\n\ncurl -X POST <M3_COORDINATOR_IP_ADDRESS>:<CONFIGURED_PORT\n(\ndefault \n7201\n)\n>/api/v1/database/namespace/create -d \n'{\n\n\n  \"namespaceName\": \"default_unaggregated\",\n\n\n  \"retentionTime\": \"24h\"\n\n\n}'\n\n\n\n\n\nwill create a namespace called \ndefault_unaggregated\n with a retention of \n24 hours\n. All of the other namespace options will either use reasonable default values or be calculated based on the provided \nretentionTime\n.\n\n\nAdding a namespace does not require restarting M3DB, but will require modifying the M3Coordinator configuration to include the new namespace, and then restarting it.\n\n\nIf you feel the need to configure the namespace options yourself (for performance or other reasons), read the \nAdvanced\n section below.\n\n\nAdvanced (Hard Way)\n\n\nThe \"advanced\" API allows you to configure every aspect of the namespace that you're adding which can sometimes be helpful for development, debugging, and tuning clusters for maximum performance.\nAdding a namespace is a simple as using the \nPOST\n \napi/v1/namespace\n API on an M3Coordinator instance.\n\n\ncurl -X POST <M3_COORDINATOR_IP_ADDRESS>:<CONFIGURED_PORT(default 7201)>/api/v1/namespace -d '{\n  \"name\": \"default_unaggregated\",\n  \"options\": {\n    \"bootstrapEnabled\": true,\n    \"flushEnabled\": true,\n    \"writesToCommitLog\": true,\n    \"cleanupEnabled\": true,\n    \"snapshotEnabled\": true,\n    \"repairEnabled\": false,\n    \"retentionOptions\": {\n      \"retentionPeriod\": \"2d\",\n      \"blockSize\": \"2h\",\n      \"bufferFuture\": \"10m\",\n      \"bufferPast\": \"10m\",\n      \"blockDataExpiry\": true,\n      \"blockDataExpiryAfterNotAccessedPeriod\": \"5m\"\n    },\n    \"indexOptions\": {\n      \"enabled\": true,\n      \"blockSize\": \"2h\"\n    }\n  }\n}'\n\n\n\n\nAdding a namespace does not require restarting M3DB, but will require modifying the M3Coordinator configuration to include the new namespace, and then restarting it.\n\n\nDeleting a Namespace\n\n\nDeleting a namespace is a simple as using the \nDELETE\n \n/api/v1/namespace\n API on an M3Coordinator instance.\n\n\ncurl -X DELETE <M3_COORDINATOR_IP_ADDRESS>:<CONFIGURED_PORT(default 7201)>/api/v1/namespace/<NAMESPACE_NAME>\n\n\nNote that deleting a namespace will not have any effect on the M3DB nodes until they are all restarted. In addition, the namespace will need to be removed from the M3Coordinator configuration and then the M3Coordinator node will need to be restarted.\n\n\nModifying a Namespace\n\n\nThere is currently no atomic namespace modification endpoint. Instead, you will need to delete a namespace and then add it back again with the same name, but modified settings. Review the individual namespace settings above to determine whether or not a given setting is safe to modify. For example, it is never safe to modify the blockSize of a namespace.\n\n\nAlso, be very careful not to restart the M3DB nodes after deleting the namespace, but before adding it back. If you do this, the M3DB nodes may detect the existing data files on disk and delete them since they are not configured to retain that namespace.\n\n\nViewing a Namespace\n\n\nIn order to view a namespace and its attributes, use the \nGET\n \n/api/v1/namespace\n API on a M3Coordinator instance.\nAdditionally, for readability/debugging purposes, you can add the \ndebug=true\n parameter to the URL to view block sizes, buffer sizes, etc.\nin duration format as opposed to nanoseconds (default).\n\n\nNamespace Attributes\n\n\nbootstrapEnabled\n\n\nThis controls whether M3DB will attempt to \nbootstrap\n the namespace on startup. This value should always be set to \ntrue\n unless you have a very good reason to change it as setting it to \nfalse\n can cause data loss when restarting nodes.\n\n\nCan be modified without creating a new namespace: \nyes\n\n\nflushEnabled\n\n\nThis controls whether M3DB will periodically flush blocks to disk once they become immutable. This value should always be set to \ntrue\n unless you have a very good reason to change it as setting it to \nfalse\n will cause increased memory utilization and potential data loss when restarting nodes.\n\n\nCan be modified without creating a new namespace: \nyes\n\n\nwritesToCommitlog\n\n\nThis controls whether M3DB will includes writes to this namespace in the commitlog. This value should always be set to \ntrue\n unless you have a very good reason to change it as setting it to \nfalse\n will cause potential data loss when restarting nodes.\n\n\nCan be modified without creating a new namespace: \nyes\n\n\nsnapshotEnabled\n\n\nThis controls whether M3DB will periodically write out \nsnapshot files\n for this namespace which act as compacted commitlog files. This value should always be set to \ntrue\n unless you have a very good reason to change it as setting it to \nfalse\n will increasing bootstrapping times (reading commitlog files is slower than reading snapshot files) and increase disk utilization (snapshot files are compressed but commitlog files are uncompressed).\n\n\nCan be modified without creating a new namespace: \nyes\n\n\nrepairEnabled\n\n\nIf enabled, the M3DB nodes will attempt to compare the data they own with the data of their peers and emit metrics about any discrepancies. This feature is experimental and we do not recommend enabling it under any circumstances.\n\n\nretentionOptions\n\n\nretentionPeriod\n\n\nThis controls the duration of time that M3DB will retain data for the namespace. For example, if this is set to 30 days, then data within this namespace will be available for querying up to 30 days after it is written. Note that this retention operates at the block level, not the write level, so its possible for individual datapoints to only be available for less than the specified retention. For example, if the blockSize was set to 24 hour and the retention was set to 30 days then a write that arrived at the very end of a 24 hour block would only be available for 29 days, but the node itself would always support querying the last 30 days worth of data.\n\n\nCan be modified without creating a new namespace: \nyes\n\n\nblockSize\n\n\nThis is the most important value to consider when tuning the performance of an M3DB namespace. Read the \nstorage engine documentation\n for more details, but the basic idea is that larger blockSizes will use more memory, but achieve higher compression. Similarly, smaller blockSizes will use less memory, but have worse compression. In testing, good compression occurs with blocksizes containing around 720 samples per timeseries.\n\n\nCan be modified without creating a new namespace: \nno\n\n\nBelow are recommendations for block size based on resolution:\n\n\n\n\n\n\n\n\nResolution\n\n\nBlock Size\n\n\n\n\n\n\n\n\n\n\n5s\n\n\n60m\n\n\n\n\n\n\n15s\n\n\n3h\n\n\n\n\n\n\n30s\n\n\n6h\n\n\n\n\n\n\n1m\n\n\n12h\n\n\n\n\n\n\n5m\n\n\n60h\n\n\n\n\n\n\n\n\nbufferFuture and bufferPast\n\n\nThese values control how far into the future and the past (compared to the system time on an M3DB node) writes for the namespace will be accepted. For example, consider the following configuration:\n\n\nbufferPast: 10m\nbufferFuture: 20m\ncurrentSystemTime: 2:35:00PM\n\n\n\n\nNow consider the following writes (all of which arrive at 2:35:00PM system time, but include datapoints with the specified timestamps):\n\n\n2:25:00PM - Accepted, within the 10m bufferPast\n\n2:24:59PM - Rejected, outside the 10m bufferPast\n\n2:55:00PM - Accepted, within the 20m bufferFuture\n\n2:55:01PM - Rejected, outside the 20m bufferFuture\n\n\n\n\nWhile it may be tempting to configure \nbufferPast\n and \nbufferFuture\n to very large values to prevent writes from being rejected, this may cause performance issues. M3DB is a timeseries database that is optimized for realtime data. Out of order writes, as well as writes for times that are very far into the future or past are much more expensive and will cause additional CPU / memory pressure. In addition, M3DB cannot evict a block from memory until it is no longer mutable and large \nbufferPast\n and \nbufferFuture\n values effectively increase the amount of time that a block is mutable for which means that it must be kept in memory for a longer period of time.\n\n\nCan be modified without creating a new namespace: \nyes\n\n\nIndex Options\n\n\nenabled\n\n\nWhether to use the built-in indexing. Must be \ntrue\n.\n\n\nCan be modified without creating a new namespace: \nno\n\n\nblockSize\n\n\nThe size of blocks (in duration) that the index uses.\nShould match the databases \nblocksize\n for optimal memory usage.\n\n\nCan be modified without creating a new namespace: \nno",
            "title": "Namespace Configuration"
        },
        {
            "location": "/operational_guide/namespace_configuration/#namespace-configuration",
            "text": "",
            "title": "Namespace Configuration"
        },
        {
            "location": "/operational_guide/namespace_configuration/#introduction",
            "text": "Namespaces in M3DB are analogous to tables in other databases. Each namespace has a unique name as well as distinct configuration with regards to data retention and blocksize. For more information about namespaces and the technical details of their implementation, read our  storage engine documentation .",
            "title": "Introduction"
        },
        {
            "location": "/operational_guide/namespace_configuration/#namespace-operations",
            "text": "The operations below include sample cURLs, but you can always review the API documentation by navigating to  http://<M3_COORDINATOR_HOST_NAME>:<CONFIGURED_PORT(default 7201)>/api/v1/openapi  or our  online API documentation .  Additionally, the following headers can be used in the namespace operations:    Cluster-Environment-Name : \n This header is used to specify the cluster environment name. If not set, the default  default_env  is used.   Cluster-Zone-Name : \n This header is used to specify the cluster zone name. If not set, the default  embedded  is used.",
            "title": "Namespace Operations"
        },
        {
            "location": "/operational_guide/namespace_configuration/#adding-a-namespace",
            "text": "",
            "title": "Adding a Namespace"
        },
        {
            "location": "/operational_guide/namespace_configuration/#recommended-easy-way",
            "text": "The recommended way to add a namespace to M3DB is to use our  api/v1/database/namespace/create  endpoint. This API abstracts over a lot of the complexity of configuring a namespace and requires only two pieces of configuration to be provided: the name of the namespace, as well as its retention.  For example, the following cURL:  curl -X POST <M3_COORDINATOR_IP_ADDRESS>:<CONFIGURED_PORT ( default  7201 ) >/api/v1/database/namespace/create -d  '{    \"namespaceName\": \"default_unaggregated\",    \"retentionTime\": \"24h\"  }'   will create a namespace called  default_unaggregated  with a retention of  24 hours . All of the other namespace options will either use reasonable default values or be calculated based on the provided  retentionTime .  Adding a namespace does not require restarting M3DB, but will require modifying the M3Coordinator configuration to include the new namespace, and then restarting it.  If you feel the need to configure the namespace options yourself (for performance or other reasons), read the  Advanced  section below.",
            "title": "Recommended (Easy way)"
        },
        {
            "location": "/operational_guide/namespace_configuration/#advanced-hard-way",
            "text": "The \"advanced\" API allows you to configure every aspect of the namespace that you're adding which can sometimes be helpful for development, debugging, and tuning clusters for maximum performance.\nAdding a namespace is a simple as using the  POST   api/v1/namespace  API on an M3Coordinator instance.  curl -X POST <M3_COORDINATOR_IP_ADDRESS>:<CONFIGURED_PORT(default 7201)>/api/v1/namespace -d '{\n  \"name\": \"default_unaggregated\",\n  \"options\": {\n    \"bootstrapEnabled\": true,\n    \"flushEnabled\": true,\n    \"writesToCommitLog\": true,\n    \"cleanupEnabled\": true,\n    \"snapshotEnabled\": true,\n    \"repairEnabled\": false,\n    \"retentionOptions\": {\n      \"retentionPeriod\": \"2d\",\n      \"blockSize\": \"2h\",\n      \"bufferFuture\": \"10m\",\n      \"bufferPast\": \"10m\",\n      \"blockDataExpiry\": true,\n      \"blockDataExpiryAfterNotAccessedPeriod\": \"5m\"\n    },\n    \"indexOptions\": {\n      \"enabled\": true,\n      \"blockSize\": \"2h\"\n    }\n  }\n}'  Adding a namespace does not require restarting M3DB, but will require modifying the M3Coordinator configuration to include the new namespace, and then restarting it.",
            "title": "Advanced (Hard Way)"
        },
        {
            "location": "/operational_guide/namespace_configuration/#deleting-a-namespace",
            "text": "Deleting a namespace is a simple as using the  DELETE   /api/v1/namespace  API on an M3Coordinator instance.  curl -X DELETE <M3_COORDINATOR_IP_ADDRESS>:<CONFIGURED_PORT(default 7201)>/api/v1/namespace/<NAMESPACE_NAME>  Note that deleting a namespace will not have any effect on the M3DB nodes until they are all restarted. In addition, the namespace will need to be removed from the M3Coordinator configuration and then the M3Coordinator node will need to be restarted.",
            "title": "Deleting a Namespace"
        },
        {
            "location": "/operational_guide/namespace_configuration/#modifying-a-namespace",
            "text": "There is currently no atomic namespace modification endpoint. Instead, you will need to delete a namespace and then add it back again with the same name, but modified settings. Review the individual namespace settings above to determine whether or not a given setting is safe to modify. For example, it is never safe to modify the blockSize of a namespace.  Also, be very careful not to restart the M3DB nodes after deleting the namespace, but before adding it back. If you do this, the M3DB nodes may detect the existing data files on disk and delete them since they are not configured to retain that namespace.",
            "title": "Modifying a Namespace"
        },
        {
            "location": "/operational_guide/namespace_configuration/#viewing-a-namespace",
            "text": "In order to view a namespace and its attributes, use the  GET   /api/v1/namespace  API on a M3Coordinator instance.\nAdditionally, for readability/debugging purposes, you can add the  debug=true  parameter to the URL to view block sizes, buffer sizes, etc.\nin duration format as opposed to nanoseconds (default).",
            "title": "Viewing a Namespace"
        },
        {
            "location": "/operational_guide/namespace_configuration/#namespace-attributes",
            "text": "",
            "title": "Namespace Attributes"
        },
        {
            "location": "/operational_guide/namespace_configuration/#bootstrapenabled",
            "text": "This controls whether M3DB will attempt to  bootstrap  the namespace on startup. This value should always be set to  true  unless you have a very good reason to change it as setting it to  false  can cause data loss when restarting nodes.  Can be modified without creating a new namespace:  yes",
            "title": "bootstrapEnabled"
        },
        {
            "location": "/operational_guide/namespace_configuration/#flushenabled",
            "text": "This controls whether M3DB will periodically flush blocks to disk once they become immutable. This value should always be set to  true  unless you have a very good reason to change it as setting it to  false  will cause increased memory utilization and potential data loss when restarting nodes.  Can be modified without creating a new namespace:  yes",
            "title": "flushEnabled"
        },
        {
            "location": "/operational_guide/namespace_configuration/#writestocommitlog",
            "text": "This controls whether M3DB will includes writes to this namespace in the commitlog. This value should always be set to  true  unless you have a very good reason to change it as setting it to  false  will cause potential data loss when restarting nodes.  Can be modified without creating a new namespace:  yes",
            "title": "writesToCommitlog"
        },
        {
            "location": "/operational_guide/namespace_configuration/#snapshotenabled",
            "text": "This controls whether M3DB will periodically write out  snapshot files  for this namespace which act as compacted commitlog files. This value should always be set to  true  unless you have a very good reason to change it as setting it to  false  will increasing bootstrapping times (reading commitlog files is slower than reading snapshot files) and increase disk utilization (snapshot files are compressed but commitlog files are uncompressed).  Can be modified without creating a new namespace:  yes",
            "title": "snapshotEnabled"
        },
        {
            "location": "/operational_guide/namespace_configuration/#repairenabled",
            "text": "If enabled, the M3DB nodes will attempt to compare the data they own with the data of their peers and emit metrics about any discrepancies. This feature is experimental and we do not recommend enabling it under any circumstances.",
            "title": "repairEnabled"
        },
        {
            "location": "/operational_guide/namespace_configuration/#retentionoptions",
            "text": "",
            "title": "retentionOptions"
        },
        {
            "location": "/operational_guide/namespace_configuration/#retentionperiod",
            "text": "This controls the duration of time that M3DB will retain data for the namespace. For example, if this is set to 30 days, then data within this namespace will be available for querying up to 30 days after it is written. Note that this retention operates at the block level, not the write level, so its possible for individual datapoints to only be available for less than the specified retention. For example, if the blockSize was set to 24 hour and the retention was set to 30 days then a write that arrived at the very end of a 24 hour block would only be available for 29 days, but the node itself would always support querying the last 30 days worth of data.  Can be modified without creating a new namespace:  yes",
            "title": "retentionPeriod"
        },
        {
            "location": "/operational_guide/namespace_configuration/#blocksize",
            "text": "This is the most important value to consider when tuning the performance of an M3DB namespace. Read the  storage engine documentation  for more details, but the basic idea is that larger blockSizes will use more memory, but achieve higher compression. Similarly, smaller blockSizes will use less memory, but have worse compression. In testing, good compression occurs with blocksizes containing around 720 samples per timeseries.  Can be modified without creating a new namespace:  no  Below are recommendations for block size based on resolution:     Resolution  Block Size      5s  60m    15s  3h    30s  6h    1m  12h    5m  60h",
            "title": "blockSize"
        },
        {
            "location": "/operational_guide/namespace_configuration/#bufferfuture-and-bufferpast",
            "text": "These values control how far into the future and the past (compared to the system time on an M3DB node) writes for the namespace will be accepted. For example, consider the following configuration:  bufferPast: 10m\nbufferFuture: 20m\ncurrentSystemTime: 2:35:00PM  Now consider the following writes (all of which arrive at 2:35:00PM system time, but include datapoints with the specified timestamps):  2:25:00PM - Accepted, within the 10m bufferPast\n\n2:24:59PM - Rejected, outside the 10m bufferPast\n\n2:55:00PM - Accepted, within the 20m bufferFuture\n\n2:55:01PM - Rejected, outside the 20m bufferFuture  While it may be tempting to configure  bufferPast  and  bufferFuture  to very large values to prevent writes from being rejected, this may cause performance issues. M3DB is a timeseries database that is optimized for realtime data. Out of order writes, as well as writes for times that are very far into the future or past are much more expensive and will cause additional CPU / memory pressure. In addition, M3DB cannot evict a block from memory until it is no longer mutable and large  bufferPast  and  bufferFuture  values effectively increase the amount of time that a block is mutable for which means that it must be kept in memory for a longer period of time.  Can be modified without creating a new namespace:  yes",
            "title": "bufferFuture and bufferPast"
        },
        {
            "location": "/operational_guide/namespace_configuration/#index-options",
            "text": "",
            "title": "Index Options"
        },
        {
            "location": "/operational_guide/namespace_configuration/#enabled",
            "text": "Whether to use the built-in indexing. Must be  true .  Can be modified without creating a new namespace:  no",
            "title": "enabled"
        },
        {
            "location": "/operational_guide/namespace_configuration/#blocksize_1",
            "text": "The size of blocks (in duration) that the index uses.\nShould match the databases  blocksize  for optimal memory usage.  Can be modified without creating a new namespace:  no",
            "title": "blockSize"
        },
        {
            "location": "/operational_guide/bootstrapping_crash_recovery/",
            "text": "Bootstrapping & Crash Recovery\n\n\nIntroduction\n\n\nWe recommend reading the \nplacement operational guide\n before reading the rest of this document.\n\n\nWhen an M3DB node is turned on (goes through a placement change) it needs to go through a bootstrapping process to determine the integrity of data that it has, replay writes from the commit log, and/or stream missing data from its peers. In most cases, as long as you're running with the default and recommended bootstrapper configuration of: \nfilesystem,commitlog,peers,uninitialized_topology\n then you should not need to worry about the bootstrapping process at all and M3DB will take care of doing the right thing such that you don't lose data and consistency guarantees are met. Note that the order of the configured bootstrappers \ndoes\n matter.\n\n\nGenerally speaking, we recommend that operators do not modify the bootstrappers configuration, but in the rare case that you to, this document is designed to help you understand the implications of doing so.\n\n\nM3DB currently supports 5 different bootstrappers:\n\n\n\n\nfilesystem\n\n\ncommitlog\n\n\npeers\n\n\nuninitialized_topology\n\n\nnoop-all\n\n\n\n\nWhen the bootstrapping process begins, M3DB nodes need to determine two things:\n\n\n\n\nWhat shards the bootstrapping node should bootstrap, which can be determined from the cluster placement.\n\n\nWhat time-ranges the bootstrapping node needs to bootstrap those shards for, which can be determined from the namespace retention.\n\n\n\n\nFor example, imagine a M3DB node that is responsible for shards 1, 5, 13, and 25 according to the cluster placement. In addition, it has a single namespace called \"metrics\" with a retention of 48 hours. When the M3DB node is started, the node will determine that it needs to bootstrap shards 1, 5, 13, and 25 for the time range starting at the current time and ending 48 hours ago. In order to obtain all this data, it will run the configured bootstrappers in the specified order. Every bootstrapper will notify the bootstrapping process of which shard/ranges it was able to bootstrap and the bootstrapping process will continue working its way through the list of bootstrappers until all the shards/ranges required have been marked as fulfilled. Otherwise the M3DB node will fail to start.\n\n\nBootstrappers\n\n\nFilesystem Bootstrapper\n\n\nThe \nfilesystem\n bootstrapper's responsibility is to determine which immutable \nFileset files\n exist on disk, and if so, mark them as fulfilled. The \nfilesystem\n bootstrapper achieves this by scanning M3DB's directory structure and determining which Fileset files exist on disk. Unlike the other bootstrappers, the \nfilesystem\n bootstrapper does not need to load any data into memory, it simply verifies the checksums of the data on disk and other components of the M3DB node will handle reading (and caching) the data dynamically once it begins to serve reads.\n\n\nCommitlog Bootstrapper\n\n\nThe \ncommitlog\n bootstrapper's responsibility is to read the commitlog and snapshot (compacted commitlogs) files on disk and recover any data that has not yet been written out as an immutable Fileset file. Unlike the \nfilesystem\n bootstrapper, the commit log bootstrapper cannot simply check which files are on disk in order to determine if it can satisfy a bootstrap request. Instead, the \ncommitlog\n bootstrapper determines whether it can satisfy a bootstrap request using a simple heuristic.\n\n\nOn a shard-by-shard basis, the \ncommitlog\n bootstrapper will consult the cluster placement to see if the node it is running on has ever achieved the \nAvailable\n status for the specified shard. If so, then the commit log bootstrapper should have all the data since the last Fileset file was flushed and will return that it can satisfy any time range for that shard. In other words, the commit log bootstrapper is all-or-nothing for a given shard: it will either return that it can satisfy any time range for a given shard or none at all. In addition, the \ncommitlog\n bootstrapper \nassumes\n it is running after the \nfilesystem\n bootstrapper. M3DB will not allow you to run with a configuration where the \nfilesystem\n bootstrapper is placed after the \ncommitlog\n bootstrapper, but it will allow you to run the \ncommitlog\n bootstrapper without the \nfilesystem\n bootstrapper which can result in loss of data, depending on the workload.\n\n\nPeers Bootstrapper\n\n\nThe \npeers\n bootstrapper's responsibility is to stream in data for shard/ranges from other M3DB nodes (peers) in the cluster. This bootstrapper is only useful in M3DB clusters with more than a single node \nand\n where the replication factor is set to a value larger than 1. The \npeers\n bootstrapper will determine whether or not it can satisfy a bootstrap request on a shard-by-shard basis by consulting the cluster placement and determining if there are enough peers to satisfy the bootstrap request. For example, imagine the following M3DB placement where node A is trying to perform a peer bootstrap:\n\n\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502     Node A      \u2502          \u2502     Node B      \u2502        \u2502     Node C      \u2502\n\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                         \u2502   \u2502                       \u2502   \u2502                      \u2502\n\u2502                         \u2502   \u2502                       \u2502   \u2502                      \u2502\n\u2502  Shard 1: Initializing  \u2502   \u2502 Shard 1: Initializing \u2502   \u2502  Shard 1: Available  \u2502\n\u2502  Shard 2: Initializing  \u2502   \u2502 Shard 2: Initializing \u2502   \u2502  Shard 2: Available  \u2502\n\u2502  Shard 3: Initializing  \u2502   \u2502 Shard 3: Initializing \u2502   \u2502  Shard 3: Available  \u2502\n\u2502                         \u2502   \u2502                       \u2502   \u2502                      \u2502\n\u2502                         \u2502   \u2502                       \u2502   \u2502                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\n\n\nIn this case, the \npeers\n bootstrapper running on node A will not be able to fullfill any requests because node B is in the \nInitializing\n state for all of its shards and cannot fulfill bootstrap requests. This means that node A's \npeers\n bootstrapper cannot meet its default consistency level of majority for bootstrapping (1 < 2 which is majority with a replication factor of 3). On the other hand, node A would be able to peer bootstrap its shards in the following placement because its peers (nodes B/C) have sufficient replicas of the shards it needs in the \nAvailable\n state:\n\n\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502     Node A      \u2502          \u2502     Node B      \u2502        \u2502     Node C      \u2502\n\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                         \u2502   \u2502                       \u2502   \u2502                      \u2502\n\u2502                         \u2502   \u2502                       \u2502   \u2502                      \u2502\n\u2502  Shard 1: Initializing  \u2502   \u2502 Shard 1: Available    \u2502   \u2502  Shard 1: Available  \u2502\n\u2502  Shard 2: Initializing  \u2502   \u2502 Shard 2: Available    \u2502   \u2502  Shard 2: Available  \u2502\n\u2502  Shard 3: Initializing  \u2502   \u2502 Shard 3: Available    \u2502   \u2502  Shard 3: Available  \u2502\n\u2502                         \u2502   \u2502                       \u2502   \u2502                      \u2502\n\u2502                         \u2502   \u2502                       \u2502   \u2502                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\n\n\nNote that a bootstrap consistency level of \nmajority\n is the default value, but can be modified by changing the value of the key \nm3db.client.bootstrap-consistency-level\n in \netcd\n to one of: \nnone\n, \none\n, \nunstrict_majority\n (attempt to read from majority, but settle for less if any errors occur), \nmajority\n (strict majority), and \nall\n. For example, if an entire cluster with a replication factor of 3 was restarted simultaneously, all the nodes would get stuck in an infinite loop trying to peer bootstrap from each other and not achieving majority until an operator modified this value. Note that this can happen even if all the shards were in the \nAvailable\n state because M3DB nodes will reject all read requests for a shard until they have bootstrapped that shard (which has to happen everytime the node is restarted).\n\n\nNote\n: Any bootstrappers configuration that does not include the \npeers\n bootstrapper will be unable to handle dynamic placement changes of any kind.\n\n\nUninitialized Topology Bootstrapper\n\n\nThe purpose of the \nuninitialized_topology\n bootstrapper is to succeed bootstraps for all time ranges for shards that have never been completely bootstrapped (at a cluster level). This allows us to run the default bootstrapper configuration of: \nfilesystem,commitlog,peers,uninitialized_topology\n such that the \nfilesystem\n and \ncommitlog\n bootstrappers are used by default in node restarts, the \npeers\n bootstrapper is used for node adds/removes/replaces, and bootstraps still succeed for brand new placement where both the \ncommitlog\n and \npeers\n bootstrappers will be unable to succeed any bootstraps. In other words, the \nuninitialized_topology\n bootstrapper allows us to place the \ncommitlog\n bootstrapper \nbefore\n the \npeers\n bootstrapper and still succeed bootstraps with brand new placements without resorting to using the noop-all bootstrapper which suceeds bootstraps for all shard/time-ranges regardless of the status of the placement.\n\n\nThe \nuninitialized_topology\n bootstrapper determines whether a placement is \"new\" for a given shard by counting the number of nodes in the \nInitializing\n state and \nLeaving\n states and there are more \nInitializing\n than \nLeaving\n, then it succeeds the bootstrap because that means the placement has never reached a state where all nodes are \nAvailable\n.\n\n\nNo Operational All Bootstrapper\n\n\nThe \nnoop-all\n bootstrapper succeeds all bootstraps regardless of requests shards/time ranges.\n\n\nBootstrappers Configuration\n\n\nNow that we've gone over the various bootstrappers, let's consider how M3DB will behave in different configurations. Note that we include \nuninitialized_topology\n at the end of all the lists of bootstrappers because its required to get a new placement up and running in the first place, but is not required after that (although leaving it in has no detrimental effects). Also note that any configuration that does not include the \npeers\n bootstrapper will not be able to handle dynamic placement changes like node adds/removes/replaces.\n\n\nfilesystem,commitlog,peers,uninitialized_topology (default)\n\n\nThis is the default bootstrappers configuration for M3DB and will behave \"as expected\" in the sense that it will maintain M3DB's consistency guarantees at all times, handle node adds/replaces/removes correctly, and still work with brand new placements / topologies. \nThis is the only configuration that we recommend using in production\n.\n\n\nIn the general case, the node will use only the \nfilesystem\n and \ncommitlog\n bootstrappers on node startup. However, in the case of a node add/remove/replace, the \ncommitlog\n bootstrapper will detect that it is unable to fulfill the bootstrap request (because the node has never reached the \nAvailable\n state) and defer to the \npeers\n bootstrapper to stream in the data.\n\n\nAdditionally, if it is a brand new placement where even the \npeers\n bootstrapper cannot fulfill the bootstrap, this will be detected by the \nuninitialized_topology\n bootstrapper which will succeed the bootstrap.\n\n\nfilesystem,peers,uninitialized_topology\n\n\nEverytime a node is restarted it will attempt to stream in all of the the data for any blocks that it has never flushed, which is generally the currently active block and possibly the previous block as well. This mode can be useful if you want to improve performance or save disk space by operating nodes without a commitlog, or want to force a repair of any unflushed blocks. This mode can lead to violations of M3DB's consistency guarantees due to the fact that commit logs are being ignored. In addition, if you lose a replication factors worth or more of hosts at the same time, the node will not be able to bootstrap unless an operator modifies the bootstrap consistency level configuration in etcd (see \npeers\n bootstrap section above). Finally, this mode adds additional network and resource pressure on other nodes in the cluster while one node is peer bootstrapping from them which can be problematic in catastrophic scenarios where all the nodes are trying to stream data from each other.\n\n\npeers,uninitialized_topology\n\n\nEvery time a node is restarted, it will attempt to stream in \nall\n of the data that it is responsible for from its peers, completely ignoring the immutable Fileset files it already has on disk. This mode can be useful if you want to improve performance or save disk space by operating nodes without a commitlog, or want to force a repair of all data on an individual node. This mode can lead to violations of M3DB's consistency guarantees due to the fact that the commit logs are being ignored. In addition, if you lose a replication factors worth or more of hosts at the same time, the node will not be able to bootstrap unless an operator modifies the bootstrap consistency level configuration in etcd (see \npeers\n bootstrap section above). Finally, this mode adds additional network and resource pressure on other nodes in the cluster while one node is peer bootstrapping from them which can be problematic in catastrophic scenarios where all the nodes are trying to stream data from each other.\n\n\nInvalid bootstrappers configuration\n\n\nFor the sake of completeness, we've included a short discussion below of some bootstrapping configurations that we consider \"invalid\" in that they are likely to lose data / violate M3DB's consistency guarantees and/or not handle placement changes in a correct way.\n\n\nfilesystem,commitlog,uninitialized_topology\n\n\nThis bootstrapping configuration will work just fine if nodes are never added/replaced/removed, but will fail when attempting a node add/replace/remove.\n\n\nfilesystem,uninitialized_topology\n\n\nEvery time a node is restarted it will utilize the immutable Fileset files its already written out to disk, but any data that it had received since it wrote out the last set of immutable files will be lost.\n\n\ncommitlog,uninitialized_topology\n\n\nEvery time a node is restarted it will read all the commit log and snapshot files it has on disk, but it will ignore all the data in the immutable Fileset files that it has already written.\n\n\nCrash Recovery\n\n\nNOTE:\n These steps should not be necessary in most cases, especially if using the default bootstrappers configuration\nof \nfilesystem,commitlog,peers,uninitialized_topology\n. However in the case the configuration is non-default or the\ncluster has been down for a prolonged period of time these steps may be necessary. A good indicator would be log\nmessages related to failing to bootstrap from peers due to consistency issues.\n\n\nM3DB may require manual intervention to recover in the event of a prolonged loss of quorum. This is because the \nPeers\nBoostrapper\n must read from a majority of nodes owning a shard to bootstrap.\n\n\nTo relax this bootstrapping constraint, a value stored in etcd must be modified that corresponds to the\n\nm3db.client.bootstrap-consistency-level\n runtime flag. Until the coordinator\n\nsupports\n an API for this, this must be done manually. The M3 contributors are\naware of how cumbersome this is and are working on this API.\n\n\nTo update this value in etcd, first determine the environment the M3DB node is using. For example in \nthis\n\nconfiguration, it is \ndefault_env\n. If using the M3DB Operator, the value will be \n$KUBE_NAMESPACE/$CLUSTER_NAME\n, where\n\n$KUBE_NAMESPACE\n is the name of the Kubernetes namespace the cluster is located in and \n$CLUSTER_NAME\n is the name you\nhave assigned the cluster (such as \ndefault/my-test-cluster\n).\n\n\nThe following base64-encoded string represents a Protobuf-serialized \nmessage\n containing the string\n\nunstrict_majority\n: \nChF1bnN0cmljdF9tYWpvcml0eQ==\n. Decode this string and place it in the following etcd key, where\n\n$ENV\n is the value determined above:\n\n\n_kv/$ENV/m3db.client.bootstrap-consistency-level\n\n\n\n\nNote that on MacOS, \nbase64\n requires the \n-D\n flag to decode, whereas elsewhere it is likely \n-d\n. Also note the use of\n\necho -n\n to ensure removal of newlines if your shell does not support the \n<<<STRING\n pattern.\n\n\nOnce the cluster is recovered, the value should be deleted from etcd to revert to normal behavior using \netcdctl del\n.\n\n\nExamples:\n\n\n# On Linux, using a recent bash, update the value for env=default_env\n\n\n<<<\nChF1bnN0cmljdF9tYWpvcml0eQ\n==\n base64 -d \n|\n env \nETCDCTL_API\n=\n3\n etcdctl put _kv/default_env/m3db.client.bootstrap-consistency-level\n\n\n# On Linux, using a limited shell, update the value for env=default_env\n\n\necho\n -n \n\"ChF1bnN0cmljdF9tYWpvcml0eQ==\"\n \n|\n base64 -d \n|\n env \nETCDCTL_API\n=\n3\n etcdctl put _kv/default_env/m3db.client.bootstrap-consistency-level\n\n\n# On MacOS, update the value for a cluster \"test_cluster\" in Kubernetes namespace \"m3db\"\n\n\necho\n -n \n\"ChF1bnN0cmljdF9tYWpvcml0eQ==\"\n \n|\n base64 -D \n|\n kubectl \nexec\n -i \n$ETCD_POD\n -- env \nETCDCTL_API\n=\n3\n etcdctl put _kv/m3db/test_cluster/m3db.client.bootstrap-consistency-level\n\n\n# Delete the key to restore normal behavior\n\nenv \nETCDCTL_API\n=\n3\n etcdctl del _kv/default_env/m3db.client.bootstrap-consistency-level",
            "title": "Bootstrapping & Crash Recovery"
        },
        {
            "location": "/operational_guide/bootstrapping_crash_recovery/#bootstrapping-crash-recovery",
            "text": "",
            "title": "Bootstrapping &amp; Crash Recovery"
        },
        {
            "location": "/operational_guide/bootstrapping_crash_recovery/#introduction",
            "text": "We recommend reading the  placement operational guide  before reading the rest of this document.  When an M3DB node is turned on (goes through a placement change) it needs to go through a bootstrapping process to determine the integrity of data that it has, replay writes from the commit log, and/or stream missing data from its peers. In most cases, as long as you're running with the default and recommended bootstrapper configuration of:  filesystem,commitlog,peers,uninitialized_topology  then you should not need to worry about the bootstrapping process at all and M3DB will take care of doing the right thing such that you don't lose data and consistency guarantees are met. Note that the order of the configured bootstrappers  does  matter.  Generally speaking, we recommend that operators do not modify the bootstrappers configuration, but in the rare case that you to, this document is designed to help you understand the implications of doing so.  M3DB currently supports 5 different bootstrappers:   filesystem  commitlog  peers  uninitialized_topology  noop-all   When the bootstrapping process begins, M3DB nodes need to determine two things:   What shards the bootstrapping node should bootstrap, which can be determined from the cluster placement.  What time-ranges the bootstrapping node needs to bootstrap those shards for, which can be determined from the namespace retention.   For example, imagine a M3DB node that is responsible for shards 1, 5, 13, and 25 according to the cluster placement. In addition, it has a single namespace called \"metrics\" with a retention of 48 hours. When the M3DB node is started, the node will determine that it needs to bootstrap shards 1, 5, 13, and 25 for the time range starting at the current time and ending 48 hours ago. In order to obtain all this data, it will run the configured bootstrappers in the specified order. Every bootstrapper will notify the bootstrapping process of which shard/ranges it was able to bootstrap and the bootstrapping process will continue working its way through the list of bootstrappers until all the shards/ranges required have been marked as fulfilled. Otherwise the M3DB node will fail to start.",
            "title": "Introduction"
        },
        {
            "location": "/operational_guide/bootstrapping_crash_recovery/#bootstrappers",
            "text": "",
            "title": "Bootstrappers"
        },
        {
            "location": "/operational_guide/bootstrapping_crash_recovery/#filesystem-bootstrapper",
            "text": "The  filesystem  bootstrapper's responsibility is to determine which immutable  Fileset files  exist on disk, and if so, mark them as fulfilled. The  filesystem  bootstrapper achieves this by scanning M3DB's directory structure and determining which Fileset files exist on disk. Unlike the other bootstrappers, the  filesystem  bootstrapper does not need to load any data into memory, it simply verifies the checksums of the data on disk and other components of the M3DB node will handle reading (and caching) the data dynamically once it begins to serve reads.",
            "title": "Filesystem Bootstrapper"
        },
        {
            "location": "/operational_guide/bootstrapping_crash_recovery/#commitlog-bootstrapper",
            "text": "The  commitlog  bootstrapper's responsibility is to read the commitlog and snapshot (compacted commitlogs) files on disk and recover any data that has not yet been written out as an immutable Fileset file. Unlike the  filesystem  bootstrapper, the commit log bootstrapper cannot simply check which files are on disk in order to determine if it can satisfy a bootstrap request. Instead, the  commitlog  bootstrapper determines whether it can satisfy a bootstrap request using a simple heuristic.  On a shard-by-shard basis, the  commitlog  bootstrapper will consult the cluster placement to see if the node it is running on has ever achieved the  Available  status for the specified shard. If so, then the commit log bootstrapper should have all the data since the last Fileset file was flushed and will return that it can satisfy any time range for that shard. In other words, the commit log bootstrapper is all-or-nothing for a given shard: it will either return that it can satisfy any time range for a given shard or none at all. In addition, the  commitlog  bootstrapper  assumes  it is running after the  filesystem  bootstrapper. M3DB will not allow you to run with a configuration where the  filesystem  bootstrapper is placed after the  commitlog  bootstrapper, but it will allow you to run the  commitlog  bootstrapper without the  filesystem  bootstrapper which can result in loss of data, depending on the workload.",
            "title": "Commitlog Bootstrapper"
        },
        {
            "location": "/operational_guide/bootstrapping_crash_recovery/#peers-bootstrapper",
            "text": "The  peers  bootstrapper's responsibility is to stream in data for shard/ranges from other M3DB nodes (peers) in the cluster. This bootstrapper is only useful in M3DB clusters with more than a single node  and  where the replication factor is set to a value larger than 1. The  peers  bootstrapper will determine whether or not it can satisfy a bootstrap request on a shard-by-shard basis by consulting the cluster placement and determining if there are enough peers to satisfy the bootstrap request. For example, imagine the following M3DB placement where node A is trying to perform a peer bootstrap:      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502     Node A      \u2502          \u2502     Node B      \u2502        \u2502     Node C      \u2502\n\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                         \u2502   \u2502                       \u2502   \u2502                      \u2502\n\u2502                         \u2502   \u2502                       \u2502   \u2502                      \u2502\n\u2502  Shard 1: Initializing  \u2502   \u2502 Shard 1: Initializing \u2502   \u2502  Shard 1: Available  \u2502\n\u2502  Shard 2: Initializing  \u2502   \u2502 Shard 2: Initializing \u2502   \u2502  Shard 2: Available  \u2502\n\u2502  Shard 3: Initializing  \u2502   \u2502 Shard 3: Initializing \u2502   \u2502  Shard 3: Available  \u2502\n\u2502                         \u2502   \u2502                       \u2502   \u2502                      \u2502\n\u2502                         \u2502   \u2502                       \u2502   \u2502                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  In this case, the  peers  bootstrapper running on node A will not be able to fullfill any requests because node B is in the  Initializing  state for all of its shards and cannot fulfill bootstrap requests. This means that node A's  peers  bootstrapper cannot meet its default consistency level of majority for bootstrapping (1 < 2 which is majority with a replication factor of 3). On the other hand, node A would be able to peer bootstrap its shards in the following placement because its peers (nodes B/C) have sufficient replicas of the shards it needs in the  Available  state:      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502     Node A      \u2502          \u2502     Node B      \u2502        \u2502     Node C      \u2502\n\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                         \u2502   \u2502                       \u2502   \u2502                      \u2502\n\u2502                         \u2502   \u2502                       \u2502   \u2502                      \u2502\n\u2502  Shard 1: Initializing  \u2502   \u2502 Shard 1: Available    \u2502   \u2502  Shard 1: Available  \u2502\n\u2502  Shard 2: Initializing  \u2502   \u2502 Shard 2: Available    \u2502   \u2502  Shard 2: Available  \u2502\n\u2502  Shard 3: Initializing  \u2502   \u2502 Shard 3: Available    \u2502   \u2502  Shard 3: Available  \u2502\n\u2502                         \u2502   \u2502                       \u2502   \u2502                      \u2502\n\u2502                         \u2502   \u2502                       \u2502   \u2502                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  Note that a bootstrap consistency level of  majority  is the default value, but can be modified by changing the value of the key  m3db.client.bootstrap-consistency-level  in  etcd  to one of:  none ,  one ,  unstrict_majority  (attempt to read from majority, but settle for less if any errors occur),  majority  (strict majority), and  all . For example, if an entire cluster with a replication factor of 3 was restarted simultaneously, all the nodes would get stuck in an infinite loop trying to peer bootstrap from each other and not achieving majority until an operator modified this value. Note that this can happen even if all the shards were in the  Available  state because M3DB nodes will reject all read requests for a shard until they have bootstrapped that shard (which has to happen everytime the node is restarted).  Note : Any bootstrappers configuration that does not include the  peers  bootstrapper will be unable to handle dynamic placement changes of any kind.",
            "title": "Peers Bootstrapper"
        },
        {
            "location": "/operational_guide/bootstrapping_crash_recovery/#uninitialized-topology-bootstrapper",
            "text": "The purpose of the  uninitialized_topology  bootstrapper is to succeed bootstraps for all time ranges for shards that have never been completely bootstrapped (at a cluster level). This allows us to run the default bootstrapper configuration of:  filesystem,commitlog,peers,uninitialized_topology  such that the  filesystem  and  commitlog  bootstrappers are used by default in node restarts, the  peers  bootstrapper is used for node adds/removes/replaces, and bootstraps still succeed for brand new placement where both the  commitlog  and  peers  bootstrappers will be unable to succeed any bootstraps. In other words, the  uninitialized_topology  bootstrapper allows us to place the  commitlog  bootstrapper  before  the  peers  bootstrapper and still succeed bootstraps with brand new placements without resorting to using the noop-all bootstrapper which suceeds bootstraps for all shard/time-ranges regardless of the status of the placement.  The  uninitialized_topology  bootstrapper determines whether a placement is \"new\" for a given shard by counting the number of nodes in the  Initializing  state and  Leaving  states and there are more  Initializing  than  Leaving , then it succeeds the bootstrap because that means the placement has never reached a state where all nodes are  Available .",
            "title": "Uninitialized Topology Bootstrapper"
        },
        {
            "location": "/operational_guide/bootstrapping_crash_recovery/#no-operational-all-bootstrapper",
            "text": "The  noop-all  bootstrapper succeeds all bootstraps regardless of requests shards/time ranges.",
            "title": "No Operational All Bootstrapper"
        },
        {
            "location": "/operational_guide/bootstrapping_crash_recovery/#bootstrappers-configuration",
            "text": "Now that we've gone over the various bootstrappers, let's consider how M3DB will behave in different configurations. Note that we include  uninitialized_topology  at the end of all the lists of bootstrappers because its required to get a new placement up and running in the first place, but is not required after that (although leaving it in has no detrimental effects). Also note that any configuration that does not include the  peers  bootstrapper will not be able to handle dynamic placement changes like node adds/removes/replaces.",
            "title": "Bootstrappers Configuration"
        },
        {
            "location": "/operational_guide/bootstrapping_crash_recovery/#filesystemcommitlogpeersuninitialized_topology-default",
            "text": "This is the default bootstrappers configuration for M3DB and will behave \"as expected\" in the sense that it will maintain M3DB's consistency guarantees at all times, handle node adds/replaces/removes correctly, and still work with brand new placements / topologies.  This is the only configuration that we recommend using in production .  In the general case, the node will use only the  filesystem  and  commitlog  bootstrappers on node startup. However, in the case of a node add/remove/replace, the  commitlog  bootstrapper will detect that it is unable to fulfill the bootstrap request (because the node has never reached the  Available  state) and defer to the  peers  bootstrapper to stream in the data.  Additionally, if it is a brand new placement where even the  peers  bootstrapper cannot fulfill the bootstrap, this will be detected by the  uninitialized_topology  bootstrapper which will succeed the bootstrap.",
            "title": "filesystem,commitlog,peers,uninitialized_topology (default)"
        },
        {
            "location": "/operational_guide/bootstrapping_crash_recovery/#filesystempeersuninitialized_topology",
            "text": "Everytime a node is restarted it will attempt to stream in all of the the data for any blocks that it has never flushed, which is generally the currently active block and possibly the previous block as well. This mode can be useful if you want to improve performance or save disk space by operating nodes without a commitlog, or want to force a repair of any unflushed blocks. This mode can lead to violations of M3DB's consistency guarantees due to the fact that commit logs are being ignored. In addition, if you lose a replication factors worth or more of hosts at the same time, the node will not be able to bootstrap unless an operator modifies the bootstrap consistency level configuration in etcd (see  peers  bootstrap section above). Finally, this mode adds additional network and resource pressure on other nodes in the cluster while one node is peer bootstrapping from them which can be problematic in catastrophic scenarios where all the nodes are trying to stream data from each other.",
            "title": "filesystem,peers,uninitialized_topology"
        },
        {
            "location": "/operational_guide/bootstrapping_crash_recovery/#peersuninitialized_topology",
            "text": "Every time a node is restarted, it will attempt to stream in  all  of the data that it is responsible for from its peers, completely ignoring the immutable Fileset files it already has on disk. This mode can be useful if you want to improve performance or save disk space by operating nodes without a commitlog, or want to force a repair of all data on an individual node. This mode can lead to violations of M3DB's consistency guarantees due to the fact that the commit logs are being ignored. In addition, if you lose a replication factors worth or more of hosts at the same time, the node will not be able to bootstrap unless an operator modifies the bootstrap consistency level configuration in etcd (see  peers  bootstrap section above). Finally, this mode adds additional network and resource pressure on other nodes in the cluster while one node is peer bootstrapping from them which can be problematic in catastrophic scenarios where all the nodes are trying to stream data from each other.",
            "title": "peers,uninitialized_topology"
        },
        {
            "location": "/operational_guide/bootstrapping_crash_recovery/#invalid-bootstrappers-configuration",
            "text": "For the sake of completeness, we've included a short discussion below of some bootstrapping configurations that we consider \"invalid\" in that they are likely to lose data / violate M3DB's consistency guarantees and/or not handle placement changes in a correct way.",
            "title": "Invalid bootstrappers configuration"
        },
        {
            "location": "/operational_guide/bootstrapping_crash_recovery/#filesystemcommitloguninitialized_topology",
            "text": "This bootstrapping configuration will work just fine if nodes are never added/replaced/removed, but will fail when attempting a node add/replace/remove.",
            "title": "filesystem,commitlog,uninitialized_topology"
        },
        {
            "location": "/operational_guide/bootstrapping_crash_recovery/#filesystemuninitialized_topology",
            "text": "Every time a node is restarted it will utilize the immutable Fileset files its already written out to disk, but any data that it had received since it wrote out the last set of immutable files will be lost.",
            "title": "filesystem,uninitialized_topology"
        },
        {
            "location": "/operational_guide/bootstrapping_crash_recovery/#commitloguninitialized_topology",
            "text": "Every time a node is restarted it will read all the commit log and snapshot files it has on disk, but it will ignore all the data in the immutable Fileset files that it has already written.",
            "title": "commitlog,uninitialized_topology"
        },
        {
            "location": "/operational_guide/bootstrapping_crash_recovery/#crash-recovery",
            "text": "NOTE:  These steps should not be necessary in most cases, especially if using the default bootstrappers configuration\nof  filesystem,commitlog,peers,uninitialized_topology . However in the case the configuration is non-default or the\ncluster has been down for a prolonged period of time these steps may be necessary. A good indicator would be log\nmessages related to failing to bootstrap from peers due to consistency issues.  M3DB may require manual intervention to recover in the event of a prolonged loss of quorum. This is because the  Peers\nBoostrapper  must read from a majority of nodes owning a shard to bootstrap.  To relax this bootstrapping constraint, a value stored in etcd must be modified that corresponds to the m3db.client.bootstrap-consistency-level  runtime flag. Until the coordinator supports  an API for this, this must be done manually. The M3 contributors are\naware of how cumbersome this is and are working on this API.  To update this value in etcd, first determine the environment the M3DB node is using. For example in  this \nconfiguration, it is  default_env . If using the M3DB Operator, the value will be  $KUBE_NAMESPACE/$CLUSTER_NAME , where $KUBE_NAMESPACE  is the name of the Kubernetes namespace the cluster is located in and  $CLUSTER_NAME  is the name you\nhave assigned the cluster (such as  default/my-test-cluster ).  The following base64-encoded string represents a Protobuf-serialized  message  containing the string unstrict_majority :  ChF1bnN0cmljdF9tYWpvcml0eQ== . Decode this string and place it in the following etcd key, where $ENV  is the value determined above:  _kv/$ENV/m3db.client.bootstrap-consistency-level  Note that on MacOS,  base64  requires the  -D  flag to decode, whereas elsewhere it is likely  -d . Also note the use of echo -n  to ensure removal of newlines if your shell does not support the  <<<STRING  pattern.  Once the cluster is recovered, the value should be deleted from etcd to revert to normal behavior using  etcdctl del .  Examples:  # On Linux, using a recent bash, update the value for env=default_env  <<< ChF1bnN0cmljdF9tYWpvcml0eQ ==  base64 -d  |  env  ETCDCTL_API = 3  etcdctl put _kv/default_env/m3db.client.bootstrap-consistency-level # On Linux, using a limited shell, update the value for env=default_env  echo  -n  \"ChF1bnN0cmljdF9tYWpvcml0eQ==\"   |  base64 -d  |  env  ETCDCTL_API = 3  etcdctl put _kv/default_env/m3db.client.bootstrap-consistency-level # On MacOS, update the value for a cluster \"test_cluster\" in Kubernetes namespace \"m3db\"  echo  -n  \"ChF1bnN0cmljdF9tYWpvcml0eQ==\"   |  base64 -D  |  kubectl  exec  -i  $ETCD_POD  -- env  ETCDCTL_API = 3  etcdctl put _kv/m3db/test_cluster/m3db.client.bootstrap-consistency-level # Delete the key to restore normal behavior \nenv  ETCDCTL_API = 3  etcdctl del _kv/default_env/m3db.client.bootstrap-consistency-level",
            "title": "Crash Recovery"
        },
        {
            "location": "/operational_guide/kernel_configuration/",
            "text": "Docker & Kernel Configuration\n\n\nThis document lists the Kernel tweaks M3DB needs to run well. If you are running on Kubernetes, you may use our\n\nsysctl-setter\n \nDaemonSet\n that will set these\nvalues for you. Please read the comment in that manifest to understand the implications of applying it.\n\n\nRunning with Docker\n\n\nWhen running M3DB inside Docker, it is recommended to add the \nSYS_RESOURCE\n capability to the container (using the\n\n--cap-add\n argument to \ndocker run\n) so that it can raise its file limits:\n\n\ndocker run --cap-add SYS_RESOURCE quay.io/m3/m3dbnode:latest\n\n\n\n\nIf M3DB is being run as a non-root user, M3's \nsetcap\n images are required:\n\ndocker run --cap-add SYS_RESOURCE -u 1000:1000 quay.io/m3/m3dbnode:latest-setcap\n\n\n\nMore information on Docker's capability settings can be found \nhere\n.\n\n\nvm.max_map_count\n\n\nM3DB uses a lot of mmap-ed files for performance, as a result, you might need to bump \nvm.max_map_count\n. We suggest setting this value to \n3000000\n, so you don\u2019t have to come back and debug issues later.\n\n\nOn Linux, you can increase the limits by running the following command as root:\n\nsysctl -w vm.max_map_count=3000000\n\n\n\nTo set this value permanently, update the \nvm.max_map_count\n setting in \n/etc/sysctl.conf\n.\n\n\nvm.swappiness\n\n\nvm.swappiness\n controls how much the virtual memory subsystem will try to swap to disk. By default, the kernel configures this value to \n60\n, and will try to swap out items in memory even when there is plenty of RAM available to the system.\n\n\nWe recommend sizing clusters such that M3DB is running on a substrate (hosts/containers) such that no-swapping is necessary, i.e. the process is only using 30-50% of the maximum available memory. And therefore recommend setting the value of \nvm.swappiness\n to \n1\n. This tells the kernel to swap as little as possible, without altogether disabling swapping.\n\n\nOn Linux, you can configure this by running the following as root:\n\nsysctl -w vm.swappiness=1\n\n\n\nTo set this value permanently, update the \nvm.swappiness\n setting in \n/etc/sysctl.conf\n.\n\n\nrlimits\n\n\nM3DB also can use a high number of files and we suggest setting a high max open number of files due to per partition fileset volumes.\n\n\nYou may need to override the system and process-level limits set by the kernel with the following commands. To check the existing values run:\n\n\nsysctl -n fs.file-max\n\n\n\n\nand\n\n\nsysctl -n fs.nr_open\n\n\n\n\nto see the kernel and process limits respectively.\nIf either of the values are less than three million (our minimum recommended value), then you can update them with the following commands:\n\n\nsysctl -w fs.file-max=3000000\n\n\n\n\nsysctl -w fs.nr_open=3000000\n\n\n\n\nTo set these values permanently, update the \nfs.file-max\n and \nfs.nr_open\n settings in \n/etc/sysctl.conf\n.\n\n\nAlternatively, if you wish to have M3DB run under \nsystemd\n you can use our \nservice example\n which will set sane defaults.\nKeep in mind that you'll still need to configure the kernel and process limits because systemd will not allow a process to exceed them and will silently fallback to a default value which could cause M3DB to crash due to hitting the file descriptor limit.\nAlso note that systemd has a \nsystem.conf\n file and a \nuser.conf\n file which may contain limits that the service-specific configuration files cannot override.\nBe sure to check that those files aren't configured with values lower than the value you configure at the service level.\n\n\nBefore running the process make sure the limits are set, if running manually you can raise the limit for the current user with \nulimit -n 3000000\n.\n\n\nAutomatic Limit Raising\n\n\nDuring startup, M3DB will attempt to raise its open file limit to the current value of \nfs.nr_open\n. This is a benign\noperation; if it fails M3DB, will simply emit a warning.",
            "title": "Docker & Kernel Configuration"
        },
        {
            "location": "/operational_guide/kernel_configuration/#docker-kernel-configuration",
            "text": "This document lists the Kernel tweaks M3DB needs to run well. If you are running on Kubernetes, you may use our sysctl-setter   DaemonSet  that will set these\nvalues for you. Please read the comment in that manifest to understand the implications of applying it.",
            "title": "Docker &amp; Kernel Configuration"
        },
        {
            "location": "/operational_guide/kernel_configuration/#running-with-docker",
            "text": "When running M3DB inside Docker, it is recommended to add the  SYS_RESOURCE  capability to the container (using the --cap-add  argument to  docker run ) so that it can raise its file limits:  docker run --cap-add SYS_RESOURCE quay.io/m3/m3dbnode:latest  If M3DB is being run as a non-root user, M3's  setcap  images are required: docker run --cap-add SYS_RESOURCE -u 1000:1000 quay.io/m3/m3dbnode:latest-setcap  More information on Docker's capability settings can be found  here .",
            "title": "Running with Docker"
        },
        {
            "location": "/operational_guide/kernel_configuration/#vmmax_map_count",
            "text": "M3DB uses a lot of mmap-ed files for performance, as a result, you might need to bump  vm.max_map_count . We suggest setting this value to  3000000 , so you don\u2019t have to come back and debug issues later.  On Linux, you can increase the limits by running the following command as root: sysctl -w vm.max_map_count=3000000  To set this value permanently, update the  vm.max_map_count  setting in  /etc/sysctl.conf .",
            "title": "vm.max_map_count"
        },
        {
            "location": "/operational_guide/kernel_configuration/#vmswappiness",
            "text": "vm.swappiness  controls how much the virtual memory subsystem will try to swap to disk. By default, the kernel configures this value to  60 , and will try to swap out items in memory even when there is plenty of RAM available to the system.  We recommend sizing clusters such that M3DB is running on a substrate (hosts/containers) such that no-swapping is necessary, i.e. the process is only using 30-50% of the maximum available memory. And therefore recommend setting the value of  vm.swappiness  to  1 . This tells the kernel to swap as little as possible, without altogether disabling swapping.  On Linux, you can configure this by running the following as root: sysctl -w vm.swappiness=1  To set this value permanently, update the  vm.swappiness  setting in  /etc/sysctl.conf .",
            "title": "vm.swappiness"
        },
        {
            "location": "/operational_guide/kernel_configuration/#rlimits",
            "text": "M3DB also can use a high number of files and we suggest setting a high max open number of files due to per partition fileset volumes.  You may need to override the system and process-level limits set by the kernel with the following commands. To check the existing values run:  sysctl -n fs.file-max  and  sysctl -n fs.nr_open  to see the kernel and process limits respectively.\nIf either of the values are less than three million (our minimum recommended value), then you can update them with the following commands:  sysctl -w fs.file-max=3000000  sysctl -w fs.nr_open=3000000  To set these values permanently, update the  fs.file-max  and  fs.nr_open  settings in  /etc/sysctl.conf .  Alternatively, if you wish to have M3DB run under  systemd  you can use our  service example  which will set sane defaults.\nKeep in mind that you'll still need to configure the kernel and process limits because systemd will not allow a process to exceed them and will silently fallback to a default value which could cause M3DB to crash due to hitting the file descriptor limit.\nAlso note that systemd has a  system.conf  file and a  user.conf  file which may contain limits that the service-specific configuration files cannot override.\nBe sure to check that those files aren't configured with values lower than the value you configure at the service level.  Before running the process make sure the limits are set, if running manually you can raise the limit for the current user with  ulimit -n 3000000 .",
            "title": "rlimits"
        },
        {
            "location": "/operational_guide/kernel_configuration/#automatic-limit-raising",
            "text": "During startup, M3DB will attempt to raise its open file limit to the current value of  fs.nr_open . This is a benign\noperation; if it fails M3DB, will simply emit a warning.",
            "title": "Automatic Limit Raising"
        },
        {
            "location": "/operational_guide/etcd/",
            "text": "etcd\n\n\nThe M3 stack leverages \netcd\n as a distributed key-value storage to:\n\n\n\n\nUpdate cluster configuration in realtime\n\n\nManage placements for our distributed / sharded tiers like M3DB and M3Aggregator\n\n\nPerform leader-election in M3Aggregator\n\n\n\n\nand much more!\n\n\nOverview\n\n\nM3DB\n ships with support for running embedded \netcd\n (called \nseed nodes\n), and while this is convenient for testing and development, we don't recommend running with this setup in production.\n\n\nBoth \nM3\n and \netcd\n are complex distributed systems, and trying to operate both within the same binary is challenging and dangerous for production workloads.\n\n\nInstead, we recommend running an external \netcd\n cluster that is isolated from the \nM3\n stack so that performing operations like node adds, removes, and replaces are easier.\n\n\nWhile M3 relies on \netcd\n to provide strong consistency, the operations we use it for are all low-throughput so you should be able to operate a very low maintenance \netcd\n cluster. \nA 3-node setup for high availability\n should be more than sufficient for most workloads.\n\n\nConfiguring an External etcd Cluster\n\n\nM3DB\n\n\nMost of our documentation demonstrates how to run \nM3DB\n with embedded etcd nodes. Once you're ready to switch to an external \netcd\n cluster, all you need to do is modify the \nM3DB\n config to remove the \nseedNodes\n field entirely and then change the \nendpoints\n under \netcdClusters\n to point to your external \netcd\n nodes instead of the \nM3DB\n seed nodes.\n\n\nFor example this portion of the config\n\n\nconfig\n:\n\n    \nservice\n:\n\n        \nenv\n:\n \ndefault_env\n\n        \nzone\n:\n \nembedded\n\n        \nservice\n:\n \nm3db\n\n        \ncacheDir\n:\n \n/var/lib/m3kv\n\n        \netcdClusters\n:\n\n            \n-\n \nzone\n:\n \nembedded\n\n              \nendpoints\n:\n\n                  \n-\n \nhttp://m3db_seed1:2379\n\n                  \n-\n \nhttp://m3db_seed2:2379\n\n                  \n-\n \nhttp://m3db_seed3:2379\n\n    \nseedNodes\n:\n\n        \ninitialCluster\n:\n\n            \n-\n \nhostID\n:\n \nm3db_seed1\n\n              \nendpoint\n:\n \nhttp://m3db_seed1:2380\n\n            \n-\n \nhostID\n:\n \nm3db_seed2\n\n              \nendpoint\n:\n \nhttp://m3db_seed2:2380\n\n            \n-\n \nhostID\n:\n \nm3db_seed3\n\n              \nendpoint\n:\n \nhttp://m3db_seed3:2380\n\n\n\n\n\nwould become\n\n\nconfig\n:\n\n    \nservice\n:\n\n        \nenv\n:\n \ndefault_env\n\n        \nzone\n:\n \nembedded\n\n        \nservice\n:\n \nm3db\n\n        \ncacheDir\n:\n \n/var/lib/m3kv\n\n        \netcdClusters\n:\n\n            \n-\n \nzone\n:\n \nembedded\n\n              \nendpoints\n:\n\n                  \n-\n \nhttp://external_etcd1:2379\n\n                  \n-\n \nhttp://external_etcd2:2379\n\n                  \n-\n \nhttp://external_etcd3:2379\n\n\n\n\n\nNote\n: \nM3DB\n placements and namespaces are stored in \netcd\n so if you want to switch to an external \netcd\n cluster you'll need to recreate all your placements and namespaces. You can do this manually or use \netcdctl\n's \nMirror Maker\n functionality.\n\n\nM3Coordinator\n\n\nM3Coordinator\n does not run embedded \netcd\n, so configuring it to use an external \netcd\n cluster is simple. Just replace the \nendpoints\n under \netcdClusters\n in the YAML config to point to your external \netcd\n nodes instead of the \nM3DB\n seed nodes. See the \nM3DB\n example above for a detailed before/after comparison of the YAML config.\n\n\netcd Operations\n\n\nEmbedded etcd\n\n\nIf you're running \nM3DB seed nodes\n with embedded \netcd\n (which we do not recommend for production workloads) and need to perform a node add/replace/remove then follow our \nplacement configuration guide\n and pay special attention to follow the special instructions for \nseed nodes\n.\n\n\nExternal etcd\n\n\nJust follow the instructions in the \netcd docs.",
            "title": "etcd"
        },
        {
            "location": "/operational_guide/etcd/#etcd",
            "text": "The M3 stack leverages  etcd  as a distributed key-value storage to:   Update cluster configuration in realtime  Manage placements for our distributed / sharded tiers like M3DB and M3Aggregator  Perform leader-election in M3Aggregator   and much more!",
            "title": "etcd"
        },
        {
            "location": "/operational_guide/etcd/#overview",
            "text": "M3DB  ships with support for running embedded  etcd  (called  seed nodes ), and while this is convenient for testing and development, we don't recommend running with this setup in production.  Both  M3  and  etcd  are complex distributed systems, and trying to operate both within the same binary is challenging and dangerous for production workloads.  Instead, we recommend running an external  etcd  cluster that is isolated from the  M3  stack so that performing operations like node adds, removes, and replaces are easier.  While M3 relies on  etcd  to provide strong consistency, the operations we use it for are all low-throughput so you should be able to operate a very low maintenance  etcd  cluster.  A 3-node setup for high availability  should be more than sufficient for most workloads.",
            "title": "Overview"
        },
        {
            "location": "/operational_guide/etcd/#configuring-an-external-etcd-cluster",
            "text": "",
            "title": "Configuring an External etcd Cluster"
        },
        {
            "location": "/operational_guide/etcd/#m3db",
            "text": "Most of our documentation demonstrates how to run  M3DB  with embedded etcd nodes. Once you're ready to switch to an external  etcd  cluster, all you need to do is modify the  M3DB  config to remove the  seedNodes  field entirely and then change the  endpoints  under  etcdClusters  to point to your external  etcd  nodes instead of the  M3DB  seed nodes.  For example this portion of the config  config : \n     service : \n         env :   default_env \n         zone :   embedded \n         service :   m3db \n         cacheDir :   /var/lib/m3kv \n         etcdClusters : \n             -   zone :   embedded \n               endpoints : \n                   -   http://m3db_seed1:2379 \n                   -   http://m3db_seed2:2379 \n                   -   http://m3db_seed3:2379 \n     seedNodes : \n         initialCluster : \n             -   hostID :   m3db_seed1 \n               endpoint :   http://m3db_seed1:2380 \n             -   hostID :   m3db_seed2 \n               endpoint :   http://m3db_seed2:2380 \n             -   hostID :   m3db_seed3 \n               endpoint :   http://m3db_seed3:2380   would become  config : \n     service : \n         env :   default_env \n         zone :   embedded \n         service :   m3db \n         cacheDir :   /var/lib/m3kv \n         etcdClusters : \n             -   zone :   embedded \n               endpoints : \n                   -   http://external_etcd1:2379 \n                   -   http://external_etcd2:2379 \n                   -   http://external_etcd3:2379   Note :  M3DB  placements and namespaces are stored in  etcd  so if you want to switch to an external  etcd  cluster you'll need to recreate all your placements and namespaces. You can do this manually or use  etcdctl 's  Mirror Maker  functionality.",
            "title": "M3DB"
        },
        {
            "location": "/operational_guide/etcd/#m3coordinator",
            "text": "M3Coordinator  does not run embedded  etcd , so configuring it to use an external  etcd  cluster is simple. Just replace the  endpoints  under  etcdClusters  in the YAML config to point to your external  etcd  nodes instead of the  M3DB  seed nodes. See the  M3DB  example above for a detailed before/after comparison of the YAML config.",
            "title": "M3Coordinator"
        },
        {
            "location": "/operational_guide/etcd/#etcd-operations",
            "text": "",
            "title": "etcd Operations"
        },
        {
            "location": "/operational_guide/etcd/#embedded-etcd",
            "text": "If you're running  M3DB seed nodes  with embedded  etcd  (which we do not recommend for production workloads) and need to perform a node add/replace/remove then follow our  placement configuration guide  and pay special attention to follow the special instructions for  seed nodes .",
            "title": "Embedded etcd"
        },
        {
            "location": "/operational_guide/etcd/#external-etcd",
            "text": "Just follow the instructions in the  etcd docs.",
            "title": "External etcd"
        },
        {
            "location": "/operational_guide/mapping_rollup/",
            "text": "Mapping Rules\n\n\nMapping rules are used to configure the storage policy for metrics. The storage policy\ndetermines how long to store metrics for and at what resolution to keep them at.\nFor example, a storage policy of \n1m:48h\n tells M3 to keep the metrics for \n48hrs\n at a\n\n1min\n resolution. Mapping rules can be configured in the \nm3coordinator\n configuration file\nunder the \ndownsample\n > \nrules\n > \nmappingRules\n stanza. We will use the following as an\nexample. \n\n\ndownsample\n:\n\n  \nrules\n:\n\n    \nmappingRules\n:\n\n      \n-\n \nname\n:\n \n\"mysql\n \nmetrics\"\n\n        \nfilter\n:\n \n\"app:mysql*\"\n\n        \naggregations\n:\n \n[\n\"Last\"\n]\n\n        \nstoragePolicies\n:\n\n          \n-\n \nresolution\n:\n \n1m\n\n            \nretention\n:\n \n48h\n\n      \n-\n \nname\n:\n \n\"nginx\n \nmetrics\"\n\n        \nfilter\n:\n \n\"app:nginx*\"\n\n        \naggregations\n:\n \n[\n\"Last\"\n]\n\n        \nstoragePolicies\n:\n\n          \n-\n \nresolution\n:\n \n30s\n\n            \nretention\n:\n \n24h\n\n          \n-\n \nresolution\n:\n \n1m\n\n            \nretention\n:\n \n48h\n\n\n\n\n\nHere, we have two mapping rules configured -- one for \nmysql\n metrics and one for \nnginx\n\nmetrics. The filter determines what metrics each rule applies to. The \nmysql metrics\n rule \nwill apply to any metrics where the \napp\n tag contains \nmysql*\n as the value (\n*\n being a wildcard).\nSimilarly, the \nnginx metrics\n rule will apply to all metrics where the \napp\n tag contains \n\nnginx*\n as the value.\n\n\nThe \naggregations\n field determines what functions to apply to the datapoints within a \nresolution tile. For example, if an application emits a metric every \n10sec\n and the resolution\nfor that metrics's storage policy is \n1min\n, M3 will need to combine 6 datapoints. If the \naggregations\n\npolicy is \nLast\n, M3 will take the last value in that \n1min\n bucket. \naggregations\n can be one \nof the following:\n\n\nLast\nMin\nMax\nMean\nMedian\nCount\nSum\nSumSq\nStdev\nP10\nP20\nP30\nP40\nP50\nP60\nP70\nP80\nP90\nP95\nP99\nP999\nP9999\n\n\n\n\nLastly, the \nstoragePolicies\n field determines which namespaces to store the metrics in. For example, \nthe \nmysql\n metrics will be sent to the \n1m:48h\n namespace, while the \nnginx\n metrics will be sent to \nboth the \n1m:48h\n and \n30s:24h\n namespaces.\n\n\nNote:\n the namespaces listed under the \nstoragePolicies\n stanza must exist in M3DB.\n\n\nRollup Rules\n\n\nRollup rules are used to rollup metrics and aggregate in different ways by \narbitrary dimensions before they are stored. \n\n\nHere's an example of creating a new monotonic counter called \n\nhttp_request_rollup_no_pod_bucket\n from a set of histogram metrics originally \ncalled \nhttp_request_bucket\n:\n\n\ndownsample\n:\n\n  \nrules\n:\n\n    \nrollupRules\n:\n\n      \n-\n \nname\n:\n \n\"http_request\n \nlatency\n \nby\n \nroute\n \nand\n \ngit_sha\n \nwithout\n \npod\"\n\n        \nfilter\n:\n \n\"__name__:http_request_bucket\n \nk8s_pod:*\n \nle:*\n \ngit_sha:*\n \nroute:*\"\n\n        \ntransforms\n:\n\n        \n-\n \ntransform\n:\n\n            \ntype\n:\n \n\"Increase\"\n\n        \n-\n \nrollup\n:\n\n            \nmetricName\n:\n \n\"http_request_rollup_no_pod_bucket\"\n\n            \ngroupBy\n:\n \n[\n\"le\"\n,\n \n\"git_sha\"\n,\n \n\"route\"\n,\n \n\"status_code\"\n,\n \n\"region\"\n]\n\n            \naggregations\n:\n \n[\n\"Sum\"\n]\n\n        \n-\n \ntransform\n:\n\n            \ntype\n:\n \n\"Add\"\n\n        \nstoragePolicies\n:\n\n        \n-\n \nresolution\n:\n \n30s\n\n          \nretention\n:\n \n720h\n\n\n\n\n\nWhile the above example can be used to create a new rolled up metric, \noften times the goal of rollup rules is to eliminate the underlaying, \nraw metrics. In order to do this, a \nmappingRule\n will need to be \nadded like in the following example (using the metric above as an example) \nwith \ndrop\n set to \ntrue\n. Additionally, if \nall\n of the underlaying metrics are\nbeing dropped, there is no need to change the metric name (e.g. in the \n\nrollupRule\n, the \nmetricName\n field can be equal to the existing metric) --\nsee below for an example.\n\n\ndownsample\n:\n\n  \nrules\n:\n\n    \nmappingRules\n:\n\n      \n-\n \nname\n:\n \n\"http_request\n \nlatency\n \nby\n \nroute\n \nand\n \ngit_sha\n \ndrop\n \nraw\"\n\n        \nfilter\n:\n \n\"__name__:http_request_bucket\n \nk8s_pod:*\n \nle:*\n \ngit_sha:*\n \nroute:*\"\n\n        \ndrop\n:\n \ntrue\n\n    \nrollupRules\n:\n\n      \n-\n \nname\n:\n \n\"http_request\n \nlatency\n \nby\n \nroute\n \nand\n \ngit_sha\n \nwithout\n \npod\"\n\n        \nfilter\n:\n \n\"__name__:http_request_bucket\n \nk8s_pod:*\n \nle:*\n \ngit_sha:*\n \nroute:*\"\n\n        \ntransforms\n:\n\n        \n-\n \ntransform\n:\n\n            \ntype\n:\n \n\"Increase\"\n\n        \n-\n \nrollup\n:\n\n            \nmetricName\n:\n \n\"http_request_bucket\"\n \n# metric name doesn't change\n\n            \ngroupBy\n:\n \n[\n\"le\"\n,\n \n\"git_sha\"\n,\n \n\"route\"\n,\n \n\"status_code\"\n,\n \n\"region\"\n]\n\n            \naggregations\n:\n \n[\n\"Sum\"\n]\n\n        \n-\n \ntransform\n:\n\n            \ntype\n:\n \n\"Add\"\n\n        \nstoragePolicies\n:\n\n        \n-\n \nresolution\n:\n \n30s\n\n          \nretention\n:\n \n720h\n\n\n\n\n\nNote:\n In order to store rolled up metrics in an \nunaggregated\n namespace,\na matching \naggregated\n namespace must be added to the coordinator config. For \nexample, if in the above rule, the \n720h\n namespace under \nstoragePolicies\n \nis \nunaggregated\n, the following will need to be added to the coordinator config.\n\n\n-\n \nnamespace\n:\n \ndefault\n\n  \nresolution\n:\n \n30s\n\n  \nretention\n:\n \n720h\n\n  \ntype\n:\n \naggregated\n\n  \ndownsample\n:\n\n    \nall\n:\n \nfalse",
            "title": "Configuring Mapping & Rollup Rules"
        },
        {
            "location": "/operational_guide/mapping_rollup/#mapping-rules",
            "text": "Mapping rules are used to configure the storage policy for metrics. The storage policy\ndetermines how long to store metrics for and at what resolution to keep them at.\nFor example, a storage policy of  1m:48h  tells M3 to keep the metrics for  48hrs  at a 1min  resolution. Mapping rules can be configured in the  m3coordinator  configuration file\nunder the  downsample  >  rules  >  mappingRules  stanza. We will use the following as an\nexample.   downsample : \n   rules : \n     mappingRules : \n       -   name :   \"mysql   metrics\" \n         filter :   \"app:mysql*\" \n         aggregations :   [ \"Last\" ] \n         storagePolicies : \n           -   resolution :   1m \n             retention :   48h \n       -   name :   \"nginx   metrics\" \n         filter :   \"app:nginx*\" \n         aggregations :   [ \"Last\" ] \n         storagePolicies : \n           -   resolution :   30s \n             retention :   24h \n           -   resolution :   1m \n             retention :   48h   Here, we have two mapping rules configured -- one for  mysql  metrics and one for  nginx \nmetrics. The filter determines what metrics each rule applies to. The  mysql metrics  rule \nwill apply to any metrics where the  app  tag contains  mysql*  as the value ( *  being a wildcard).\nSimilarly, the  nginx metrics  rule will apply to all metrics where the  app  tag contains  nginx*  as the value.  The  aggregations  field determines what functions to apply to the datapoints within a \nresolution tile. For example, if an application emits a metric every  10sec  and the resolution\nfor that metrics's storage policy is  1min , M3 will need to combine 6 datapoints. If the  aggregations \npolicy is  Last , M3 will take the last value in that  1min  bucket.  aggregations  can be one \nof the following:  Last\nMin\nMax\nMean\nMedian\nCount\nSum\nSumSq\nStdev\nP10\nP20\nP30\nP40\nP50\nP60\nP70\nP80\nP90\nP95\nP99\nP999\nP9999  Lastly, the  storagePolicies  field determines which namespaces to store the metrics in. For example, \nthe  mysql  metrics will be sent to the  1m:48h  namespace, while the  nginx  metrics will be sent to \nboth the  1m:48h  and  30s:24h  namespaces.  Note:  the namespaces listed under the  storagePolicies  stanza must exist in M3DB.",
            "title": "Mapping Rules"
        },
        {
            "location": "/operational_guide/mapping_rollup/#rollup-rules",
            "text": "Rollup rules are used to rollup metrics and aggregate in different ways by \narbitrary dimensions before they are stored.   Here's an example of creating a new monotonic counter called  http_request_rollup_no_pod_bucket  from a set of histogram metrics originally \ncalled  http_request_bucket :  downsample : \n   rules : \n     rollupRules : \n       -   name :   \"http_request   latency   by   route   and   git_sha   without   pod\" \n         filter :   \"__name__:http_request_bucket   k8s_pod:*   le:*   git_sha:*   route:*\" \n         transforms : \n         -   transform : \n             type :   \"Increase\" \n         -   rollup : \n             metricName :   \"http_request_rollup_no_pod_bucket\" \n             groupBy :   [ \"le\" ,   \"git_sha\" ,   \"route\" ,   \"status_code\" ,   \"region\" ] \n             aggregations :   [ \"Sum\" ] \n         -   transform : \n             type :   \"Add\" \n         storagePolicies : \n         -   resolution :   30s \n           retention :   720h   While the above example can be used to create a new rolled up metric, \noften times the goal of rollup rules is to eliminate the underlaying, \nraw metrics. In order to do this, a  mappingRule  will need to be \nadded like in the following example (using the metric above as an example) \nwith  drop  set to  true . Additionally, if  all  of the underlaying metrics are\nbeing dropped, there is no need to change the metric name (e.g. in the  rollupRule , the  metricName  field can be equal to the existing metric) --\nsee below for an example.  downsample : \n   rules : \n     mappingRules : \n       -   name :   \"http_request   latency   by   route   and   git_sha   drop   raw\" \n         filter :   \"__name__:http_request_bucket   k8s_pod:*   le:*   git_sha:*   route:*\" \n         drop :   true \n     rollupRules : \n       -   name :   \"http_request   latency   by   route   and   git_sha   without   pod\" \n         filter :   \"__name__:http_request_bucket   k8s_pod:*   le:*   git_sha:*   route:*\" \n         transforms : \n         -   transform : \n             type :   \"Increase\" \n         -   rollup : \n             metricName :   \"http_request_bucket\"   # metric name doesn't change \n             groupBy :   [ \"le\" ,   \"git_sha\" ,   \"route\" ,   \"status_code\" ,   \"region\" ] \n             aggregations :   [ \"Sum\" ] \n         -   transform : \n             type :   \"Add\" \n         storagePolicies : \n         -   resolution :   30s \n           retention :   720h   Note:  In order to store rolled up metrics in an  unaggregated  namespace,\na matching  aggregated  namespace must be added to the coordinator config. For \nexample, if in the above rule, the  720h  namespace under  storagePolicies  \nis  unaggregated , the following will need to be added to the coordinator config.  -   namespace :   default \n   resolution :   30s \n   retention :   720h \n   type :   aggregated \n   downsample : \n     all :   false",
            "title": "Rollup Rules"
        },
        {
            "location": "/operational_guide/upgrading_m3/",
            "text": "Upgrading M3\n\n\nOverview\n\n\nThis guide explains how to upgrade M3 from one version to another (e.g. from 0.14.0 to 0.15.0).\nThis includes upgrading:\n\n\n\n\nm3dbnode\n\n\nm3coordinator\n\n\nm3query\n\n\nm3aggregator\n\n\n\n\nm3dbnode\n\n\nGraphs to monitor\n\n\nWhile upgrading M3DB nodes, it's important to monitor the status of bootstrapping the individual nodes. This can be monitored using the \nM3DB Node Details\n dashboard.\nTypically, the \nBootstrapped\n graph under \nBackground Tasks\n and the graphs within the \nCPU and Memory Utilization\n give a good understanding of how well bootstrapping is going.\n\n\nKubernetes\n\n\nIf running \nM3DB\n on Kubernetes, upgrade by completing the following steps. \n\n\n\n\n\n\nIdentify the version of m3dbnode to upgrade to \non Quay\n.\n\n\n\n\n\n\nReplace the Docker image in the \nStatefulSet\n manifest (or \nm3db-operator\n manifest) to be the new version of m3dbnode.\n\n\n\n\n\n\nspec\n:\n\n  \nimage\n:\n \nquay.io/m3db/m3dbnode:$VERSION\n\n\n\n\n\n\n\nOnce updated, apply the updated manifest and a rolling restart will be performed. You must wait until the \nStatefulSet\n is entirely upgraded and bootstrapped (as per the M3DB Node Details dashboard) before proceeding to the next \nStatefulSet\n otherwise multiple replicas will be unavailable at once.\n\n\n\n\nkubectl apply -f <m3dbnode_manifest>\n\n\n\n\nDowngrading\n\n\nThe \nupgrading\n steps above can also be used to downgrade M3DB. However, it is important to refer to the release notes to make sure that versions are\nbackwards compatible.\n\n\nm3coordinator\n\n\nm3coordinator\n can be upgraded using similar steps as \nm3dbnode\n, however, the images can be \nfound here\n instead.\n\n\nm3query\n\n\nm3query\n can be upgraded using similar steps as \nm3dbnode\n, however, the images can be \nfound here\n instead.\n\n\nm3aggregator\n\n\nm3aggregator\n can be upgraded using similar steps as \nm3dbnode\n, however, the images can be \nfound here\n instead.\n\n\nNon-Kubernetes\n\n\nIt is very important that for each replica set, only one node gets upgraded at a time. However, multiple nodes can be upgraded across replica sets. \n\n\n1) Download new binary (linux example below).\n\n\nwget \n\"https://github.com/m3db/m3/releases/download/v\n$VERSION\n/m3_\n$VERSION_linux_amd64\n.tar.gz\"\n \n&&\n tar xvzf m3_\n$VERSION_linux_amd64\n.tar.gz \n&&\n rm m3_\n$VERSION_linux_amd64\n.tar.gz\n\n\n\n\n2) Stop and upgrade one M3DB node at a time per replica set using the \nsystemd unit\n.\n\n\n# stop m3dbnode\n\nsudo systemctl stop m3dbnode\n\n\n# start m3dbnode with the new binary (which should be placed in the path specified in the systemd unit)\n\nsudo systemctl start m3dbnode\n\n\n\n\nNote:\n If unable to stop \nm3dbnode\n using \nsystemctl\n, use \npkill\n instead.\n\n\n# stop m3dbnode\n\npkill m3dbnode\n\n\n# start m3dbnode with new binary\n\n./m3_\n$VERSION_linux_amd64\n/m3dbnode -f <config-name.yml>\n\n\n\n\n3) Confirm m3dbnode has finished bootstrapping.\n\n\n20:10:12.911218[I] updating database namespaces [{adds [default]} {updates []} {removals []}]\n20:10:13.462798[I] node tchannelthrift: listening on 0.0.0.0:9000\n20:10:13.463107[I] cluster tchannelthrift: listening on 0.0.0.0:9001\n20:10:13.747173[I] node httpjson: listening on 0.0.0.0:9002\n20:10:13.747506[I] cluster httpjson: listening on 0.0.0.0:9003\n20:10:13.747763[I] bootstrapping shards for range starting ...\n...\n20:10:13.757834[I] bootstrap finished [{namespace metrics} {duration 10.1261ms}]\n20:10:13.758001[I] bootstrapped\n20:10:14.764771[I] successfully updated topology to 3 hosts\n\n\n\n\n4) Repeat steps 2 and 3 until all nodes have been upgraded.",
            "title": "Upgrading M3"
        },
        {
            "location": "/operational_guide/upgrading_m3/#upgrading-m3",
            "text": "",
            "title": "Upgrading M3"
        },
        {
            "location": "/operational_guide/upgrading_m3/#overview",
            "text": "This guide explains how to upgrade M3 from one version to another (e.g. from 0.14.0 to 0.15.0).\nThis includes upgrading:   m3dbnode  m3coordinator  m3query  m3aggregator",
            "title": "Overview"
        },
        {
            "location": "/operational_guide/upgrading_m3/#m3dbnode",
            "text": "",
            "title": "m3dbnode"
        },
        {
            "location": "/operational_guide/upgrading_m3/#graphs-to-monitor",
            "text": "While upgrading M3DB nodes, it's important to monitor the status of bootstrapping the individual nodes. This can be monitored using the  M3DB Node Details  dashboard.\nTypically, the  Bootstrapped  graph under  Background Tasks  and the graphs within the  CPU and Memory Utilization  give a good understanding of how well bootstrapping is going.",
            "title": "Graphs to monitor"
        },
        {
            "location": "/operational_guide/upgrading_m3/#kubernetes",
            "text": "If running  M3DB  on Kubernetes, upgrade by completing the following steps.     Identify the version of m3dbnode to upgrade to  on Quay .    Replace the Docker image in the  StatefulSet  manifest (or  m3db-operator  manifest) to be the new version of m3dbnode.    spec : \n   image :   quay.io/m3db/m3dbnode:$VERSION    Once updated, apply the updated manifest and a rolling restart will be performed. You must wait until the  StatefulSet  is entirely upgraded and bootstrapped (as per the M3DB Node Details dashboard) before proceeding to the next  StatefulSet  otherwise multiple replicas will be unavailable at once.   kubectl apply -f <m3dbnode_manifest>",
            "title": "Kubernetes"
        },
        {
            "location": "/operational_guide/upgrading_m3/#downgrading",
            "text": "The  upgrading  steps above can also be used to downgrade M3DB. However, it is important to refer to the release notes to make sure that versions are\nbackwards compatible.",
            "title": "Downgrading"
        },
        {
            "location": "/operational_guide/upgrading_m3/#m3coordinator",
            "text": "m3coordinator  can be upgraded using similar steps as  m3dbnode , however, the images can be  found here  instead.",
            "title": "m3coordinator"
        },
        {
            "location": "/operational_guide/upgrading_m3/#m3query",
            "text": "m3query  can be upgraded using similar steps as  m3dbnode , however, the images can be  found here  instead.",
            "title": "m3query"
        },
        {
            "location": "/operational_guide/upgrading_m3/#m3aggregator",
            "text": "m3aggregator  can be upgraded using similar steps as  m3dbnode , however, the images can be  found here  instead.",
            "title": "m3aggregator"
        },
        {
            "location": "/operational_guide/upgrading_m3/#non-kubernetes",
            "text": "It is very important that for each replica set, only one node gets upgraded at a time. However, multiple nodes can be upgraded across replica sets.   1) Download new binary (linux example below).  wget  \"https://github.com/m3db/m3/releases/download/v $VERSION /m3_ $VERSION_linux_amd64 .tar.gz\"   &&  tar xvzf m3_ $VERSION_linux_amd64 .tar.gz  &&  rm m3_ $VERSION_linux_amd64 .tar.gz  2) Stop and upgrade one M3DB node at a time per replica set using the  systemd unit .  # stop m3dbnode \nsudo systemctl stop m3dbnode # start m3dbnode with the new binary (which should be placed in the path specified in the systemd unit) \nsudo systemctl start m3dbnode  Note:  If unable to stop  m3dbnode  using  systemctl , use  pkill  instead.  # stop m3dbnode \npkill m3dbnode # start m3dbnode with new binary \n./m3_ $VERSION_linux_amd64 /m3dbnode -f <config-name.yml>  3) Confirm m3dbnode has finished bootstrapping.  20:10:12.911218[I] updating database namespaces [{adds [default]} {updates []} {removals []}]\n20:10:13.462798[I] node tchannelthrift: listening on 0.0.0.0:9000\n20:10:13.463107[I] cluster tchannelthrift: listening on 0.0.0.0:9001\n20:10:13.747173[I] node httpjson: listening on 0.0.0.0:9002\n20:10:13.747506[I] cluster httpjson: listening on 0.0.0.0:9003\n20:10:13.747763[I] bootstrapping shards for range starting ...\n...\n20:10:13.757834[I] bootstrap finished [{namespace metrics} {duration 10.1261ms}]\n20:10:13.758001[I] bootstrapped\n20:10:14.764771[I] successfully updated topology to 3 hosts  4) Repeat steps 2 and 3 until all nodes have been upgraded.",
            "title": "Non-Kubernetes"
        },
        {
            "location": "/operational_guide/repairs/",
            "text": "Background Repairs (beta)\n\n\nNote:\n This feature is in beta and only available for use with M3DB when run with the inverted index off. It can be run with the inverted index on however metrics will not be re-indexed if they are repaired so will be invisible to that node for queries.\n\n\nOverview\n\n\nBackground repairs enable M3DB to eventually reach a consistent state such that all nodes have identical view.\nAn M3DB cluster can be configured to repair itself in the background. If background repairs are enabled, M3DB nodes will continuously scan the metadata of other nodes. If a mismatch is detected, affected nodes will perform a repair such that each node in the cluster eventually settles on a consistent view of the data.\n\n\nA repair is performed individually by each node when it detects a mismatch between its metadata and the metadata of its peers. Each node will stream the data for the relevant series, merge the data from its peers with its own, and then write out the resulting merged dataset to disk to make the repair durable. In other words, there is no coordination between individual nodes during the repair process, each node is detecting mismatches on its own and performing a \"best effort\" repair by merging all available data from all peers into a new stream.\n\n\nConfiguration\n\n\nThe feature can be enabled by adding the following configuration to \nm3dbnode.yml\n under the \ndb\n section:\n\n\ndb\n:\n\n  \n... (other configuration)\n\n  \nrepair\n:\n\n    \nenabled\n:\n \ntrue\n\n\n\n\n\nBy default M3DB will limit the amount of repaired data that can be held in memory at once to 2GiB. This is intended to prevent the M3DB nodes from streaming data from their peers too quickly and running out of memory. Once the 2GiB limit is hit the repair process will throttle itself until some of the streamed data has been flushed to disk (and as a result can be evicted from memory). This limit can be overriden with the following configuration:\n\n\ndb\n:\n\n  \n... (other configuration)\n\n  \nlimits\n:\n\n    \nmaxOutstandingRepairedBytes\n:\n \n2147483648\n \n# 2GiB\n\n\n\n\n\nIn addition, the following two optional fields can also be configured:\n\n\ndb\n:\n\n  \n... (other configuration)\n\n  \nrepair\n:\n\n    \nenabled\n:\n \ntrue\n\n    \nthrottle\n:\n \n10s\n\n    \ncheckInterval\n:\n \n10s\n\n\n\n\n\nThe \nthrottle\n field controls how long the M3DB node will pause between repairing each shard/blockStart combination and the \ncheckInterval\n field controls how often M3DB will run the scheduling/prioritization algorithm that determines which blocks to repair next. In most situations, operators should omit these fields and rely on the default values.\n\n\nCaveats and Limitations\n\n\n\n\nBackground repairs do not currently support M3DB's inverted index; as a result, it can only be used for clusters / namespaces where the indexing feature is disabled.\n\n\nBackground repairs will wait until (\nblock start\n + \nblock size\n + \nbuffer past\n) has elapsed before attempting to repair a block. For example, if M3DB is configured with a 2 hour block size and a 20 minute buffer past that M3DB will not attempt to repair the \n12PM->2PM\n block until at least \n2:20PM\n. This limitation is in place primarily to reduce \"churn\" caused by repairing mutable data that is actively being modified. \nNote\n: This limitation has no impact or negative interaction with M3DB's cold writes feature. In other words, even though it may take some time before a block becomes available for repairs, M3DB will repair the same block repeatedly until it falls out of retention so mismatches between nodes that were caused by \"cold\" writes will still eventually be repaired.",
            "title": "Repairs"
        },
        {
            "location": "/operational_guide/repairs/#background-repairs-beta",
            "text": "Note:  This feature is in beta and only available for use with M3DB when run with the inverted index off. It can be run with the inverted index on however metrics will not be re-indexed if they are repaired so will be invisible to that node for queries.",
            "title": "Background Repairs (beta)"
        },
        {
            "location": "/operational_guide/repairs/#overview",
            "text": "Background repairs enable M3DB to eventually reach a consistent state such that all nodes have identical view.\nAn M3DB cluster can be configured to repair itself in the background. If background repairs are enabled, M3DB nodes will continuously scan the metadata of other nodes. If a mismatch is detected, affected nodes will perform a repair such that each node in the cluster eventually settles on a consistent view of the data.  A repair is performed individually by each node when it detects a mismatch between its metadata and the metadata of its peers. Each node will stream the data for the relevant series, merge the data from its peers with its own, and then write out the resulting merged dataset to disk to make the repair durable. In other words, there is no coordination between individual nodes during the repair process, each node is detecting mismatches on its own and performing a \"best effort\" repair by merging all available data from all peers into a new stream.",
            "title": "Overview"
        },
        {
            "location": "/operational_guide/repairs/#configuration",
            "text": "The feature can be enabled by adding the following configuration to  m3dbnode.yml  under the  db  section:  db : \n   ... (other configuration) \n   repair : \n     enabled :   true   By default M3DB will limit the amount of repaired data that can be held in memory at once to 2GiB. This is intended to prevent the M3DB nodes from streaming data from their peers too quickly and running out of memory. Once the 2GiB limit is hit the repair process will throttle itself until some of the streamed data has been flushed to disk (and as a result can be evicted from memory). This limit can be overriden with the following configuration:  db : \n   ... (other configuration) \n   limits : \n     maxOutstandingRepairedBytes :   2147483648   # 2GiB   In addition, the following two optional fields can also be configured:  db : \n   ... (other configuration) \n   repair : \n     enabled :   true \n     throttle :   10s \n     checkInterval :   10s   The  throttle  field controls how long the M3DB node will pause between repairing each shard/blockStart combination and the  checkInterval  field controls how often M3DB will run the scheduling/prioritization algorithm that determines which blocks to repair next. In most situations, operators should omit these fields and rely on the default values.",
            "title": "Configuration"
        },
        {
            "location": "/operational_guide/repairs/#caveats-and-limitations",
            "text": "Background repairs do not currently support M3DB's inverted index; as a result, it can only be used for clusters / namespaces where the indexing feature is disabled.  Background repairs will wait until ( block start  +  block size  +  buffer past ) has elapsed before attempting to repair a block. For example, if M3DB is configured with a 2 hour block size and a 20 minute buffer past that M3DB will not attempt to repair the  12PM->2PM  block until at least  2:20PM . This limitation is in place primarily to reduce \"churn\" caused by repairing mutable data that is actively being modified.  Note : This limitation has no impact or negative interaction with M3DB's cold writes feature. In other words, even though it may take some time before a block becomes available for repairs, M3DB will repair the same block repeatedly until it falls out of retention so mismatches between nodes that were caused by \"cold\" writes will still eventually be repaired.",
            "title": "Caveats and Limitations"
        },
        {
            "location": "/operational_guide/replication_between_clusters/",
            "text": "Replication between clusters (beta)\n\n\nOverview\n\n\nM3DB clusters can be configured to passively replicate data from other clusters. This feature is most commonly used when operators wish to run two (or more) regional clusters that function independently while passively replicating data from the other cluster in an eventually consistent manner.\n\n\nThe cross-cluster replication feature is built on-top of the \nbackground repairs\n feature. As a result, it has all the same caveats and limitations. Specifically, it does not currently work with clusters that use M3DB's indexing feature and the replication delay between two clusters will be at least (\nblock size\n + \nbufferPast\n) for data written at the beginning of a block for a given namespace. For use-cases where a large replication delay is unacceptable, the current recommendation is to dual-write to both clusters in parallel and then rely upon the cross-cluster replication feature to repair any discrepancies between the clusters caused by failed dual-writes. This recommendation is likely to change in the future once support for low-latency replication is added to M3DB in the form of commitlog tailing.\n\n\nWhile cross-cluster replication is built on top of the background repairs feature, background repairs do not need to be enabled for cross-cluster replication to be enabled. In other words, clusters can be configured such that:\n\n\n\n\nBackground repairs (within a cluster) are disabled and replication is also disabled.\n\n\nBackground repairs (within a cluster) are enabled, but replication is disabled.\n\n\nBackground repairs (within a cluster) are disabled, but replication is enabled.\n\n\nBackground repairs (within a cluster) are enabled and replication is also enabled.\n\n\n\n\nConfiguration\n\n\nImportant\n: All M3DB clusters involved in the cross-cluster replication process must be configured such that they have the exact same:\n\n\n\n\nNumber of shards\n\n\nReplication factor\n\n\nNamespace configuration\n\n\n\n\nThe replication feature can be enabled by adding the following configuration to \nm3dbnode.yml\n under the \ndb\n section:\n\n\ndb\n:\n\n  \n... (other configuration)\n\n  \nreplication\n:\n\n    \nclusters\n:\n\n      \n-\n \nname\n:\n \n\"some-other-cluster\"\n\n        \nrepairEnabled\n:\n \ntrue\n\n        \nclient\n:\n\n          \nconfig\n:\n\n            \nservice\n:\n\n              \nenv\n:\n \n<ETCD_ENV>\n\n              \nzone\n:\n \n<ETCD_ZONE>\n\n              \nservice\n:\n \n<ETCD_SERVICE>\n\n              \ncacheDir\n:\n \n/var/lib/m3kv\n\n              \netcdClusters\n:\n\n                \n-\n \nzone\n:\n \n<ETCD_ZONE>\n\n                  \nendpoints\n:\n\n                    \n-\n \n<ETCD_ENDPOINT_01_HOST>:<ETCD_ENDPOINT_01_PORT>\n\n\n\n\n\nNote that the \nrepairEnabled\n field in the configuration above is independent of the \nenabled\n field under the \nrepairs\n section. For example, the example above will enable replication of data from \nsome-other-cluster\n but will not perform background repairs within the cluster the M3DB node belongs to.\n\n\nHowever, the following configuration:\n\n\ndb\n:\n\n  \n... (other configuration)\n\n  \nrepair\n:\n\n    \nenabled\n:\n \ntrue\n\n\n  \nreplication\n:\n\n    \nclusters\n:\n\n      \n-\n \nname\n:\n \n\"some-other-cluster\"\n\n        \nrepairEnabled\n:\n \ntrue\n\n        \nclient\n:\n\n          \nconfig\n:\n\n            \nservice\n:\n\n              \nenv\n:\n \n<ETCD_ENV>\n\n              \nzone\n:\n \n<ETCD_ZONE>\n\n              \nservice\n:\n \n<ETCD_SERVICE>\n\n              \ncacheDir\n:\n \n/var/lib/m3kv\n\n              \netcdClusters\n:\n\n                \n-\n \nzone\n:\n \n<ETCD_ZONE>\n\n                  \nendpoints\n:\n\n                    \n-\n \n<ETCD_ENDPOINT_01_HOST>:<ETCD_ENDPOINT_01_PORT>\n\n\n\n\n\nwould enable both replication of data from \nsome-other-cluster\n as well as background repairs within the cluster that the M3DB node belongs to.",
            "title": "Replication Between Clusters"
        },
        {
            "location": "/operational_guide/replication_between_clusters/#replication-between-clusters-beta",
            "text": "",
            "title": "Replication between clusters (beta)"
        },
        {
            "location": "/operational_guide/replication_between_clusters/#overview",
            "text": "M3DB clusters can be configured to passively replicate data from other clusters. This feature is most commonly used when operators wish to run two (or more) regional clusters that function independently while passively replicating data from the other cluster in an eventually consistent manner.  The cross-cluster replication feature is built on-top of the  background repairs  feature. As a result, it has all the same caveats and limitations. Specifically, it does not currently work with clusters that use M3DB's indexing feature and the replication delay between two clusters will be at least ( block size  +  bufferPast ) for data written at the beginning of a block for a given namespace. For use-cases where a large replication delay is unacceptable, the current recommendation is to dual-write to both clusters in parallel and then rely upon the cross-cluster replication feature to repair any discrepancies between the clusters caused by failed dual-writes. This recommendation is likely to change in the future once support for low-latency replication is added to M3DB in the form of commitlog tailing.  While cross-cluster replication is built on top of the background repairs feature, background repairs do not need to be enabled for cross-cluster replication to be enabled. In other words, clusters can be configured such that:   Background repairs (within a cluster) are disabled and replication is also disabled.  Background repairs (within a cluster) are enabled, but replication is disabled.  Background repairs (within a cluster) are disabled, but replication is enabled.  Background repairs (within a cluster) are enabled and replication is also enabled.",
            "title": "Overview"
        },
        {
            "location": "/operational_guide/replication_between_clusters/#configuration",
            "text": "Important : All M3DB clusters involved in the cross-cluster replication process must be configured such that they have the exact same:   Number of shards  Replication factor  Namespace configuration   The replication feature can be enabled by adding the following configuration to  m3dbnode.yml  under the  db  section:  db : \n   ... (other configuration) \n   replication : \n     clusters : \n       -   name :   \"some-other-cluster\" \n         repairEnabled :   true \n         client : \n           config : \n             service : \n               env :   <ETCD_ENV> \n               zone :   <ETCD_ZONE> \n               service :   <ETCD_SERVICE> \n               cacheDir :   /var/lib/m3kv \n               etcdClusters : \n                 -   zone :   <ETCD_ZONE> \n                   endpoints : \n                     -   <ETCD_ENDPOINT_01_HOST>:<ETCD_ENDPOINT_01_PORT>   Note that the  repairEnabled  field in the configuration above is independent of the  enabled  field under the  repairs  section. For example, the example above will enable replication of data from  some-other-cluster  but will not perform background repairs within the cluster the M3DB node belongs to.  However, the following configuration:  db : \n   ... (other configuration) \n   repair : \n     enabled :   true \n\n   replication : \n     clusters : \n       -   name :   \"some-other-cluster\" \n         repairEnabled :   true \n         client : \n           config : \n             service : \n               env :   <ETCD_ENV> \n               zone :   <ETCD_ZONE> \n               service :   <ETCD_SERVICE> \n               cacheDir :   /var/lib/m3kv \n               etcdClusters : \n                 -   zone :   <ETCD_ZONE> \n                   endpoints : \n                     -   <ETCD_ENDPOINT_01_HOST>:<ETCD_ENDPOINT_01_PORT>   would enable both replication of data from  some-other-cluster  as well as background repairs within the cluster that the M3DB node belongs to.",
            "title": "Configuration"
        },
        {
            "location": "/integrations/prometheus/",
            "text": "Prometheus\n\n\nThis document is a getting started guide to integrating M3DB with Prometheus.\n\n\nM3 Coordinator configuration\n\n\nTo write to a remote M3DB cluster the simplest configuration is to run \nm3coordinator\n as a sidecar alongside Prometheus.\n\n\nStart by downloading the \nconfig template\n. Update the \nnamespaces\n and the \nclient\n section for a new cluster to match your cluster's configuration.\n\n\nYou'll need to specify the static IPs or hostnames of your M3DB seed nodes, and the name and retention values of the namespace you set up.  You can leave the namespace storage metrics type as \nunaggregated\n since it's required by default to have a cluster that receives all Prometheus metrics unaggregated.  In the future you might also want to aggregate and downsample metrics for longer retention, and you can come back and update the config once you've setup those clusters. You can read more about our aggregation functionality \nhere\n.\n\n\nIt should look something like:\n\n\nlistenAddress:\n  type: \"config\"\n  value: \"0.0.0.0:7201\"\n\nlogging:\n  level: info\n\nmetrics:\n  scope:\n    prefix: \"coordinator\"\n  prometheus:\n    handlerPath: /metrics\n    listenAddress: 0.0.0.0:7203 # until https://github.com/m3db/m3/issues/682 is resolved\n  sanitization: prometheus\n  samplingRate: 1.0\n  extended: none\n\ntagOptions:\n  idScheme: quoted\n\nclusters:\n   - namespaces:\n# We created a namespace called \"default\" and had set it to retention \"48h\".\n       - namespace: default\n         retention: 48h\n         type: unaggregated\n     client:\n       config:\n         service:\n           env: default_env\n           zone: embedded\n           service: m3db\n           cacheDir: /var/lib/m3kv\n           etcdClusters:\n             - zone: embedded\n               endpoints:\n# We have five M3DB nodes but only three are seed nodes, they are listed here.\n                 - M3DB_NODE_01_STATIC_IP_ADDRESS:2379\n                 - M3DB_NODE_02_STATIC_IP_ADDRESS:2379\n                 - M3DB_NODE_03_STATIC_IP_ADDRESS:2379\n       writeConsistencyLevel: majority\n       readConsistencyLevel: unstrict_majority\n       writeTimeout: 10s\n       fetchTimeout: 15s\n       connectTimeout: 20s\n       writeRetry:\n         initialBackoff: 500ms\n         backoffFactor: 3\n         maxRetries: 2\n         jitter: true\n       fetchRetry:\n         initialBackoff: 500ms\n         backoffFactor: 2\n         maxRetries: 3\n         jitter: true\n       backgroundHealthCheckFailLimit: 4\n       backgroundHealthCheckFailThrottleFactor: 0.5\n\n\n\n\nNow start the process up:\n\n\nm3coordinator -f <config-name.yml>\n\n\n\n\nOr, use the docker container:\n\n\ndocker pull quay.io/m3db/m3coordinator:latest\ndocker run -p 7201:7201 --name m3coordinator -v <config-name.yml>:/etc/m3coordinator/m3coordinator.yml quay.io/m3db/m3coordinator:latest\n\n\n\n\nPrometheus configuration\n\n\nAdd to your Prometheus configuration the \nm3coordinator\n sidecar remote read/write endpoints, something like:\n\n\nremote_read:\n  - url: \"http://localhost:7201/api/v1/prom/remote/read\"\n    # To test reading even when local Prometheus has the data\n    read_recent: true\nremote_write:\n  - url: \"http://localhost:7201/api/v1/prom/remote/write\"\n\n\n\n\nAlso, we recommend adding \nM3DB\n and \nM3Coordinator\n/\nM3Query\n to your list of jobs under \nscrape_configs\n so that you can monitor them using Prometheus. With this scraping setup, you can also use our pre-configured \nM3DB Grafana dashboard\n.\n\n\n-\n \njob_name:\n \n'm\n3\ndb'\n\n  \nstatic_configs:\n\n    \n-\n \ntargets:\n \n[\n'<M\n3\nDB_HOST_NAME_\n1\n>:\n7203\n'\n,\n \n'<M\n3\nDB_HOST_NAME_\n2\n>:\n7203\n'\n,\n \n'<M\n3\nDB_HOST_NAME_\n3\n>:\n7203\n'\n]\n\n\n-\n \njob_name:\n \n'm\n3\ncoordinator'\n\n  \nstatic_configs:\n\n    \n-\n \ntargets:\n \n[\n'<M\n3\nCOORDINATOR_HOST_NAME_\n1\n>:\n7203\n'\n]\n\n\n\n\n\nNOTE:\n If you are running \nM3DB\n with embedded \nM3Coordinator\n, you should only have one job. We recommend just calling this job \nm3\n. For example:\n\n\n-\n \njob_name:\n \n'm\n3\n'\n\n  \nstatic_configs:\n\n    \n-\n \ntargets:\n \n[\n'<HOST_NAME>:\n7203\n'\n]\n\n\n\n\n\nQuerying With Grafana\n\n\nWhen using the Prometheus integration with Grafana, there are two different ways you can query for your metrics. The first option is to configure Grafana to query Prometheus directly by following \nthese instructions.\n\n\nAlternatively, you can configure Grafana to read metrics directly from \nM3Coordinator\n in which case you will bypass Prometheus entirely and use M3's \nPromQL\n engine instead. To set this up, follow the same instructions from the previous step, but set the \nurl\n to: \nhttp://<M3_COORDINATOR_HOST_NAME>:7201\n.",
            "title": "Prometheus"
        },
        {
            "location": "/integrations/prometheus/#prometheus",
            "text": "This document is a getting started guide to integrating M3DB with Prometheus.",
            "title": "Prometheus"
        },
        {
            "location": "/integrations/prometheus/#m3-coordinator-configuration",
            "text": "To write to a remote M3DB cluster the simplest configuration is to run  m3coordinator  as a sidecar alongside Prometheus.  Start by downloading the  config template . Update the  namespaces  and the  client  section for a new cluster to match your cluster's configuration.  You'll need to specify the static IPs or hostnames of your M3DB seed nodes, and the name and retention values of the namespace you set up.  You can leave the namespace storage metrics type as  unaggregated  since it's required by default to have a cluster that receives all Prometheus metrics unaggregated.  In the future you might also want to aggregate and downsample metrics for longer retention, and you can come back and update the config once you've setup those clusters. You can read more about our aggregation functionality  here .  It should look something like:  listenAddress:\n  type: \"config\"\n  value: \"0.0.0.0:7201\"\n\nlogging:\n  level: info\n\nmetrics:\n  scope:\n    prefix: \"coordinator\"\n  prometheus:\n    handlerPath: /metrics\n    listenAddress: 0.0.0.0:7203 # until https://github.com/m3db/m3/issues/682 is resolved\n  sanitization: prometheus\n  samplingRate: 1.0\n  extended: none\n\ntagOptions:\n  idScheme: quoted\n\nclusters:\n   - namespaces:\n# We created a namespace called \"default\" and had set it to retention \"48h\".\n       - namespace: default\n         retention: 48h\n         type: unaggregated\n     client:\n       config:\n         service:\n           env: default_env\n           zone: embedded\n           service: m3db\n           cacheDir: /var/lib/m3kv\n           etcdClusters:\n             - zone: embedded\n               endpoints:\n# We have five M3DB nodes but only three are seed nodes, they are listed here.\n                 - M3DB_NODE_01_STATIC_IP_ADDRESS:2379\n                 - M3DB_NODE_02_STATIC_IP_ADDRESS:2379\n                 - M3DB_NODE_03_STATIC_IP_ADDRESS:2379\n       writeConsistencyLevel: majority\n       readConsistencyLevel: unstrict_majority\n       writeTimeout: 10s\n       fetchTimeout: 15s\n       connectTimeout: 20s\n       writeRetry:\n         initialBackoff: 500ms\n         backoffFactor: 3\n         maxRetries: 2\n         jitter: true\n       fetchRetry:\n         initialBackoff: 500ms\n         backoffFactor: 2\n         maxRetries: 3\n         jitter: true\n       backgroundHealthCheckFailLimit: 4\n       backgroundHealthCheckFailThrottleFactor: 0.5  Now start the process up:  m3coordinator -f <config-name.yml>  Or, use the docker container:  docker pull quay.io/m3db/m3coordinator:latest\ndocker run -p 7201:7201 --name m3coordinator -v <config-name.yml>:/etc/m3coordinator/m3coordinator.yml quay.io/m3db/m3coordinator:latest",
            "title": "M3 Coordinator configuration"
        },
        {
            "location": "/integrations/prometheus/#prometheus-configuration",
            "text": "Add to your Prometheus configuration the  m3coordinator  sidecar remote read/write endpoints, something like:  remote_read:\n  - url: \"http://localhost:7201/api/v1/prom/remote/read\"\n    # To test reading even when local Prometheus has the data\n    read_recent: true\nremote_write:\n  - url: \"http://localhost:7201/api/v1/prom/remote/write\"  Also, we recommend adding  M3DB  and  M3Coordinator / M3Query  to your list of jobs under  scrape_configs  so that you can monitor them using Prometheus. With this scraping setup, you can also use our pre-configured  M3DB Grafana dashboard .  -   job_name:   'm 3 db' \n   static_configs: \n     -   targets:   [ '<M 3 DB_HOST_NAME_ 1 >: 7203 ' ,   '<M 3 DB_HOST_NAME_ 2 >: 7203 ' ,   '<M 3 DB_HOST_NAME_ 3 >: 7203 ' ]  -   job_name:   'm 3 coordinator' \n   static_configs: \n     -   targets:   [ '<M 3 COORDINATOR_HOST_NAME_ 1 >: 7203 ' ]   NOTE:  If you are running  M3DB  with embedded  M3Coordinator , you should only have one job. We recommend just calling this job  m3 . For example:  -   job_name:   'm 3 ' \n   static_configs: \n     -   targets:   [ '<HOST_NAME>: 7203 ' ]",
            "title": "Prometheus configuration"
        },
        {
            "location": "/integrations/prometheus/#querying-with-grafana",
            "text": "When using the Prometheus integration with Grafana, there are two different ways you can query for your metrics. The first option is to configure Grafana to query Prometheus directly by following  these instructions.  Alternatively, you can configure Grafana to read metrics directly from  M3Coordinator  in which case you will bypass Prometheus entirely and use M3's  PromQL  engine instead. To set this up, follow the same instructions from the previous step, but set the  url  to:  http://<M3_COORDINATOR_HOST_NAME>:7201 .",
            "title": "Querying With Grafana"
        },
        {
            "location": "/integrations/graphite/",
            "text": "Graphite\n\n\nThis document is a getting started guide to integrating the M3 stack with Graphite.\n\n\nOverview\n\n\nM3 supports ingesting Graphite metrics using the \nCarbon plaintext protocol\n. We also support a variety of aggregation and storage policies for the ingestion pathway (similar to \nstorage-schemas.conf\n when using Graphite Carbon) that are documented below. Finally, on the query side, we support the majority of \ngraphite query functions\n.\n\n\nIngestion\n\n\nSetting up the M3 stack to ingest carbon metrics is straightforward. First, make sure you've followed our \nother documentation\n to get m3coordinator and M3DB setup. Also, familiarize yourself with how M3 \nhandles aggregation\n.\n\n\nOnce you have both of those services running properly, modify your m3coordinator configuration to add the following lines and restart it:\n\n\ncarbon\n:\n\n  \ningester\n:\n\n    \nlistenAddress\n:\n \n\"0.0.0.0:7204\"\n\n\n\n\n\nThis will enable a line-based TCP carbon ingestion server on the specified port. By default, the server will write all carbon metrics to every aggregated namespace specified in the m3coordinator \nconfiguration file\n and aggregate them using a default strategy of \nmean\n (equivalent to Graphite's \nAverage\n).\n\n\nThis default setup makes sense if your carbon metrics are unaggregated, however, if you've already aggregated your data using something like \nstatsite\n then you may want to disable M3 aggregation. In that case, you can do something like the following:\n\n\ncarbon\n:\n\n  \ningester\n:\n\n    \nlistenAddress\n:\n \n\"0.0.0.0:7204\"\n\n    \nrules\n:\n\n      \n-\n \npattern\n:\n \n.*\n\n        \naggregation\n:\n\n          \nenabled\n:\n \nfalse\n\n        \npolicies\n:\n\n          \n-\n \nresolution\n:\n \n1m\n\n            \nretention\n:\n \n48h\n\n\n\n\n\nThis replaces M3's default behavior with a single rule which states that all metrics (since \n.*\n will match any string) should be written to whichever aggregated M3DB namespace has been configured with a resolution of \n1 minute\n and a retention of \n48 hours\n, bypassing aggregation / downsampling altogether. Note that there \nmust\n be a configured M3DB namespace with the specified resolution/retention or the coordinator will fail to start.\n\n\nIn the situation that you choose to use M3's aggregation functionality, there are a variety of aggregation types you can choose from. For example:\n\n\ncarbon\n:\n\n  \ningester\n:\n\n    \nlistenAddress\n:\n \n\"0.0.0.0:7204\"\n\n    \nrules\n:\n\n      \n-\n \npattern\n:\n \n.*\n\n        \naggregation\n:\n\n          \ntype\n:\n \nlast\n\n        \npolicies\n:\n\n          \n-\n \nresolution\n:\n \n1m\n\n            \nretention\n:\n \n48h\n\n\n\n\n\nThe config above will aggregate ingested carbon metrics into \n1 minute\n tiles, but instead of taking the \nmean\n of every datapoint, it will emit the \nlast\n datapoint that was received within a given tile's window.\n\n\nSimilar to Graphite's \nstorage-schemas.conf\n, M3 carbon ingestion rules are applied in order and only the first pattern that matches is applied. In addition, the rules can be as simple or as complex as you like. For example:\n\n\ncarbon\n:\n\n  \ningester\n:\n\n    \nlistenAddress\n:\n \n\"0.0.0.0:7204\"\n\n    \nrules\n:\n\n      \n-\n \npattern\n:\n \nstats.internal.financial-service.*\n\n        \naggregation\n:\n\n          \ntype\n:\n \nmax\n\n        \npolicies\n:\n\n          \n-\n \nresolution\n:\n \n1m\n\n            \nretention\n:\n \n4320h\n\n          \n-\n \nresolution\n:\n \n10s\n\n            \nretention\n:\n \n24h\n\n      \n-\n \npattern\n:\n \nstats.internal.rest-proxy.*\n\n        \naggregation\n:\n\n          \ntype\n:\n \nmean\n\n        \npolicies\n:\n\n          \n-\n \nresolution\n:\n \n10s\n\n            \nretention\n:\n \n2h\n\n      \n-\n \npattern\n:\n \nstats.cloud.*\n\n        \naggregation\n:\n\n          \nenabled\n:\n \nfalse\n\n        \npolicies\n:\n\n          \n-\n \nresolution\n:\n \n1m\n\n            \nretention\n:\n \n2h\n\n      \n-\n \npattern\n:\n \n.*\n\n        \naggregation\n:\n\n          \ntype\n:\n \nmean\n\n        \npolicies\n:\n\n          \n-\n \nresolution\n:\n \n1m\n\n            \nretention\n:\n \n48h\n\n\n\n\n\nLets break that down.\n\n\nThe first rule states that any metric matching the pattern \nstats.internal.financial-service.*\n should be aggregated using the \nmax\n function (meaning the datapoint with the highest value that is received in a given window will be retained) to generate two different tiles, one with \n1 minute\n resolution and another with \n10 second\n resolution which will be written out to M3DB namespaces with \n4320 hour\n and \n24 hour\n retentions respectively.\n\n\nThe second rule will aggregate all the metrics coming from our \nrest-proxy\n service using a \nmean\n type aggregation (all datapoints within a given window will be averaged) to generate \n10 second\n tiles and write them out to an M3DB namespace that stores data for \ntwo hours\n.\n\n\nThe third will match any metrics coming from our cloud environment. In this hypoethical example, our cloud metrics are already aggregated using an application like \nstatsite\n, so instead of aggregating them again, we just write them directly to an M3DB namespace that retains data for \ntwo hours\n. Note that while we're not aggregating the data in M3 here, we still need to provide a resolution so that the ingester can match the storage policy to a known M3DB namespace, as well as so that when we fan out queries to multiple namespaces we know the resolution of the data contained in each namespace.\n\n\nFinally, our last rule uses a \"catch-all\" pattern to capture any metrics that don't match any of our other rules and aggregate them using the \nmean\n function into \n1 minute\n tiles which we store for \n48 hours\n.\n\n\nDebug mode\n\n\nIf at any time you're not sure which metrics are being matched by which patterns, or want more visibility into how the carbon ingestion rule are being evaluated, modify the config to enable debug mode:\n\n\ncarbon\n:\n\n  \ningester\n:\n\n    \ndebug\n:\n \ntrue\n\n    \nlistenAddress\n:\n \n\"0.0.0.0:7204\"\n\n\n\n\n\nThis will make the carbon ingestion emit logs for every step that is taking. \nNote\n: If your coordinator is ingesting a lot of data, enabling this mode could bring the proccess to a halt due to the I/O overhead, so use this feature cautiously in production environments.\n\n\nSupported Aggregation Functions\n\n\n\n\nlast\n\n\nmin\n\n\nmax\n\n\nmean\n\n\nmedian\n\n\ncount\n\n\nsum\n\n\nsumsq\n\n\nstdev\n\n\np10\n\n\np20\n\n\np30\n\n\np40\n\n\np50\n\n\np60\n\n\np70\n\n\np80\n\n\np90\n\n\np95\n\n\np99\n\n\np999\n\n\np9999\n\n\n\n\nQuerying\n\n\nM3 supports the the majority of \ngraphite query functions\n and can be used to query metrics that were ingested via the ingestion pathway described above.\n\n\nGrafana\n\n\nM3Coordinator\n implements the Graphite source interface, so you can add it as a \ngraphite\n source in Grafana by following \nthese instructions.\n\n\nNote that you'll need to set the URL to: \nhttp://<M3_COORDINATOR_HOST_NAME>:7201/api/v1/graphite\n\n\nDirect\n\n\nYou can query for metrics directly by issuing HTTP GET requests directly against the \nM3Coordinator\n \n/api/v1/graphite/render\n endpoint which runs on port \n7201\n by default. For example:\n\n\n(\nexport\n \nnow\n=\n$(\ndate +%s\n)\n \n&&\n curl \n\"localhost:7201/api/v1/graphite/render?target=transformNull(foo.*.baz)&from=\n$((\n$now\n-\n300\n))\n\"\n \n|\n jq .\n)\n\n\n\n\n\nwill query for all metrics matching the \nfoo.*.baz\n pattern, applying the \ntransformNull\n function, and returning all datapoints for the last 5 minutes.",
            "title": "Graphite"
        },
        {
            "location": "/integrations/graphite/#graphite",
            "text": "This document is a getting started guide to integrating the M3 stack with Graphite.",
            "title": "Graphite"
        },
        {
            "location": "/integrations/graphite/#overview",
            "text": "M3 supports ingesting Graphite metrics using the  Carbon plaintext protocol . We also support a variety of aggregation and storage policies for the ingestion pathway (similar to  storage-schemas.conf  when using Graphite Carbon) that are documented below. Finally, on the query side, we support the majority of  graphite query functions .",
            "title": "Overview"
        },
        {
            "location": "/integrations/graphite/#ingestion",
            "text": "Setting up the M3 stack to ingest carbon metrics is straightforward. First, make sure you've followed our  other documentation  to get m3coordinator and M3DB setup. Also, familiarize yourself with how M3  handles aggregation .  Once you have both of those services running properly, modify your m3coordinator configuration to add the following lines and restart it:  carbon : \n   ingester : \n     listenAddress :   \"0.0.0.0:7204\"   This will enable a line-based TCP carbon ingestion server on the specified port. By default, the server will write all carbon metrics to every aggregated namespace specified in the m3coordinator  configuration file  and aggregate them using a default strategy of  mean  (equivalent to Graphite's  Average ).  This default setup makes sense if your carbon metrics are unaggregated, however, if you've already aggregated your data using something like  statsite  then you may want to disable M3 aggregation. In that case, you can do something like the following:  carbon : \n   ingester : \n     listenAddress :   \"0.0.0.0:7204\" \n     rules : \n       -   pattern :   .* \n         aggregation : \n           enabled :   false \n         policies : \n           -   resolution :   1m \n             retention :   48h   This replaces M3's default behavior with a single rule which states that all metrics (since  .*  will match any string) should be written to whichever aggregated M3DB namespace has been configured with a resolution of  1 minute  and a retention of  48 hours , bypassing aggregation / downsampling altogether. Note that there  must  be a configured M3DB namespace with the specified resolution/retention or the coordinator will fail to start.  In the situation that you choose to use M3's aggregation functionality, there are a variety of aggregation types you can choose from. For example:  carbon : \n   ingester : \n     listenAddress :   \"0.0.0.0:7204\" \n     rules : \n       -   pattern :   .* \n         aggregation : \n           type :   last \n         policies : \n           -   resolution :   1m \n             retention :   48h   The config above will aggregate ingested carbon metrics into  1 minute  tiles, but instead of taking the  mean  of every datapoint, it will emit the  last  datapoint that was received within a given tile's window.  Similar to Graphite's  storage-schemas.conf , M3 carbon ingestion rules are applied in order and only the first pattern that matches is applied. In addition, the rules can be as simple or as complex as you like. For example:  carbon : \n   ingester : \n     listenAddress :   \"0.0.0.0:7204\" \n     rules : \n       -   pattern :   stats.internal.financial-service.* \n         aggregation : \n           type :   max \n         policies : \n           -   resolution :   1m \n             retention :   4320h \n           -   resolution :   10s \n             retention :   24h \n       -   pattern :   stats.internal.rest-proxy.* \n         aggregation : \n           type :   mean \n         policies : \n           -   resolution :   10s \n             retention :   2h \n       -   pattern :   stats.cloud.* \n         aggregation : \n           enabled :   false \n         policies : \n           -   resolution :   1m \n             retention :   2h \n       -   pattern :   .* \n         aggregation : \n           type :   mean \n         policies : \n           -   resolution :   1m \n             retention :   48h   Lets break that down.  The first rule states that any metric matching the pattern  stats.internal.financial-service.*  should be aggregated using the  max  function (meaning the datapoint with the highest value that is received in a given window will be retained) to generate two different tiles, one with  1 minute  resolution and another with  10 second  resolution which will be written out to M3DB namespaces with  4320 hour  and  24 hour  retentions respectively.  The second rule will aggregate all the metrics coming from our  rest-proxy  service using a  mean  type aggregation (all datapoints within a given window will be averaged) to generate  10 second  tiles and write them out to an M3DB namespace that stores data for  two hours .  The third will match any metrics coming from our cloud environment. In this hypoethical example, our cloud metrics are already aggregated using an application like  statsite , so instead of aggregating them again, we just write them directly to an M3DB namespace that retains data for  two hours . Note that while we're not aggregating the data in M3 here, we still need to provide a resolution so that the ingester can match the storage policy to a known M3DB namespace, as well as so that when we fan out queries to multiple namespaces we know the resolution of the data contained in each namespace.  Finally, our last rule uses a \"catch-all\" pattern to capture any metrics that don't match any of our other rules and aggregate them using the  mean  function into  1 minute  tiles which we store for  48 hours .",
            "title": "Ingestion"
        },
        {
            "location": "/integrations/graphite/#debug-mode",
            "text": "If at any time you're not sure which metrics are being matched by which patterns, or want more visibility into how the carbon ingestion rule are being evaluated, modify the config to enable debug mode:  carbon : \n   ingester : \n     debug :   true \n     listenAddress :   \"0.0.0.0:7204\"   This will make the carbon ingestion emit logs for every step that is taking.  Note : If your coordinator is ingesting a lot of data, enabling this mode could bring the proccess to a halt due to the I/O overhead, so use this feature cautiously in production environments.",
            "title": "Debug mode"
        },
        {
            "location": "/integrations/graphite/#supported-aggregation-functions",
            "text": "last  min  max  mean  median  count  sum  sumsq  stdev  p10  p20  p30  p40  p50  p60  p70  p80  p90  p95  p99  p999  p9999",
            "title": "Supported Aggregation Functions"
        },
        {
            "location": "/integrations/graphite/#querying",
            "text": "M3 supports the the majority of  graphite query functions  and can be used to query metrics that were ingested via the ingestion pathway described above.",
            "title": "Querying"
        },
        {
            "location": "/integrations/graphite/#grafana",
            "text": "M3Coordinator  implements the Graphite source interface, so you can add it as a  graphite  source in Grafana by following  these instructions.  Note that you'll need to set the URL to:  http://<M3_COORDINATOR_HOST_NAME>:7201/api/v1/graphite",
            "title": "Grafana"
        },
        {
            "location": "/integrations/graphite/#direct",
            "text": "You can query for metrics directly by issuing HTTP GET requests directly against the  M3Coordinator   /api/v1/graphite/render  endpoint which runs on port  7201  by default. For example:  ( export   now = $( date +%s )   &&  curl  \"localhost:7201/api/v1/graphite/render?target=transformNull(foo.*.baz)&from= $(( $now - 300 )) \"   |  jq . )   will query for all metrics matching the  foo.*.baz  pattern, applying the  transformNull  function, and returning all datapoints for the last 5 minutes.",
            "title": "Direct"
        },
        {
            "location": "/integrations/grafana/",
            "text": "Grafana\n\n\nM3 supports a variety of Grafana integrations.\n\n\nPrometheus / Graphite Sources\n\n\nM3Coordinator can function as a datasource for Prometheus as well as Graphite. See the \nPrometheus integration\n and \nGraphite integration\n documents respectively for more information.\n\n\nPre-configured Prometheus Dashboards\n\n\nAll M3 applications expose Prometheus metrics on port \n7203\n by default as described in the \nPrometheus integration guide\n, so if you're already monitoring your M3 stack with Prometheus and Grafana you can use our pre-configured dashboards.\n\n\nM3DB Prometheus / Grafana dashboard\n\n\nM3Coordinator Prometheus / Grafana dashboard: TODO\n\n\nAlternatively, you can obtain the JSON for our most up-to-date dashboards from our \ngit repo\n directly.",
            "title": "Grafana"
        },
        {
            "location": "/integrations/grafana/#grafana",
            "text": "M3 supports a variety of Grafana integrations.",
            "title": "Grafana"
        },
        {
            "location": "/integrations/grafana/#prometheus-graphite-sources",
            "text": "M3Coordinator can function as a datasource for Prometheus as well as Graphite. See the  Prometheus integration  and  Graphite integration  documents respectively for more information.",
            "title": "Prometheus / Graphite Sources"
        },
        {
            "location": "/integrations/grafana/#pre-configured-prometheus-dashboards",
            "text": "All M3 applications expose Prometheus metrics on port  7203  by default as described in the  Prometheus integration guide , so if you're already monitoring your M3 stack with Prometheus and Grafana you can use our pre-configured dashboards.  M3DB Prometheus / Grafana dashboard  M3Coordinator Prometheus / Grafana dashboard: TODO  Alternatively, you can obtain the JSON for our most up-to-date dashboards from our  git repo  directly.",
            "title": "Pre-configured Prometheus Dashboards"
        },
        {
            "location": "/integrations/influxdb/",
            "text": "InfluxDB\n\n\nThis document is a getting started guide to integrating InfluxDB data pipelines \nwith M3.\n\n\nWriting metrics using InfluxDB line protocol\n\n\nTo write metrics to M3 using the InfluxDB line protocol, simply form the request \nas you typically would line separated and POST the body to \n/api/v1/influxdb/write\n \non the coordinator. Note that timestamp is in nanoseconds from Unix epoch.\n\n\nThis example writes two metrics \nweather_temperature\n and \nweather_wind\n using \nthe current time in nanoseconds as the timestamp:\n\ncurl -i -X POST \n\"http://localhost:7201/api/v1/influxdb/write\"\n --data-binary \n\"weather,location=us-midwest temperature=82,wind=42 \n$(\nexpr \n$(\ndate +%s\n)\n \n\\*\n \n1000000000\n)\n\"\n\n\n\n\nQuerying for metrics\n\n\nAfter successfully written you can query for these metrics using PromQL. All \nmeasurements are translated into metric names by concatenating the name with\nthe measurement name.\n\n\nThe previous example forms the two following Prometheus time series:\n\nweather_temperature{location=\"us-midwest\"} 82\nweather_wind{location=\"us-midwest\"} 42\n\n\n\nAll metric names and labels are rewritten to contain only alphanumeric \ncharacters. Any non-alphanumeric characters are rewritten with an underscore.",
            "title": "InfluxDB"
        },
        {
            "location": "/integrations/influxdb/#influxdb",
            "text": "This document is a getting started guide to integrating InfluxDB data pipelines \nwith M3.",
            "title": "InfluxDB"
        },
        {
            "location": "/integrations/influxdb/#writing-metrics-using-influxdb-line-protocol",
            "text": "To write metrics to M3 using the InfluxDB line protocol, simply form the request \nas you typically would line separated and POST the body to  /api/v1/influxdb/write  \non the coordinator. Note that timestamp is in nanoseconds from Unix epoch.  This example writes two metrics  weather_temperature  and  weather_wind  using \nthe current time in nanoseconds as the timestamp: curl -i -X POST  \"http://localhost:7201/api/v1/influxdb/write\"  --data-binary  \"weather,location=us-midwest temperature=82,wind=42  $( expr  $( date +%s )   \\*   1000000000 ) \"",
            "title": "Writing metrics using InfluxDB line protocol"
        },
        {
            "location": "/integrations/influxdb/#querying-for-metrics",
            "text": "After successfully written you can query for these metrics using PromQL. All \nmeasurements are translated into metric names by concatenating the name with\nthe measurement name.  The previous example forms the two following Prometheus time series: weather_temperature{location=\"us-midwest\"} 82\nweather_wind{location=\"us-midwest\"} 42  All metric names and labels are rewritten to contain only alphanumeric \ncharacters. Any non-alphanumeric characters are rewritten with an underscore.",
            "title": "Querying for metrics"
        },
        {
            "location": "/troubleshooting/",
            "text": "Troubleshooting\n\n\nSome common problems and resolutions\n\n\nPorts 9001-9004 aren't open after starting m3db.\n\n\nThese ports will not open until a namespace and placement have been created and the nodes have bootstrapped.\n\n\nBootstrapping is slow\n\n\nDouble check your configuration against the \nbootstrapping guide\n. The nodes will log what bootstrapper they are using and what time range they are using it for.\n\n\nIf you're using the commitlog bootstrapper, and it seems to be slow, ensure that snapshotting is enabled for your namespace. Enabling snapshotting will require a node restart to take effect.\n\n\nIf an m3db node hasn't been able to snapshot for awhile, or is stuck in the commitlog bootstrapping phase for a long time due to accumulating a large number of commitlogs, consider using the peers bootstrapper. In situations where a large number of commitlogs need to be read, the peers bootstrapper will outperform the commitlog bootstrapper (faster and less memory usage) due to the fact that it will receive already-compressed data from its peers. Keep in mind that this will only work with a replication factor of 3 or larger and if the nodes peers are healthy and bootstrapped. Review the \nbootstrapping guide\n for more information.\n\n\nNodes a crashing with memory allocation errors, but there's plenty of available memory\n\n\nEnsure you've set \nvm.max_map_count\n to something like 262,144 using sysctl. Find out more in the \nClustering the Hard Way\n document.\n\n\nWhat to do if my M3DB node is OOM\u2019ing?\n\n\n\n\nEnsure that you are not co-locating coordinator, etcd or query nodes with your M3DB nodes. Colocation or embedded mode is fine for a development environment, but highly discouraged in production.\n\n\nCheck to make sure you are running adequate block sizes based on the retention of your namespace. See \nnamespace configuration\n for more information.\n\n\nEnsure that you use at most 50-60% memory utilization in the normal running state. You want to ensure enough overhead to handle bursts of metrics, especially ones with new IDs as those will take more memory initially.\n\n\nHigh cardinality metrics can also lead to OOMs especially if you are not adequately provisioned. If you have many unique timeseries such as ones containing UUIDs or timestamps as tag values, you should consider mitigating their cardinality.\n\n\n\n\nUsing the /debug/dump API\n\n\nThe \n/debug/dump\n API returns a number of helpful debugging outputs. Currently, we support the following:\n\n\n\n\nCPU profile: determines where a program spends its time while actively consuming CPU cycles (as opposed to while sleeping or waiting for I/O). Currently set to take a 5 second profile.\n\n\nHeap profile: reports memory allocation samples; used to monitor current and historical memory usage, and to check for memory leaks.\n\n\nGoroutines profile: reports the stack traces of all current goroutines.\n\n\nHost profile: returns data about the underlying host such as PID, working directory, etc.\n\n\nNamespace: returns information about the namespaces setup in M3DB\n\n\nPlacement: returns information about the placement setup in M3DB\n\n\n\n\nThis endpoint can be used on both the db nodes as well as the coordinator/query nodes. However, namespace and placement info are only available on the coordinator debug endpoint.\n\n\nTo use this, simply run the following on either the M3DB debug listen port or the regular port on M3Coordinator.\n\n\ncurl <m3db_or_m3coordinator_ip>:<port>/debug/dump > <tmp_zip_file.zip>\n\n# unzip the file\nunzip <tmp_zip_file.zip>\n\n\n\n\nNow, you will have the following files, which you can use for troubleshooting using the below commands:\n\n\ncpuSource\n\n\ngo tool pprof -http=:16000 cpuSource\n\n\n\nheapSource\n\n\ngo tool pprof -http=:16000 heapSource\n\n\n\ngoroutineProfile\n\n\nless goroutineProfile\n\n\n\nhostSource\n\n\nless hostSource | jq .\n\n\n\nnamespaceSource\n\n\nless namespaceSource | jq .\n\n\n\nplacementSource\n\n\nless placementSource | jq .",
            "title": "Troubleshooting"
        },
        {
            "location": "/troubleshooting/#troubleshooting",
            "text": "Some common problems and resolutions",
            "title": "Troubleshooting"
        },
        {
            "location": "/troubleshooting/#ports-9001-9004-arent-open-after-starting-m3db",
            "text": "These ports will not open until a namespace and placement have been created and the nodes have bootstrapped.",
            "title": "Ports 9001-9004 aren't open after starting m3db."
        },
        {
            "location": "/troubleshooting/#bootstrapping-is-slow",
            "text": "Double check your configuration against the  bootstrapping guide . The nodes will log what bootstrapper they are using and what time range they are using it for.  If you're using the commitlog bootstrapper, and it seems to be slow, ensure that snapshotting is enabled for your namespace. Enabling snapshotting will require a node restart to take effect.  If an m3db node hasn't been able to snapshot for awhile, or is stuck in the commitlog bootstrapping phase for a long time due to accumulating a large number of commitlogs, consider using the peers bootstrapper. In situations where a large number of commitlogs need to be read, the peers bootstrapper will outperform the commitlog bootstrapper (faster and less memory usage) due to the fact that it will receive already-compressed data from its peers. Keep in mind that this will only work with a replication factor of 3 or larger and if the nodes peers are healthy and bootstrapped. Review the  bootstrapping guide  for more information.",
            "title": "Bootstrapping is slow"
        },
        {
            "location": "/troubleshooting/#nodes-a-crashing-with-memory-allocation-errors-but-theres-plenty-of-available-memory",
            "text": "Ensure you've set  vm.max_map_count  to something like 262,144 using sysctl. Find out more in the  Clustering the Hard Way  document.",
            "title": "Nodes a crashing with memory allocation errors, but there's plenty of available memory"
        },
        {
            "location": "/troubleshooting/#what-to-do-if-my-m3db-node-is-ooming",
            "text": "Ensure that you are not co-locating coordinator, etcd or query nodes with your M3DB nodes. Colocation or embedded mode is fine for a development environment, but highly discouraged in production.  Check to make sure you are running adequate block sizes based on the retention of your namespace. See  namespace configuration  for more information.  Ensure that you use at most 50-60% memory utilization in the normal running state. You want to ensure enough overhead to handle bursts of metrics, especially ones with new IDs as those will take more memory initially.  High cardinality metrics can also lead to OOMs especially if you are not adequately provisioned. If you have many unique timeseries such as ones containing UUIDs or timestamps as tag values, you should consider mitigating their cardinality.",
            "title": "What to do if my M3DB node is OOM\u2019ing?"
        },
        {
            "location": "/troubleshooting/#using-the-debugdump-api",
            "text": "The  /debug/dump  API returns a number of helpful debugging outputs. Currently, we support the following:   CPU profile: determines where a program spends its time while actively consuming CPU cycles (as opposed to while sleeping or waiting for I/O). Currently set to take a 5 second profile.  Heap profile: reports memory allocation samples; used to monitor current and historical memory usage, and to check for memory leaks.  Goroutines profile: reports the stack traces of all current goroutines.  Host profile: returns data about the underlying host such as PID, working directory, etc.  Namespace: returns information about the namespaces setup in M3DB  Placement: returns information about the placement setup in M3DB   This endpoint can be used on both the db nodes as well as the coordinator/query nodes. However, namespace and placement info are only available on the coordinator debug endpoint.  To use this, simply run the following on either the M3DB debug listen port or the regular port on M3Coordinator.  curl <m3db_or_m3coordinator_ip>:<port>/debug/dump > <tmp_zip_file.zip>\n\n# unzip the file\nunzip <tmp_zip_file.zip>  Now, you will have the following files, which you can use for troubleshooting using the below commands:  cpuSource  go tool pprof -http=:16000 cpuSource  heapSource  go tool pprof -http=:16000 heapSource  goroutineProfile  less goroutineProfile  hostSource  less hostSource | jq .  namespaceSource  less namespaceSource | jq .  placementSource  less placementSource | jq .",
            "title": "Using the /debug/dump API"
        },
        {
            "location": "/faqs/",
            "text": "FAQs\n\n\n\n\n\n\nIs there a way to disable M3DB embedded \netcd\n and just use an external \netcd\n cluster?\n\nYes, you can definitely do that. It's all just about setting the etcd endpoints in config as etcd hosts instead of M3DB hosts. See \nthese docs\n for more information on configuring an external \netcd\n cluster.\n\n\n\n\n\n\nIs there a client that lets me send metrics to m3coordinator without going through Prometheus?\n\nYes, you can use the \nPrometheus remote write client\n.\n\n\n\n\n\n\nWhy does my dbnode keep OOM\u2019ing?\n\nRefer to the \ntroubleshooting guide\n.\n\n\n\n\n\n\nDo you support PromQL?\n\nYes, M3Query and M3Coordinator both support PromQL.\n\n\n\n\n\n\nDo you support Graphite?\n\nYes, M3Query and M3Coordinator both support Graphite.\n\n\n\n\n\n\nDoes M3DB store both data and (tag) metadata on the same node?\n\nYes it stores the data (i.e. the timeseries datapoints) as well as the tags since it has an embedded index. Make sure you have \nIndexEnabled\n set to \ntrue\n in your namespace configuration\n\n\n\n\n\n\nHow are writes handled and how is the data kept consistent within M3DB?\n\nM3 uses quorum/majority consistency to ensure data is written to replicas in a way that can be read back consistently. \nFor example, if you have a replication factor of 3 and you set your write and read consistencies to quorum, then all writes will only succeed if they make it to at least 2 of the 3 replicas, and reads will only succeed if they get results back from at least 2 of the 3 replicas\n\n\n\n\n\n\nDo I need to restart M3DB if I add a namespace?\n\nIf you\u2019re adding namespaces, the m3dbnode process will pickup the new namespace without a restart.\n\n\n\n\n\n\nDo I need to restart M3DB if I change or delete a namespace?\n\nIf you\u2019re removing or modifying an existing namespace, you\u2019ll need to restart the m3dbnode process in order to complete the namespace deletion/modification process. It is recommended to restart one node at a time and wait for a node to be completely bootstrapped before restarting another node.\n\n\n\n\n\n\nHow do I set up aggregation in the coordinator?\n\nRefer to the \nAggregation section\n of the M3Query how-to guide.\n\n\n\n\n\n\nHow do I set up aggregation using a separate aggregation tier?\n\nSee this \nWIP documentation\n.\n\n\n\n\n\n\nCan you delete metrics from M3DB?\n\nNot yet, but that functionality is currently being worked on.\n\n\n\n\n\n\nHow can you tell if a node is snapshotting?\n\nYou can check if your nodes are snapshotting by looking at the \nBackground tasks\n tab in the \nM3DB Grafana dashboard\n.\n\n\n\n\n\n\nHow do you list all available API endpoints?\n\nSee \nM3DB OpenAPI\n.\n\n\n\n\n\n\nWhat is the recommended way to upgrade my M3 stack?\n\nSee the \nUpgrading M3\n guide.\n\n\n\n\n\n\nWhen graphing my Prometheus data in Grafana, I see gaps. How do I resolve this?\n\nThis is due to M3 having a concept of \nnull\n datapoints whereas Prometheus does not. To resolve this, change \nStacking & Null value\n to \nConnected\n under the \nVisualization\n tab of your graph.\n\n\n\n\n\n\nI am receiving the error \n\"could not create etcd watch\",\"error\":\"etcd watch create timed out after 10s for key: _sd.placement/default_env/m3db\"\n\nThis is due to the fact that M3DB, M3Coordinator, etc. could not connect to the \netcd\n server. Make sure that the endpoints listed under in the following config section are correct AND the correct configuration file is being used.\n\netcdClusters:\n  - zone: embedded\n    endpoints:\n      - HOST1_STATIC_IP_ADDRESS:2379\n      - HOST2_STATIC_IP_ADDRESS:2379\n      - HOST3_STATIC_IP_ADDRESS:2379\n\n\n\n\n\n\n\nHow can I get a heap dump, cpu profile, etc.\n\nSee our docs on the \n/debug/dump api\n\n\n\n\n\n\nHow much memory utilization should I run M3DB at?\n\nWe recommend not going above 50%.\n\n\n\n\n\n\nWhat is the recommended hardware to run on?\n\nTBA\n\n\n\n\n\n\nWhat is the recommended way to create a new namespace?\n\nRefer to the \nNamespace configuration guide\n.\n\n\n\n\n\n\nHow can I see the cardinality of my metrics?\n\nCurrently, the best way is to go to the \nM3DB Node Details Dashboard\n and look at the \nTicking\n panel. However, this is not entirely accurate because of the way data is stored in M3DB -- time series are stored inside time-based blocks that you configure. In actuality, the \nTicking\n graph shows you how many unique series there are for the most recent block that has persisted. In the future, we plan to introduce easier ways to determine the number of unique time series.",
            "title": "FAQs"
        },
        {
            "location": "/faqs/#faqs",
            "text": "Is there a way to disable M3DB embedded  etcd  and just use an external  etcd  cluster? \nYes, you can definitely do that. It's all just about setting the etcd endpoints in config as etcd hosts instead of M3DB hosts. See  these docs  for more information on configuring an external  etcd  cluster.    Is there a client that lets me send metrics to m3coordinator without going through Prometheus? \nYes, you can use the  Prometheus remote write client .    Why does my dbnode keep OOM\u2019ing? \nRefer to the  troubleshooting guide .    Do you support PromQL? \nYes, M3Query and M3Coordinator both support PromQL.    Do you support Graphite? \nYes, M3Query and M3Coordinator both support Graphite.    Does M3DB store both data and (tag) metadata on the same node? \nYes it stores the data (i.e. the timeseries datapoints) as well as the tags since it has an embedded index. Make sure you have  IndexEnabled  set to  true  in your namespace configuration    How are writes handled and how is the data kept consistent within M3DB? \nM3 uses quorum/majority consistency to ensure data is written to replicas in a way that can be read back consistently. \nFor example, if you have a replication factor of 3 and you set your write and read consistencies to quorum, then all writes will only succeed if they make it to at least 2 of the 3 replicas, and reads will only succeed if they get results back from at least 2 of the 3 replicas    Do I need to restart M3DB if I add a namespace? \nIf you\u2019re adding namespaces, the m3dbnode process will pickup the new namespace without a restart.    Do I need to restart M3DB if I change or delete a namespace? \nIf you\u2019re removing or modifying an existing namespace, you\u2019ll need to restart the m3dbnode process in order to complete the namespace deletion/modification process. It is recommended to restart one node at a time and wait for a node to be completely bootstrapped before restarting another node.    How do I set up aggregation in the coordinator? \nRefer to the  Aggregation section  of the M3Query how-to guide.    How do I set up aggregation using a separate aggregation tier? \nSee this  WIP documentation .    Can you delete metrics from M3DB? \nNot yet, but that functionality is currently being worked on.    How can you tell if a node is snapshotting? \nYou can check if your nodes are snapshotting by looking at the  Background tasks  tab in the  M3DB Grafana dashboard .    How do you list all available API endpoints? \nSee  M3DB OpenAPI .    What is the recommended way to upgrade my M3 stack? \nSee the  Upgrading M3  guide.    When graphing my Prometheus data in Grafana, I see gaps. How do I resolve this? \nThis is due to M3 having a concept of  null  datapoints whereas Prometheus does not. To resolve this, change  Stacking & Null value  to  Connected  under the  Visualization  tab of your graph.    I am receiving the error  \"could not create etcd watch\",\"error\":\"etcd watch create timed out after 10s for key: _sd.placement/default_env/m3db\" \nThis is due to the fact that M3DB, M3Coordinator, etc. could not connect to the  etcd  server. Make sure that the endpoints listed under in the following config section are correct AND the correct configuration file is being used. etcdClusters:\n  - zone: embedded\n    endpoints:\n      - HOST1_STATIC_IP_ADDRESS:2379\n      - HOST2_STATIC_IP_ADDRESS:2379\n      - HOST3_STATIC_IP_ADDRESS:2379    How can I get a heap dump, cpu profile, etc. \nSee our docs on the  /debug/dump api    How much memory utilization should I run M3DB at? \nWe recommend not going above 50%.    What is the recommended hardware to run on? \nTBA    What is the recommended way to create a new namespace? \nRefer to the  Namespace configuration guide .    How can I see the cardinality of my metrics? \nCurrently, the best way is to go to the  M3DB Node Details Dashboard  and look at the  Ticking  panel. However, this is not entirely accurate because of the way data is stored in M3DB -- time series are stored inside time-based blocks that you configure. In actuality, the  Ticking  graph shows you how many unique series there are for the most recent block that has persisted. In the future, we plan to introduce easier ways to determine the number of unique time series.",
            "title": "FAQs"
        },
        {
            "location": "/glossary/",
            "text": "Glossary\n\n\n\n\n\n\nBootstrapping\n: Process by which an M3DB node is brought up. Bootstrapping consists of determining\nthe integrity of data that the node has, replay writes from the commit log, and/or stream missing data\nfrom its peers.\n\n\n\n\n\n\nCardinality\n: The number of unique metrics within the M3DB index. Cardinality increases with the\nnumber of unique tag/value combinations that are being emitted. \n\n\n\n\n\n\nDatapoint\n: A single timestamp/value. Timeseries are composed of multiple datapoints and a series\nof tag/value pairs. \n\n\n\n\n\n\nLabels\n: Pairs of descriptive words that give meaning to a metric. \nTags\n and \nLabels\n are interchangeable terms.\n\n\n\n\n\n\nMetric\n: A collection of uniquely identifiable tags.\n\n\n\n\n\n\nM3\n: Highly scalable, distributed metrics platform that is comprised of a native, distributed time\nseries database, a highly-dynamic and performant aggregation service, a query engine, and other\nsupporting infrastructure.\n\n\n\n\n\n\nM3Coordinator\n: A service within M3 that coordinates reads and writes between upstream systems,\nsuch as Prometheus, and downstream systems, such as M3DB.\n\n\n\n\n\n\nM3DB\n: Distributed time series database influenced by \nGorilla\n\nand \nCassandra\n released as open source by Uber Technologies.\n\n\n\n\n\n\nM3Query\n: A distributed query engine for M3DB. Unlike M3Coordinator, M3Query only provides supports for reads.\n\n\n\n\n\n\nNamespace\n: Similar to a table in other types of databases, namespaces in M3DB have a unique name\nand a set of configuration options, such as data retention and block size.\n\n\n\n\n\n\nPlacement\n: Map of the M3DB cluster's shard replicas to nodes. Each M3DB cluster has only one placement.\n\nPlacement\n and \nTopology\n are interchangeable terms.\n\n\n\n\n\n\nShard\n: Effectively the same as a \"virtual shard\" in Cassandra in that it provides an arbitrary\ndistribution of time series data via a simple hash of the series ID.\n\n\n\n\n\n\nTags\n: Pairs of descriptive words that give meaning to a metric. \nTags\n and \nLabels\n are interchangeable terms.\n\n\n\n\n\n\nTimeseries\n: A series of data points tracking a particular metric over time.\n\n\n\n\n\n\nTopology\n: Map of the M3DB cluster's shard replicas to nodes. Each M3DB cluster has only one placement.\n\nPlacement\n and \nTopology\n are interchangeable terms.",
            "title": "Glossary"
        },
        {
            "location": "/glossary/#glossary",
            "text": "Bootstrapping : Process by which an M3DB node is brought up. Bootstrapping consists of determining\nthe integrity of data that the node has, replay writes from the commit log, and/or stream missing data\nfrom its peers.    Cardinality : The number of unique metrics within the M3DB index. Cardinality increases with the\nnumber of unique tag/value combinations that are being emitted.     Datapoint : A single timestamp/value. Timeseries are composed of multiple datapoints and a series\nof tag/value pairs.     Labels : Pairs of descriptive words that give meaning to a metric.  Tags  and  Labels  are interchangeable terms.    Metric : A collection of uniquely identifiable tags.    M3 : Highly scalable, distributed metrics platform that is comprised of a native, distributed time\nseries database, a highly-dynamic and performant aggregation service, a query engine, and other\nsupporting infrastructure.    M3Coordinator : A service within M3 that coordinates reads and writes between upstream systems,\nsuch as Prometheus, and downstream systems, such as M3DB.    M3DB : Distributed time series database influenced by  Gorilla \nand  Cassandra  released as open source by Uber Technologies.    M3Query : A distributed query engine for M3DB. Unlike M3Coordinator, M3Query only provides supports for reads.    Namespace : Similar to a table in other types of databases, namespaces in M3DB have a unique name\nand a set of configuration options, such as data retention and block size.    Placement : Map of the M3DB cluster's shard replicas to nodes. Each M3DB cluster has only one placement. Placement  and  Topology  are interchangeable terms.    Shard : Effectively the same as a \"virtual shard\" in Cassandra in that it provides an arbitrary\ndistribution of time series data via a simple hash of the series ID.    Tags : Pairs of descriptive words that give meaning to a metric.  Tags  and  Labels  are interchangeable terms.    Timeseries : A series of data points tracking a particular metric over time.    Topology : Map of the M3DB cluster's shard replicas to nodes. Each M3DB cluster has only one placement. Placement  and  Topology  are interchangeable terms.",
            "title": "Glossary"
        },
        {
            "location": "/misc/writing_docs/",
            "text": "Tips For Writing Documentation\n\n\nWriting is easy. Writing \nwell\n is hard. Writing documentation is even harder and, as one might\nexpect, writing \ngood documentation\n can be very hard. The challenge ultimately stems from the fact\nthat good documentation requires attention to several things: consistency, clarity, detail,\ninformation density, unambiguity, and grammar (of course).\n\n\nThis page describes tips for writing documentation (for M3 or otherwise). There are many different,\nequally-valid writing styles; this document isn't meant as a prescription for how to write, but\ninstead as optional (but encouraged) guidelines geared toward writing consistent, clear,\neasy-to-read documentation.\n\n\nTip #1: Avoid first- and second-person pronouns.\n\n\nDocumentation should always be written from a \nthird-person point of view\n (specifically,\nwith a \nthird-person objective narrative voice\n); avoid using \nfirst-\n and\n\nsecond-person\n. The point of documentation is not to be a message from the author or to\nconvey their point of view, but rather to be objective, factual, and canonical.\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n\n\n\n\n\n\n\n\nBAD\n\n\nYou will need to set Value X to \"foo\", as our testing indicated that \"bar\" can cause issues when solar rays are present.\n\n\n\n\n\n\nBAD\n\n\nWe suggest setting Value X to \"foo\", as testing indicated that \"bar\" can cause issues when solar rays are present.\n\n\n\n\n\n\nGOOD\n\n\nValue X must not be set to \"bar\", as testing indicated that it may cause issues when solar rays are present.\n\n\n\n\n\n\nGOOD\n\n\nValue X must be set to \"foo\"; a value of \"bar\" may cause issues when solar rays are present.\n\n\n\n\n\n\n\n\nTip #2: Avoid subjective language.\n\n\nSubjective language \u2013 language that is open to interpretation, particularly based on perspective \u2013\nshould be avoided wherever possible. Documentation should state objective or broadly empirical\ntruths (and should be supported by facts). Introducing perspective inherently creates ambiguity and\nvagueness in the document, as its meaning becomes reader-dependent. Importantly, cases of subjective\nlanguage are distinct from cases of conditional behavior.\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n\n\n\n\n\n\n\n\nBAD\n\n\nSetting PowerLevel to 9000 is required, because efficiency is better than reliability in high-rate situations.\n\n\n\n\n\n\nGOOD\n\n\nIn high-rate situations (\u22651k req/s), PowerLevel should be set to 9000 in order to maximize efficiency; otherwise, the system defaults to ensuring reliability.\n\n\n\n\n\n\n\n\nTip #3: Focus on concrete ideas.\n\n\nIt can be easy to get sidetracked within documentation, or to bury critical points amidst tangent,\nancillary, or otherwise extraneous information. When writing documentation, always ask, \"What am I\ntrying to say?\". \nUsing terse bullets\n can help structure information, and using a\n\nself-defined guideline for sentence (or paragraph) length\n can help to optimize\nthe message such that the information density is high (i.e., no fluff or filler).\n\n\nTip #4: Use concise sentences (or fragments).\n\n\nSentences and sentence fragments should be simple and to the point (but not stuttering). Each\nsentence should ideally convey a single idea, but clearly-structured compound sentences are\nacceptable as well.\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n\n\n\n\n\n\n\n\nBAD\n\n\nIn order to time travel the MaxSpeed setting must be set to 88MPH because lower speeds will not achieve the desired result, but users should be careful around turns as high speeds are dangerous.\n\n\n\n\n\n\nBAD\n\n\nTime travel requires MaxSpeed to be set. The ideal value is 88MPH. Lower speeds will not achieve the desired result. Users should be careful around turns as high speeds are dangerous.\n\n\n\n\n\n\nGOOD\n\n\nSetting MaxSpeed to 88MPH is required to enable time travel \u2013 lower speeds will not achieve the desired result. However, users should take care around turns, as high speeds are dangerous.\n\n\n\n\n\n\n\n\nTip #5: Write down questions that the documentation should answer.\n\n\nThe point of writing documentation is to document the subject and educate the reader, but every\npiece of documentation should be written with an understanding of which questions it is answering.\nQuestions can be as specific as \"How do I deploy service X?\", or as vague as \"How does the system\nwork?\", but for vaguer questions, there are ostensibly sub-questions (or\n\nskeleton bullets\n) that help to flesh them out.\n\n\nThese questions can be included as a preface to the documentation if desired, which can serve to set\nthe reader's expectations and frame of reference (both of which can help to improve comprehension\nand retention).\n\n\nA contrived example:\n\n\nAfter reading this document, users should be able to answer:\n\n    - What does the system's topology look like?\n    - What are components X, Y, and Z, and how are they related?\n    - When should one use Modes 1-3 of component X?\n    - How can component Y be deployed in a HA capacity?\n    - How can components X and Y be tuned to mitigate load on component Z?\n    - How can all of the components be configured for correctness versus availability?\n\n\n\n\nTip #6: Write skeleton documentation first.\n\n\nStarting with a skeleton shows the full breadth and depth of a document (or set of documents) before\nanything is written, enabling a more holistic view of the content and helping to ensure cohesion and\nflow. Conversely, by fleshing out each section as it's added, it can be easy to lose track of the\nscope of the section relative to the document, or the scope of the document itself. This approach\nalso serves as a TODO list, helping to ensure that sections aren't forgotten about and enabling the\ndocumentation to be divided and conquered.\n\n\nA contrived example:\n\n\n# The Document Title\n\n## Overview\n\n    - One-sentence or elevator pitch summary\n    - Introduce components X, Y, and Z\n\n## Component X\n\n### Overview\n    - Responsible for all client ingress\n    - Sole entrypoint into the system\n    - Can be deployed to support multiple client encodings\n    - Adapter layer for translating all ingress from clients into <format>\n    - Traffic goes: Client -> Component X -> Component Y -> Component Z\n\n### Deployment modes\n    - HA\n    - Lossless\n    - High-throughput\n\n## Component Y\n\n### Overview\n    - Dynamic, stateless, full-mesh cluster topology\n    - Deduplicates all ingress from Component X\n    - Partitions data among Component Z backends using perfect hashing\n\n### Deployment modes\n    - Dynamic cluster\n    - Static cluster\n    - Full mesh\n    - Ring network\n    - Isolated\n\n## Component Z\n\n### Overview\n    - Persists normalized client data\n    - Maintains a sharded, in-memory cache\n\n### Deployment modes\n    - HA\n    - Prioritized throughput\n    - Writethrough cache\n    - Readthrough cache\n    - Ring buffer\n    - LRU\n\n\n\n\nTip #7: When in doubt, less is more.\n\n\nWhile it may seem counterintuitive to start by writing less in a given document, terseness \u2013\nspecifically, fewer sentences or words to explain an idea \u2013 is easier to fix than verboseness.\nIt is far easier to find gaps in a reader's comprehension that are caused by missing documentation\nthan it is to try and find the minimal subset of documentation necessary to convey an idea. In other\nwords: start short, and build out.\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n\n\n\n\n\n\n\n\nBAD\n\n\nConfiguring the cache to be a LRU readthrough cache ensures that the minimal amount of memory is used at the expense of guaranteed initial cache misses. Using minimal memory is important, as servers have finite memory that does not scale with the size of persisted data, and thus the bulk of the data will incur cache misses after a certain point (though compression may be used to mitigate this in the future).\n\n\n\n\n\n\nGOOD\n\n\nConfiguring the cache as a LRU readthrough cache results in a minimal memory footprint (due to the combined behavior of both readthrough and LRU caching) at the expense of initial cache misses. Cache efficiency may be improved in the future.\n\n\n\n\n\n\n\n\nTip #8: Write for an uninitiated target audience (and call out prerequisites).\n\n\nEvery piece of technical documentation has at least one baseline prerequisite that the documentation\nbuilds upon; otherwise, every document would need to explain\n\nhow computers work\n. However, in the case of M3 (as an example), the reader\nmight be someone writing code that emits metrics, an infrastructure engineer that wants to run a\nmetrics stack, someone with experience running other metrics stacks that is looking for nuanced\ntradeoffs between this and other platforms, or a distributed systems enthusiast (or a thousand other\nfolks).\n\n\nIt's relatively straightforward to tailor a document to a given audience: an end-user looking to\nemit metrics is a \nmuch\n different demographic than someone wanting to run M3 in their\ninfrastructure (etc), and as such, there is often no conflation. However, the skill level or\ntechnical depth of a given audience is often varied: a person who only vaguely understands what a\nmetric really is will require a much broader set of information than a person who has experience\nusing metrics in complex and interesting ways.\n\n\nThus, it's important to cater to a baseline: don't expect that every piece of technical jargon will\nbe understood. Similarly, it's not a document's responsibility to satisfy its own prerequisites -\nit's okay to inform the reader that they should be familiar with X, Y, and Z, else they won't get\nmuch out of the document. If there are prerequisites that normalize the baseline of most or all\nreaders, call that out at the beginning (e.g., \"Users should read and understand Document X before\nreading this document\").\n\n\nTip #9: Make sure that documentation is reviewed thoroughly.\n\n\nErroneous documentation can lead to confusion, misunderstanding, incorrect assumptions, or \u2013 in the\nworst case \u2013 a user unknowingly implementing bad or faulty behavior. Documentation should ideally be\nreviewed like code: thoroughly and pedantically. Depending on the complexity, it may be worth\n\nasking reviewers to answer basic questions\n and update the documentation\nbased on their responses \u2013 for example, if the question \"When should \nConfigValueX\n be set, and\nwhat are its side effects?\" can't be answered, it might merit adding more documentation around the\nsemantics of \nConfigValueX\n, as well as its direct and side effects.\n\n\nTip #10: Have \nat least\n one less-familiar person review documentation.\n\n\nIt's \nextremely\n easy to let assumptions, implicit points, jargon, etc sneak in. Folks who are\nfamiliar with a subject are more likely to subconsciously or contextually fill in the gaps, but they\nare not the documentation's intended audience. Instead, try to find one or more folks who are closer\nto the target audience (relatively speaking) review documentation, and evaluate their comprehension\n(e.g., ask basic questions that the documentation should provide answers for).\n\n\nReviewers should sanity check all assumptions, and ask any questions freely \u2013 it's important to not\nassume that they just missed the boat on some critical information that is obvious to everyone else\n(most documentation should be explanatory and not expect readers to continually read between the\nlines).\n\n\nTip #11: Document both reality \nand\n intent.\n\n\nWhile documentation should reflect the current state of the world, in many cases, it's also\nimportant to document the context or rationale around that state (the \"what\" and the \"why\").\nDocumenting only the \"what\" leaves readers to draw their own inferences and conclusions, while the\n\"why\" answers many of those questions outright (or, at the least, imparts a line of thinking that\nwill more accurately inform those inferences).\n\n\nTip #12: Focus on readability.\n\n\nThe goal of documentation is to provide information to the reader. This means that, aside from\ncorrectness, the most important metrics of documentation are its efficiency (words per idea) and\nefficacy (readability of words and retention of ideas).\n\n\nReading a sentence in isolation helps to analyze the readability of an idea in a narrow context,\nbut reading multiple sentences (or a paragraph) at a time can be a good signal of how well\ninformation flows.\n\n\nTip #13: Be pedantic about grammar.\n\n\nEnglish grammar can often be flexible, subjectively readable, or confusing (there is more than one\nway to skin a cat). Being clear and using \nsimpler, well-structured sentences\n\ncan \nhelp with readability\n by presenting digestible, unambiguous ideas.\n\nVerb tense\n and \npronouns\n can play an important role as well. Ultimately,\nthe best litmus test may be the simplest: whether a reader needs to read a sentence twice to\nunderstand it (technical terms notwithstanding).\n\n\nRealistically, it is impractical to strive for or expect perfect grammar. The goal isn't to write\nflawless English; the goal is to write \nclear, concise, unambiguous, and readable\n documentation,\nfor which grammar is an important (albeit not the only) tool.\n\n\nTip #14: Use verb tenses consistently.\n\n\nGenerally speaking, the bulk of documentation only makes sense when written in the present tense\n(its \"common tense\", so to speak), as objects are typically referred to in the abstract (e.g.\n\"Component X\" is abstract, \"The running instance of Component X\" proverbially concrete) and do not\nusually involve time (e.g. \"Component X is an upstream of Component Y\" refers to nominal abstract\nstate, \"Component X will be an upstream of Component Y\" is prospective state). Of course, this is\nnot always true: documentation can tell a story, and tenses should be used as needed to express ideas\nrelative to time.\n\n\nHowever, cases of general fact, such as describing cause and effect:\n\n\nValue X -> Behavior Y\n\n\n\n\nshould be described in the present tense:\n\n\nSetting Value X causes Behavior Y.\n\n\n\n\nversus the future tense:\n\n\nSetting Value X will cause Behavior Y.\n\n\n\n\nin order to be consistent with the documentation's common tense, to maintain a time-less (and thus\nongoing) objective truth, and to contrast actual time-relative statements (e.g. \"Setting Value X\ncauses Behavior Y, but will be updated to cause Behavior Z in the future\").\n\n\nTip #15: Make consistent references.\n\n\nWhen communicating verbally, there is no need to distinguish between represented forms for a given\nartifact, value, method, etc - one can simply say \"foo bar\" to reference \"the member function or\nproperty \nBar\n on type \nFoo\n\", or \"foo dot bar\" to reference \"the YAML key \nbar\n nested under\nthe key \nfoo\n\" (as simple examples).\n\n\nIn written documentation, however, these references can either maintain continuity throughout a\ndocument (when used consistently), or they can cause confusion (when used inconsistently). A simple\nframework for references is, \"if it's code (or code-like), it should look like code; otherwise,\ntreat it like a proper noun\". Thankfully, most references fall into two such categories:\n\n\n\n\nCode (and code-like) references\n, such as \nMyClass\n or \nmyVar\n or \nsome.config.property\n. In\n  these cases, \nPreformattedText\n should be used, and should reference the code verbatim. All\n  instances of such a reference should be consistent, and should not be abbreviated.\n\n\nProper (or effectively proper) names\n, such as \"My Component\" or \"My System Name\". In these\n  cases, \"Title Case\" should be used, with the following exceptions:\n\n\nEstablished projects, such as \"etcd\", \"PostgreSQL\", and \"GitHub\", should use the project's\n  official name and format, versus arbitrary ones (e.g. \"Etcd\", \"Postgres\", \"Github\", etc).\n\n\nFirst-party components, such as \"the M3 Aggregator\", can use shorthand when doing so does not\n  contextually result in ambiguity (e.g. \"the Aggregator\", when already discussing the Aggregator),\n  but should still be proper nouns.\n\n\n\n\n\n\n\n\nFor other or nuanced cases, it's more important to be consistent than it is to fit a reference into\none of these simple categories.\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n\n\n\n\n\n\n\n\nBAD\n\n\nThe Flux Capactitor is a Y-shaped component that enables time travel. Importantly, the enclosing vehicle must be capable of providing 1.21 gigawatts of power, as the flux capacitor requires this energy to function.\n\n\n\n\n\n\nBAD\n\n\nFluxCapacitor's power level property should be set to 1.21.\n\n\n\n\n\n\nGOOD\n\n\nThe Flux Capacitor is a Y-shaped component that enables time travel. Importantly, the enclosing vehicle must be capable of providing 1.21 gigawatts of power, as the Flux Capacitor requires this energy to function.\n\n\n\n\n\n\nGOOD\n\n\nFluxCapacitor\n's \npowerLevel\n property should be set to \n1.21\n.\n\n\n\n\n\n\n\n\nTip #16: Use \nTODO\n/\nTODOC\n placeholders liberally.\n\n\nOften, it might not yet be the right time to write a part of the documentation \u2013 either because a\nfeature isn't ready yet, it's not the most practical time investment at present, or other reasons.\nIn these cases, consider adding a \nTODO\n (in exactly the same way that code \nTODO\ns are used). If\nit's necessary/helpful to disambiguate code \nTODO\ns from documentation \nTODO\ns, \nTODOC\n can be used\ninstead: it's similar enough to match a naive \nTODO\n search, but both \nTODO\n and \nTODOC\n can be\nreasoned about independently as desired. For example:\n\n\n// FluxCapacitor TODOC(anyone)\n\n\ntype\n \nFluxCapacitor\n \nstruct\n \n{\n\n    \n// TODO(anyone): use a discrete GigaWatts type instead of a float\n\n    \nRequiredPower\n \nfloat64\n\n\n}\n\n\n\n\n\n$ grep -rnH \n'TODOC('\n .\n./time/timetravel/flux_capacitor.go:123: // FluxCapacitor TODOC\n(\nanyone\n)\n\n\n\n\n\n$ grep -rnH \n'TODO('\n .\n./time/timetravel/flux_capacitor.go:125:     // TODO\n(\nanyone\n)\n: use a discrete GigaWatts \ntype\n instead of a float",
            "title": "Tips For Writing Documentation"
        },
        {
            "location": "/misc/writing_docs/#tips-for-writing-documentation",
            "text": "Writing is easy. Writing  well  is hard. Writing documentation is even harder and, as one might\nexpect, writing  good documentation  can be very hard. The challenge ultimately stems from the fact\nthat good documentation requires attention to several things: consistency, clarity, detail,\ninformation density, unambiguity, and grammar (of course).  This page describes tips for writing documentation (for M3 or otherwise). There are many different,\nequally-valid writing styles; this document isn't meant as a prescription for how to write, but\ninstead as optional (but encouraged) guidelines geared toward writing consistent, clear,\neasy-to-read documentation.",
            "title": "Tips For Writing Documentation"
        },
        {
            "location": "/misc/writing_docs/#tip-1-avoid-first-and-second-person-pronouns",
            "text": "Documentation should always be written from a  third-person point of view  (specifically,\nwith a  third-person objective narrative voice ); avoid using  first-  and second-person . The point of documentation is not to be a message from the author or to\nconvey their point of view, but rather to be objective, factual, and canonical.     Examples       BAD  You will need to set Value X to \"foo\", as our testing indicated that \"bar\" can cause issues when solar rays are present.    BAD  We suggest setting Value X to \"foo\", as testing indicated that \"bar\" can cause issues when solar rays are present.    GOOD  Value X must not be set to \"bar\", as testing indicated that it may cause issues when solar rays are present.    GOOD  Value X must be set to \"foo\"; a value of \"bar\" may cause issues when solar rays are present.",
            "title": "Tip #1: Avoid first- and second-person pronouns."
        },
        {
            "location": "/misc/writing_docs/#tip-2-avoid-subjective-language",
            "text": "Subjective language \u2013 language that is open to interpretation, particularly based on perspective \u2013\nshould be avoided wherever possible. Documentation should state objective or broadly empirical\ntruths (and should be supported by facts). Introducing perspective inherently creates ambiguity and\nvagueness in the document, as its meaning becomes reader-dependent. Importantly, cases of subjective\nlanguage are distinct from cases of conditional behavior.     Examples       BAD  Setting PowerLevel to 9000 is required, because efficiency is better than reliability in high-rate situations.    GOOD  In high-rate situations (\u22651k req/s), PowerLevel should be set to 9000 in order to maximize efficiency; otherwise, the system defaults to ensuring reliability.",
            "title": "Tip #2: Avoid subjective language."
        },
        {
            "location": "/misc/writing_docs/#tip-3-focus-on-concrete-ideas",
            "text": "It can be easy to get sidetracked within documentation, or to bury critical points amidst tangent,\nancillary, or otherwise extraneous information. When writing documentation, always ask, \"What am I\ntrying to say?\".  Using terse bullets  can help structure information, and using a self-defined guideline for sentence (or paragraph) length  can help to optimize\nthe message such that the information density is high (i.e., no fluff or filler).",
            "title": "Tip #3: Focus on concrete ideas."
        },
        {
            "location": "/misc/writing_docs/#tip-4-use-concise-sentences-or-fragments",
            "text": "Sentences and sentence fragments should be simple and to the point (but not stuttering). Each\nsentence should ideally convey a single idea, but clearly-structured compound sentences are\nacceptable as well.     Examples       BAD  In order to time travel the MaxSpeed setting must be set to 88MPH because lower speeds will not achieve the desired result, but users should be careful around turns as high speeds are dangerous.    BAD  Time travel requires MaxSpeed to be set. The ideal value is 88MPH. Lower speeds will not achieve the desired result. Users should be careful around turns as high speeds are dangerous.    GOOD  Setting MaxSpeed to 88MPH is required to enable time travel \u2013 lower speeds will not achieve the desired result. However, users should take care around turns, as high speeds are dangerous.",
            "title": "Tip #4: Use concise sentences (or fragments)."
        },
        {
            "location": "/misc/writing_docs/#tip-5-write-down-questions-that-the-documentation-should-answer",
            "text": "The point of writing documentation is to document the subject and educate the reader, but every\npiece of documentation should be written with an understanding of which questions it is answering.\nQuestions can be as specific as \"How do I deploy service X?\", or as vague as \"How does the system\nwork?\", but for vaguer questions, there are ostensibly sub-questions (or skeleton bullets ) that help to flesh them out.  These questions can be included as a preface to the documentation if desired, which can serve to set\nthe reader's expectations and frame of reference (both of which can help to improve comprehension\nand retention).  A contrived example:  After reading this document, users should be able to answer:\n\n    - What does the system's topology look like?\n    - What are components X, Y, and Z, and how are they related?\n    - When should one use Modes 1-3 of component X?\n    - How can component Y be deployed in a HA capacity?\n    - How can components X and Y be tuned to mitigate load on component Z?\n    - How can all of the components be configured for correctness versus availability?",
            "title": "Tip #5: Write down questions that the documentation should answer."
        },
        {
            "location": "/misc/writing_docs/#tip-6-write-skeleton-documentation-first",
            "text": "Starting with a skeleton shows the full breadth and depth of a document (or set of documents) before\nanything is written, enabling a more holistic view of the content and helping to ensure cohesion and\nflow. Conversely, by fleshing out each section as it's added, it can be easy to lose track of the\nscope of the section relative to the document, or the scope of the document itself. This approach\nalso serves as a TODO list, helping to ensure that sections aren't forgotten about and enabling the\ndocumentation to be divided and conquered.  A contrived example:  # The Document Title\n\n## Overview\n\n    - One-sentence or elevator pitch summary\n    - Introduce components X, Y, and Z\n\n## Component X\n\n### Overview\n    - Responsible for all client ingress\n    - Sole entrypoint into the system\n    - Can be deployed to support multiple client encodings\n    - Adapter layer for translating all ingress from clients into <format>\n    - Traffic goes: Client -> Component X -> Component Y -> Component Z\n\n### Deployment modes\n    - HA\n    - Lossless\n    - High-throughput\n\n## Component Y\n\n### Overview\n    - Dynamic, stateless, full-mesh cluster topology\n    - Deduplicates all ingress from Component X\n    - Partitions data among Component Z backends using perfect hashing\n\n### Deployment modes\n    - Dynamic cluster\n    - Static cluster\n    - Full mesh\n    - Ring network\n    - Isolated\n\n## Component Z\n\n### Overview\n    - Persists normalized client data\n    - Maintains a sharded, in-memory cache\n\n### Deployment modes\n    - HA\n    - Prioritized throughput\n    - Writethrough cache\n    - Readthrough cache\n    - Ring buffer\n    - LRU",
            "title": "Tip #6: Write skeleton documentation first."
        },
        {
            "location": "/misc/writing_docs/#tip-7-when-in-doubt-less-is-more",
            "text": "While it may seem counterintuitive to start by writing less in a given document, terseness \u2013\nspecifically, fewer sentences or words to explain an idea \u2013 is easier to fix than verboseness.\nIt is far easier to find gaps in a reader's comprehension that are caused by missing documentation\nthan it is to try and find the minimal subset of documentation necessary to convey an idea. In other\nwords: start short, and build out.     Examples       BAD  Configuring the cache to be a LRU readthrough cache ensures that the minimal amount of memory is used at the expense of guaranteed initial cache misses. Using minimal memory is important, as servers have finite memory that does not scale with the size of persisted data, and thus the bulk of the data will incur cache misses after a certain point (though compression may be used to mitigate this in the future).    GOOD  Configuring the cache as a LRU readthrough cache results in a minimal memory footprint (due to the combined behavior of both readthrough and LRU caching) at the expense of initial cache misses. Cache efficiency may be improved in the future.",
            "title": "Tip #7: When in doubt, less is more."
        },
        {
            "location": "/misc/writing_docs/#tip-8-write-for-an-uninitiated-target-audience-and-call-out-prerequisites",
            "text": "Every piece of technical documentation has at least one baseline prerequisite that the documentation\nbuilds upon; otherwise, every document would need to explain how computers work . However, in the case of M3 (as an example), the reader\nmight be someone writing code that emits metrics, an infrastructure engineer that wants to run a\nmetrics stack, someone with experience running other metrics stacks that is looking for nuanced\ntradeoffs between this and other platforms, or a distributed systems enthusiast (or a thousand other\nfolks).  It's relatively straightforward to tailor a document to a given audience: an end-user looking to\nemit metrics is a  much  different demographic than someone wanting to run M3 in their\ninfrastructure (etc), and as such, there is often no conflation. However, the skill level or\ntechnical depth of a given audience is often varied: a person who only vaguely understands what a\nmetric really is will require a much broader set of information than a person who has experience\nusing metrics in complex and interesting ways.  Thus, it's important to cater to a baseline: don't expect that every piece of technical jargon will\nbe understood. Similarly, it's not a document's responsibility to satisfy its own prerequisites -\nit's okay to inform the reader that they should be familiar with X, Y, and Z, else they won't get\nmuch out of the document. If there are prerequisites that normalize the baseline of most or all\nreaders, call that out at the beginning (e.g., \"Users should read and understand Document X before\nreading this document\").",
            "title": "Tip #8: Write for an uninitiated target audience (and call out prerequisites)."
        },
        {
            "location": "/misc/writing_docs/#tip-9-make-sure-that-documentation-is-reviewed-thoroughly",
            "text": "Erroneous documentation can lead to confusion, misunderstanding, incorrect assumptions, or \u2013 in the\nworst case \u2013 a user unknowingly implementing bad or faulty behavior. Documentation should ideally be\nreviewed like code: thoroughly and pedantically. Depending on the complexity, it may be worth asking reviewers to answer basic questions  and update the documentation\nbased on their responses \u2013 for example, if the question \"When should  ConfigValueX  be set, and\nwhat are its side effects?\" can't be answered, it might merit adding more documentation around the\nsemantics of  ConfigValueX , as well as its direct and side effects.",
            "title": "Tip #9: Make sure that documentation is reviewed thoroughly."
        },
        {
            "location": "/misc/writing_docs/#tip-10-have-at-least-one-less-familiar-person-review-documentation",
            "text": "It's  extremely  easy to let assumptions, implicit points, jargon, etc sneak in. Folks who are\nfamiliar with a subject are more likely to subconsciously or contextually fill in the gaps, but they\nare not the documentation's intended audience. Instead, try to find one or more folks who are closer\nto the target audience (relatively speaking) review documentation, and evaluate their comprehension\n(e.g., ask basic questions that the documentation should provide answers for).  Reviewers should sanity check all assumptions, and ask any questions freely \u2013 it's important to not\nassume that they just missed the boat on some critical information that is obvious to everyone else\n(most documentation should be explanatory and not expect readers to continually read between the\nlines).",
            "title": "Tip #10: Have at least one less-familiar person review documentation."
        },
        {
            "location": "/misc/writing_docs/#tip-11-document-both-reality-and-intent",
            "text": "While documentation should reflect the current state of the world, in many cases, it's also\nimportant to document the context or rationale around that state (the \"what\" and the \"why\").\nDocumenting only the \"what\" leaves readers to draw their own inferences and conclusions, while the\n\"why\" answers many of those questions outright (or, at the least, imparts a line of thinking that\nwill more accurately inform those inferences).",
            "title": "Tip #11: Document both reality and intent."
        },
        {
            "location": "/misc/writing_docs/#tip-12-focus-on-readability",
            "text": "The goal of documentation is to provide information to the reader. This means that, aside from\ncorrectness, the most important metrics of documentation are its efficiency (words per idea) and\nefficacy (readability of words and retention of ideas).  Reading a sentence in isolation helps to analyze the readability of an idea in a narrow context,\nbut reading multiple sentences (or a paragraph) at a time can be a good signal of how well\ninformation flows.",
            "title": "Tip #12: Focus on readability."
        },
        {
            "location": "/misc/writing_docs/#tip-13-be-pedantic-about-grammar",
            "text": "English grammar can often be flexible, subjectively readable, or confusing (there is more than one\nway to skin a cat). Being clear and using  simpler, well-structured sentences \ncan  help with readability  by presenting digestible, unambiguous ideas. Verb tense  and  pronouns  can play an important role as well. Ultimately,\nthe best litmus test may be the simplest: whether a reader needs to read a sentence twice to\nunderstand it (technical terms notwithstanding).  Realistically, it is impractical to strive for or expect perfect grammar. The goal isn't to write\nflawless English; the goal is to write  clear, concise, unambiguous, and readable  documentation,\nfor which grammar is an important (albeit not the only) tool.",
            "title": "Tip #13: Be pedantic about grammar."
        },
        {
            "location": "/misc/writing_docs/#tip-14-use-verb-tenses-consistently",
            "text": "Generally speaking, the bulk of documentation only makes sense when written in the present tense\n(its \"common tense\", so to speak), as objects are typically referred to in the abstract (e.g.\n\"Component X\" is abstract, \"The running instance of Component X\" proverbially concrete) and do not\nusually involve time (e.g. \"Component X is an upstream of Component Y\" refers to nominal abstract\nstate, \"Component X will be an upstream of Component Y\" is prospective state). Of course, this is\nnot always true: documentation can tell a story, and tenses should be used as needed to express ideas\nrelative to time.  However, cases of general fact, such as describing cause and effect:  Value X -> Behavior Y  should be described in the present tense:  Setting Value X causes Behavior Y.  versus the future tense:  Setting Value X will cause Behavior Y.  in order to be consistent with the documentation's common tense, to maintain a time-less (and thus\nongoing) objective truth, and to contrast actual time-relative statements (e.g. \"Setting Value X\ncauses Behavior Y, but will be updated to cause Behavior Z in the future\").",
            "title": "Tip #14: Use verb tenses consistently."
        },
        {
            "location": "/misc/writing_docs/#tip-15-make-consistent-references",
            "text": "When communicating verbally, there is no need to distinguish between represented forms for a given\nartifact, value, method, etc - one can simply say \"foo bar\" to reference \"the member function or\nproperty  Bar  on type  Foo \", or \"foo dot bar\" to reference \"the YAML key  bar  nested under\nthe key  foo \" (as simple examples).  In written documentation, however, these references can either maintain continuity throughout a\ndocument (when used consistently), or they can cause confusion (when used inconsistently). A simple\nframework for references is, \"if it's code (or code-like), it should look like code; otherwise,\ntreat it like a proper noun\". Thankfully, most references fall into two such categories:   Code (and code-like) references , such as  MyClass  or  myVar  or  some.config.property . In\n  these cases,  PreformattedText  should be used, and should reference the code verbatim. All\n  instances of such a reference should be consistent, and should not be abbreviated.  Proper (or effectively proper) names , such as \"My Component\" or \"My System Name\". In these\n  cases, \"Title Case\" should be used, with the following exceptions:  Established projects, such as \"etcd\", \"PostgreSQL\", and \"GitHub\", should use the project's\n  official name and format, versus arbitrary ones (e.g. \"Etcd\", \"Postgres\", \"Github\", etc).  First-party components, such as \"the M3 Aggregator\", can use shorthand when doing so does not\n  contextually result in ambiguity (e.g. \"the Aggregator\", when already discussing the Aggregator),\n  but should still be proper nouns.     For other or nuanced cases, it's more important to be consistent than it is to fit a reference into\none of these simple categories.     Examples       BAD  The Flux Capactitor is a Y-shaped component that enables time travel. Importantly, the enclosing vehicle must be capable of providing 1.21 gigawatts of power, as the flux capacitor requires this energy to function.    BAD  FluxCapacitor's power level property should be set to 1.21.    GOOD  The Flux Capacitor is a Y-shaped component that enables time travel. Importantly, the enclosing vehicle must be capable of providing 1.21 gigawatts of power, as the Flux Capacitor requires this energy to function.    GOOD  FluxCapacitor 's  powerLevel  property should be set to  1.21 .",
            "title": "Tip #15: Make consistent references."
        },
        {
            "location": "/misc/writing_docs/#tip-16-use-todotodoc-placeholders-liberally",
            "text": "Often, it might not yet be the right time to write a part of the documentation \u2013 either because a\nfeature isn't ready yet, it's not the most practical time investment at present, or other reasons.\nIn these cases, consider adding a  TODO  (in exactly the same way that code  TODO s are used). If\nit's necessary/helpful to disambiguate code  TODO s from documentation  TODO s,  TODOC  can be used\ninstead: it's similar enough to match a naive  TODO  search, but both  TODO  and  TODOC  can be\nreasoned about independently as desired. For example:  // FluxCapacitor TODOC(anyone)  type   FluxCapacitor   struct   { \n     // TODO(anyone): use a discrete GigaWatts type instead of a float \n     RequiredPower   float64  }   $ grep -rnH  'TODOC('  .\n./time/timetravel/flux_capacitor.go:123: // FluxCapacitor TODOC ( anyone )   $ grep -rnH  'TODO('  .\n./time/timetravel/flux_capacitor.go:125:     // TODO ( anyone ) : use a discrete GigaWatts  type  instead of a float",
            "title": "Tip #16: Use TODO/TODOC placeholders liberally."
        }
    ]
}